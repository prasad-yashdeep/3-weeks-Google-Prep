{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xCWhsY704k7"
      },
      "source": [
        "## Train a large model on a single GPU\n",
        "\n",
        "In this section, we will practice strategies for training a large model on a single GPU. After completing this section, you should understand the effect of\n",
        "\n",
        "-   batch size\n",
        "-   gradient accumulation\n",
        "-   reduced precision/mixed precision\n",
        "-   parameter efficient fine tuning\n",
        "\n",
        "on a large model training job.\n",
        "\n",
        "This notebook will be executed inside a Jupyter interface **hosted on a GPU server instance on Chameleon**, NOT in the Chameleon Jupyter interface from which we launch experiments (provision servers, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9O0SpM804k8"
      },
      "source": [
        "### Open the notebook on Colab\n",
        "\n",
        "We should have already started a notebook server in a container on a Chameleon GPU host, and set up an SSH tunnel to this notebook server. Now, we will open this notebook in Google Colab and connect it to the runtime that you have in Chameleon. This is a convenient way to work, because the notebook and its outputs will be saved automatically in your Google Drive.\n",
        "\n",
        "-   Use this button to open the notebook in Colab: <a target=\"_blank\" href=\"https://colab.research.google.com/github/teaching-on-testbeds/llm-chi/blob/main/workspace/2_single_gpu_a100.ipynb\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/> </a>\n",
        "-   Click “File \\> Save a Copy in Drive” to save it in your own Google Drive. Work in your copy, so that the outputs will be saved automatically.\n",
        "-   Next to the “Connect” button in the top right, there is a ▼ symbol. Click on this symbol to expand the menu, and choose “Connect to a local runtime”.\n",
        "-   Paste the `http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` you copied earlier into this space, and choose “Connect”.\n",
        "\n",
        "**Alternatively, if you prefer not to use Colab** (or can’t, for some reason): just put the `http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX` URL you copied earlier into your browser to open the Jupyter interface directly. But, then you’ll have to open a terminal in that Jupyter interface and run\n",
        "\n",
        "    wget https://raw.githubusercontent.com/teaching-on-testbeds/llm-chi/refs/heads/main/workspace/2_single_gpu_a100.ipynb\n",
        "\n",
        "to get a copy of this notebook in that workspace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdsE_lPG04k9"
      },
      "source": [
        "Make sure that you can see the GPUs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnCGkAEi04k9",
        "outputId": "b5eca0a7-48f6-4420-94d9-375832c38133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 18 06:52:00 2025       \r\n",
            "+-----------------------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\r\n",
            "|-----------------------------------------+------------------------+----------------------+\r\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                                         |                        |               MIG M. |\r\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:25:00.0 Off |                    0 |\n",
            "| N/A   48C    P0             51W /  300W |       1MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q08O9_Lx04k9"
      },
      "source": [
        "### Prepare LitGPT\n",
        "\n",
        "For this tutorial, we will fine-tune an [TinyLlama](https://arxiv.org/abs/2401.02385) or [OpenLLaMA](https://github.com/openlm-research/open_llama) large language model using [`litgpt`](https://github.com/Lightning-AI/litgpt). LitGPT is a convenient wrapper around many PyTorch Lightning capabilities that makes it easy to fine-tune a GPU using a “recipe” defined in a YAML file. (We’ll also try the Python API for LitGPT in the “Multiple GPU” section of this tutorial.)\n",
        "\n",
        "You may browse the “recipes” for this experiment [in our Github repository](https://github.com/teaching-on-testbeds/llm-chi/tree/main/workspace/config).\n",
        "\n",
        "Our focus will be exclusively on comparing the time and memory requirements of training jobs under different settings - we will completely ignore the loss of the fine-tuned model, and we will make some choices to reduce the overall time of our experiment (to fit in a short Chameleon lease) that wouldn’t make sense if we really needed the fine-tuned model (e.g. using a very small fraction of the training data).\n",
        "\n",
        "First, install LitGPT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuzqK7S104k-",
        "outputId": "8c2687d2-2348-4f66-9f7f-378e26496104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litgpt==0.5.7 (from litgpt[all]==0.5.7)\n",
            "  Downloading litgpt-0.5.7-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting lightning<2.5.0.post0\n",
            "  Downloading lightning-2.5.0-py3-none-any.whl.metadata (40 kB)\n",
            "Requirement already satisfied: torch<2.6.0,>=2.5.0 in /opt/conda/lib/python3.12/site-packages (from litgpt==0.5.7->litgpt[all]==0.5.7) (2.5.1+cu124)\n",
            "Collecting numpy<2.0 (from litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting jsonargparse<=4.32.1,>=4.30.1 (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading jsonargparse-4.32.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting huggingface_hub>=0.23.5 (from litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting safetensors>=0.4.3 (from litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting tokenizers>=0.15.2 (from litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.0 in /opt/conda/lib/python3.12/site-packages (from litgpt==0.5.7->litgpt[all]==0.5.7) (4.67.1)\n",
            "Collecting lightning-thunder>=0.2.0.dev20250119 (from litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading lightning_thunder-0.2.2.dev20250216-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting bitsandbytes<0.44.2,>=0.44.0 (from litgpt[all]==0.5.7)\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting sentencepiece>=0.2.0 (from litgpt[all]==0.5.7)\n",
            "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /opt/conda/lib/python3.12/site-packages (from litgpt[all]==0.5.7) (2.32.3)\n",
            "Collecting litdata==0.2.17 (from litgpt[all]==0.5.7)\n",
            "  Downloading litdata-0.2.17-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting litserve<=0.2.4 (from litgpt[all]==0.5.7)\n",
            "  Downloading litserve-0.2.4-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: zstandard>=0.22.0 in /opt/conda/lib/python3.12/site-packages (from litgpt[all]==0.5.7) (0.23.0)\n",
            "Requirement already satisfied: pandas>=1.9.0 in /opt/conda/lib/python3.12/site-packages (from litgpt[all]==0.5.7) (2.2.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.2 in /opt/conda/lib/python3.12/site-packages (from litgpt[all]==0.5.7) (19.0.0)\n",
            "Collecting tensorboard>=2.14.0 (from litgpt[all]==0.5.7)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting torchmetrics>=1.3.1 (from litgpt[all]==0.5.7)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting datasets>=2.18.0 (from litgpt[all]==0.5.7)\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting transformers==4.47.1 (from litgpt[all]==0.5.7)\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting lm-eval>=0.4.2 (from litgpt[all]==0.5.7)\n",
            "  Downloading lm_eval-0.4.7-py3-none-any.whl.metadata (46 kB)\n",
            "Collecting uvloop>=0.2.0 (from litgpt[all]==0.5.7)\n",
            "  Downloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from litdata==0.2.17->litgpt[all]==0.5.7) (3.13.1)\n",
            "Collecting boto3 (from litdata==0.2.17->litgpt[all]==0.5.7)\n",
            "  Downloading boto3-1.36.22-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers==4.47.1->litgpt[all]==0.5.7) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers==4.47.1->litgpt[all]==0.5.7) (6.0.2)\n",
            "Collecting regex!=2019.12.17 (from transformers==4.47.1->litgpt[all]==0.5.7)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /opt/conda/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2.5.0.post0) (2024.12.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.5.0.post0)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.12/site-packages (from lightning<2.5.0.post0) (4.12.2)\n",
            "Collecting pytorch-lightning (from lightning<2.5.0.post0)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aiohttp (from datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading aiohttp-3.11.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting hf-transfer>=0.1.4 (from huggingface_hub[hf_transfer]>=0.21.0; extra == \"all\"->litgpt[all]==0.5.7)\n",
            "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting docstring-parser>=0.15 (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting looseversion==1.3.0 (from lightning-thunder>=0.2.0.dev20250119->litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: networkx>=3.3 in /opt/conda/lib/python3.12/site-packages (from lightning-thunder>=0.2.0.dev20250119->litgpt==0.5.7->litgpt[all]==0.5.7) (3.4.2)\n",
            "Collecting optree>=0.12.1 (from lightning-thunder>=0.2.0.dev20250119->litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading optree-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (47 kB)\n",
            "Collecting opt_einsum>=3.3.0 (from lightning-thunder>=0.2.0.dev20250119->litgpt==0.5.7->litgpt[all]==0.5.7)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4.0 in /opt/conda/lib/python3.12/site-packages (from lightning-thunder>=0.2.0.dev20250119->litgpt==0.5.7->litgpt[all]==0.5.7) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2.5.0.post0) (75.8.0)\n",
            "Collecting fastapi>=0.100 (from litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: httpx in /opt/conda/lib/python3.12/site-packages (from litserve<=0.2.4->litgpt[all]==0.5.7) (0.28.1)\n",
            "Collecting uvicorn>=0.29.0 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting accelerate>=0.26.0 (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading accelerate-1.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting jsonlines (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /opt/conda/lib/python3.12/site-packages (from lm-eval>=0.4.2->litgpt[all]==0.5.7) (2.10.2)\n",
            "Collecting peft>=0.2.0 (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pybind11>=2.6.2 (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hCollecting sacrebleu>=1.5.0 (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /opt/conda/lib/python3.12/site-packages (from lm-eval>=0.4.2->litgpt[all]==0.5.7) (1.6.1)\n",
            "Collecting sqlitedict (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hCollecting tqdm-multiprocess (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting word2number (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25hCollecting more_itertools (from lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading more_itertools-10.6.0-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.9.0->litgpt[all]==0.5.7) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.9.0->litgpt[all]==0.5.7) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.9.0->litgpt[all]==0.5.7) (2025.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->litgpt[all]==0.5.7) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->litgpt[all]==0.5.7) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->litgpt[all]==0.5.7) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.31.0->litgpt[all]==0.5.7) (2024.12.14)\n",
            "Collecting absl-py>=0.4 (from tensorboard>=2.14.0->litgpt[all]==0.5.7)\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard>=2.14.0->litgpt[all]==0.5.7)\n",
            "  Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard>=2.14.0->litgpt[all]==0.5.7)\n",
            "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.12/site-packages (from tensorboard>=2.14.0->litgpt[all]==0.5.7) (5.28.3)\n",
            "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.12/site-packages (from tensorboard>=2.14.0->litgpt[all]==0.5.7) (1.17.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.14.0->litgpt[all]==0.5.7)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard>=2.14.0->litgpt[all]==0.5.7)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch<2.6.0,>=2.5.0->litgpt==0.5.7->litgpt[all]==0.5.7) (1.13.1)\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from accelerate>=0.26.0->lm-eval>=0.4.2->litgpt[all]==0.5.7) (6.1.1)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7) (25.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.18.0->litgpt[all]==0.5.7)\n",
            "  Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi>=0.100->litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.12/site-packages (from fastapi>=0.100->litserve<=0.2.4->litgpt[all]==0.5.7) (2.10.6)\n",
            "Collecting nltk (from rouge-score>=0.0.4->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: colorama in /opt/conda/lib/python3.12/site-packages (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]==0.5.7) (0.4.6)\n",
            "Collecting lxml (from sacrebleu>=1.5.0->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading lxml-5.3.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]==0.5.7) (1.15.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]==0.5.7) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn>=0.24.1->lm-eval>=0.4.2->litgpt[all]==0.5.7) (3.5.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<=4.32.1,>=4.30.1->litgpt==0.5.7->litgpt[all]==0.5.7) (6.5.2)\n",
            "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.12/site-packages (from uvicorn>=0.29.0->uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.12/site-packages (from uvicorn>=0.29.0->uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7) (0.14.0)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading watchfiles-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.29.0->litserve<=0.2.4->litgpt[all]==0.5.7)\n",
            "  Downloading websockets-15.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.14.0->litgpt[all]==0.5.7) (3.0.2)\n",
            "Collecting botocore<1.37.0,>=1.36.22 (from boto3->litdata==0.2.17->litgpt[all]==0.5.7)\n",
            "  Downloading botocore-1.36.22-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->litdata==0.2.17->litgpt[all]==0.5.7)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3->litdata==0.2.17->litgpt[all]==0.5.7)\n",
            "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx->litserve<=0.2.4->litgpt[all]==0.5.7) (4.8.0)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx->litserve<=0.2.4->litgpt[all]==0.5.7) (1.0.7)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval>=0.4.2->litgpt[all]==0.5.7)\n",
            "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100->litserve<=0.2.4->litgpt[all]==0.5.7) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.100->litserve<=0.2.4->litgpt[all]==0.5.7) (2.27.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio->httpx->litserve<=0.2.4->litgpt[all]==0.5.7) (1.3.1)\n",
            "Downloading litgpt-0.5.7-py3-none-any.whl (176 kB)\n",
            "Downloading litdata-0.2.17-py3-none-any.whl (125 kB)\n",
            "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.3/815.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "Downloading jsonargparse-4.32.1-py3-none-any.whl (207 kB)\n",
            "Downloading lightning_thunder-0.2.2.dev20250216-py3-none-any.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.4/856.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading litserve-0.2.4-py3-none-any.whl (43 kB)\n",
            "Downloading lm_eval-0.4.7-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
            "Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
            "Downloading aiohttp-3.11.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "Downloading grpcio-1.70.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
            "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
            "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading optree-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (400 kB)\n",
            "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "Downloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "Downloading boto3-1.36.22-py3-none-any.whl (139 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading more_itertools-10.6.0-py3-none-any.whl (63 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading botocore-1.36.22-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
            "Downloading httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading propcache-0.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading watchfiles-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "Downloading websockets-15.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\n",
            "Downloading yarl-1.18.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
            "Downloading lxml-5.3.1-cp312-cp312-manylinux_2_28_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
            "Building wheels for collected packages: rouge-score, sqlitedict, word2number\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=71a33fce5d20238bbb4cf60edff8d38be133f83abf66d3a3376aa76eedb7a76c\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16928 sha256=bf6669b3b3054b70fa5ae20bff5e2835654af7db6b531dd074e4c156e6d115df\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/7a/6f/21/fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5625 sha256=b3313aa7e3013ef7018cc02db0d7420dd46c680aee277d07abc2852185dce72d\n",
            "  Stored in directory: /home/jovyan/.cache/pip/wheels/5b/79/fb/d25928e599c7e11fe4e00d32048cd74933f34a74c633d2aea6\n",
            "Successfully built rouge-score sqlitedict word2number\n",
            "Installing collected packages: word2number, sqlitedict, sentencepiece, looseversion, xxhash, werkzeug, websockets, uvloop, uvicorn, typeshed-client, tqdm-multiprocess, tensorboard-data-server, tcolorpy, tabulate, safetensors, regex, python-dotenv, pybind11, propcache, portalocker, pathvalidate, optree, opt_einsum, numpy, multidict, more_itertools, markdown, lxml, lightning-utilities, jsonlines, jsonargparse, jmespath, httptools, hf-transfer, grpcio, frozenlist, docstring-parser, dill, chardet, aiohappyeyeballs, absl-py, yarl, watchfiles, tensorboard, starlette, sacrebleu, nltk, multiprocess, mbstrdecoder, huggingface_hub, botocore, aiosignal, typepy, tokenizers, s3transfer, rouge-score, fastapi, aiohttp, transformers, torchmetrics, litserve, lightning-thunder, boto3, bitsandbytes, accelerate, pytorch-lightning, peft, litdata, datasets, DataProperty, tabledata, lightning, evaluate, pytablewriter, litgpt, lm-eval\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.9\n",
            "    Uninstalling dill-0.3.9:\n",
            "      Successfully uninstalled dill-0.3.9\n",
            "Successfully installed DataProperty-1.1.0 absl-py-2.1.0 accelerate-1.4.0 aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 bitsandbytes-0.44.1 boto3-1.36.22 botocore-1.36.22 chardet-5.2.0 datasets-3.3.1 dill-0.3.8 docstring-parser-0.16 evaluate-0.4.3 fastapi-0.115.8 frozenlist-1.5.0 grpcio-1.70.0 hf-transfer-0.1.9 httptools-0.6.4 huggingface_hub-0.28.1 jmespath-1.0.1 jsonargparse-4.32.1 jsonlines-4.0.0 lightning-2.5.0 lightning-thunder-0.2.2.dev20250216 lightning-utilities-0.12.0 litdata-0.2.17 litgpt-0.5.7 litserve-0.2.4 lm-eval-0.4.7 looseversion-1.3.0 lxml-5.3.1 markdown-3.7 mbstrdecoder-1.1.4 more_itertools-10.6.0 multidict-6.1.0 multiprocess-0.70.16 nltk-3.9.1 numpy-1.26.4 opt_einsum-3.4.0 optree-0.14.0 pathvalidate-3.2.3 peft-0.14.0 portalocker-3.1.1 propcache-0.2.1 pybind11-2.13.6 pytablewriter-1.2.1 python-dotenv-1.0.1 pytorch-lightning-2.5.0.post0 regex-2024.11.6 rouge-score-0.1.2 s3transfer-0.11.2 sacrebleu-2.5.1 safetensors-0.5.2 sentencepiece-0.2.0 sqlitedict-2.1.0 starlette-0.45.3 tabledata-1.3.4 tabulate-0.9.0 tcolorpy-0.1.7 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tokenizers-0.21.0 torchmetrics-1.6.1 tqdm-multiprocess-0.0.11 transformers-4.47.1 typepy-1.3.4 typeshed-client-2.7.0 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4 websockets-15.0 werkzeug-3.1.3 word2number-1.1 xxhash-3.5.0 yarl-1.18.3\n"
          ]
        }
      ],
      "source": [
        "!pip install 'litgpt[all]'==0.5.7 'lightning<2.5.0.post0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqOcr9Ji04k-"
      },
      "source": [
        "then, download the foundation models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LY7SQb604k-",
        "outputId": "1e60796f-9527-45e6-b73c-53949a6f7866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100%|█████████████████████████████| 560/560 [00:00<00:00, 5.67MB/s]\n",
            "generation_config.json: 100%|██████████████████| 129/129 [00:00<00:00, 1.07MB/s]\n",
            "pytorch_model.bin: 100%|███████████████████▉| 4.40G/4.40G [00:04<00:00, 957MB/s]\n",
            "tokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 12.5MB/s]\n",
            "tokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 47.7MB/s]\n",
            "tokenizer_config.json: 100%|███████████████████| 776/776 [00:00<00:00, 5.90MB/s]\n",
            "Converting checkpoint files to LitGPT format.\n",
            "{'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\n",
            " 'debug_mode': False,\n",
            " 'dtype': None,\n",
            " 'model_name': None}\n",
            "Loading weights: pytorch_model.bin: 100%|███████████████| 00:07<00:00, 13.98it/s\n",
            "Saving converted checkpoint to checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n"
          ]
        }
      ],
      "source": [
        "!litgpt download TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83zezx5j04k-",
        "outputId": "4d170c0e-571f-4f3d-d077-db6e4e1a8bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100%|█████████████████████████████| 506/506 [00:00<00:00, 6.63MB/s]\n",
            "generation_config.json: 100%|██████████████████| 137/137 [00:00<00:00, 1.07MB/s]\n",
            "pytorch_model.bin: 100%|██████████████████▉| 6.85G/6.85G [00:06<00:00, 1.03GB/s]\n",
            "tokenizer.model: 100%|███████████████████████| 534k/534k [00:00<00:00, 47.5MB/s]\n",
            "tokenizer_config.json: 100%|███████████████████| 593/593 [00:00<00:00, 4.78MB/s]\n",
            "Converting checkpoint files to LitGPT format.\n",
            "{'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_3b'),\n",
            " 'debug_mode': False,\n",
            " 'dtype': None,\n",
            " 'model_name': None}\n",
            "Loading weights: pytorch_model.bin: 100%|███████████████| 00:09<00:00, 10.61it/s\n",
            "Saving converted checkpoint to checkpoints/openlm-research/open_llama_3b\n"
          ]
        }
      ],
      "source": [
        "!litgpt download openlm-research/open_llama_3b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My4WdJN804k-",
        "outputId": "cc32aa1c-3088-448a-ddbc-63ddbe56e791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100%|█████████████████████████████| 507/507 [00:00<00:00, 2.91MB/s]\n",
            "generation_config.json: 100%|██████████████████| 137/137 [00:00<00:00, 1.40MB/s]\n",
            "pytorch_model-00001-of-00002.bin: 100%|███▉| 9.98G/9.98G [00:09<00:00, 1.05GB/s]\n",
            "pytorch_model-00002-of-00002.bin: 100%|████▉| 3.50G/3.50G [00:04<00:00, 859MB/s]\n",
            "pytorch_model.bin.index.json: 100%|████████| 26.8k/26.8k [00:00<00:00, 67.2MB/s]\n",
            "tokenizer.model: 100%|███████████████████████| 534k/534k [00:00<00:00, 45.3MB/s]\n",
            "tokenizer_config.json: 100%|███████████████████| 593/593 [00:00<00:00, 4.57MB/s]\n",
            "Converting checkpoint files to LitGPT format.\n",
            "{'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_7b'),\n",
            " 'debug_mode': False,\n",
            " 'dtype': None,\n",
            " 'model_name': None}\n",
            "Loading weights: pytorch_model-00002-of-00002.bin: 100%|█| 00:18<00:00,  5.46it/\n",
            "Saving converted checkpoint to checkpoints/openlm-research/open_llama_7b\n"
          ]
        }
      ],
      "source": [
        "!litgpt download openlm-research/open_llama_7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Wo_-4D04k_",
        "outputId": "190883ad-a788-4db3-bda0-66cac0451cc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting HF_HUB_ENABLE_HF_TRANSFER=1\n",
            "config.json: 100%|█████████████████████████████| 507/507 [00:00<00:00, 2.60MB/s]\n",
            "generation_config.json: 100%|███████████████████| 137/137 [00:00<00:00, 851kB/s]\n",
            "pytorch_model-00001-of-00003.bin: 100%|███▉| 9.95G/9.95G [00:09<00:00, 1.07GB/s]\n",
            "pytorch_model-00002-of-00003.bin: 100%|███▉| 9.90G/9.90G [00:09<00:00, 1.06GB/s]\n",
            "pytorch_model-00003-of-00003.bin: 100%|████▉| 6.18G/6.18G [00:06<00:00, 993MB/s]\n",
            "pytorch_model.bin.index.json: 100%|█████████| 33.4k/33.4k [00:00<00:00, 103MB/s]\n",
            "tokenizer.model: 100%|███████████████████████| 534k/534k [00:00<00:00, 47.4MB/s]\n",
            "tokenizer_config.json: 100%|███████████████████| 593/593 [00:00<00:00, 6.31MB/s]\n",
            "Converting checkpoint files to LitGPT format.\n",
            "{'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_13b'),\n",
            " 'debug_mode': False,\n",
            " 'dtype': None,\n",
            " 'model_name': None}\n",
            "Loading weights: pytorch_model-00003-of-00003.bin: 100%|█| 00:35<00:00,  2.79it/\n",
            "Saving converted checkpoint to checkpoints/openlm-research/open_llama_13b\n"
          ]
        }
      ],
      "source": [
        "!litgpt download openlm-research/open_llama_13b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXev3-Dg04k_"
      },
      "source": [
        "Also, get the “recipes” that we will use for LLM fine-tuning. Using the file browser on the left side, look at the contents of the “config” directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VaNbBYI04k_",
        "outputId": "8dfcf39c-cce1-4ae1-a24c-414a3d63475d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-chi'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 122 (delta 76), reused 71 (delta 32), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (122/122), 42.65 KiB | 2.67 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/teaching-on-testbeds/llm-chi/\n",
        "!mv llm-chi/workspace/config ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Heb7oIb04k_"
      },
      "source": [
        "### Experiment: Baseline\n",
        "\n",
        "As a baseline, let’s try an epoch of fine-tuning the TinyLlama-1.1B, using full precision and a batch size of 32:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9hidfTt04k_",
        "outputId": "d812cfda-21e9-43e0-ec45-b3ce78e6fc31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7bf454d93f80>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-tiny-llama-1.1b'),\r\n",
            " 'precision': '32-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=32,\r\n",
            "                    micro_batch_size=32,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "README.md: 100%|██████████████████████████████| 28.0/28.0 [00:00<00:00, 300kB/s]\n",
            "alpaca_2000.parquet: 100%|█████████████████| 1.76M/1.76M [00:00<00:00, 92.8MB/s]\n",
            "Generating train split: 100%|████| 2000/2000 [00:00<00:00, 105574.18 examples/s]\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 1,100,048,384\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 1 | loss train: 1.830, val: n/a | iter time: 1624.99 ms (step)\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/bin/litgpt\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/__main__.py\", line 71, in main\n",
            "    CLI(parser_data)\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/jsonargparse/_cli.py\", line 119, in CLI\n",
            "    return _run_component(component, init.get(subcommand))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/jsonargparse/_cli.py\", line 204, in _run_component\n",
            "    return component(**cfg)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/finetune/full.py\", line 122, in setup\n",
            "    fabric.launch(main, devices, resume, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/fabric.py\", line 837, in launch\n",
            "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/fabric.py\", line 923, in _wrap_and_launch\n",
            "    return to_run(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/fabric.py\", line 928, in _wrap_with_setup\n",
            "    return to_run(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/finetune/full.py\", line 171, in main\n",
            "    token_counts = fit(fabric, state, train_dataloader, val_dataloader, devices, resume, checkpoint_dir, out_dir, train, eval, data)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/finetune/full.py\", line 264, in fit\n",
            "    logits = model(input_ids)\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/wrappers.py\", line 136, in forward\n",
            "    output = self._forward_module(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/model.py\", line 161, in forward\n",
            "    x = block(x, cos, sin, mask, input_pos, input_pos_maxp1)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/model.py\", line 322, in forward\n",
            "    return self.post_mlp_norm(self.mlp(x_normed)) + x\n",
            "                              ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/model.py\", line 547, in forward\n",
            "    x_fc_1 = self.fc_1(x)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/nn/modules/linear.py\", line 125, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 44.75 MiB is free. Process 17196 has 79.20 GiB memory in use. Of the allocated memory 70.81 GiB is allocated by PyTorch, and 7.90 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/tiny-llama-full.yaml --train.global_batch_size 32 --train.micro_batch_size 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYTC4bkT04k_"
      },
      "source": [
        "This will fail because the training job won’t fit in our 80GB GPU memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAZz_j5v04k_"
      },
      "source": [
        "### Experiment: Reduced batch size\n",
        "\n",
        "But with a smaller batch size, it fits easily:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T403oO0a04k_",
        "outputId": "4237db68-622e-4104-8f53-9f3310476c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7e4eb5922960>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-tiny-llama-1.1b'),\r\n",
            " 'precision': '32-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=8,\r\n",
            "                    micro_batch_size=8,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 1,100,048,384\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 1 | loss train: 1.704, val: n/a | iter time: 505.42 ms (step)\n",
            "Epoch 1 | iter 2 step 2 | loss train: 2.023, val: n/a | iter time: 207.60 ms (step)\n",
            "Epoch 1 | iter 3 step 3 | loss train: 1.923, val: n/a | iter time: 352.72 ms (step)\n",
            "Epoch 1 | iter 4 step 4 | loss train: 1.409, val: n/a | iter time: 422.64 ms (step)\n",
            "Epoch 1 | iter 5 step 5 | loss train: 1.254, val: n/a | iter time: 327.99 ms (step)\n",
            "Epoch 1 | iter 6 step 6 | loss train: 1.391, val: n/a | iter time: 421.04 ms (step)\n",
            "Epoch 1 | iter 7 step 7 | loss train: 1.104, val: n/a | iter time: 422.03 ms (step)\n",
            "Epoch 1 | iter 8 step 8 | loss train: 1.126, val: n/a | iter time: 423.31 ms (step)\n",
            "Epoch 1 | iter 9 step 9 | loss train: 1.011, val: n/a | iter time: 344.45 ms (step)\n",
            "Epoch 1 | iter 10 step 10 | loss train: 1.131, val: n/a | iter time: 350.12 ms (step)\n",
            "Epoch 1 | iter 11 step 11 | loss train: 0.928, val: n/a | iter time: 396.97 ms (step)\n",
            "Epoch 1 | iter 12 step 12 | loss train: 1.144, val: n/a | iter time: 424.31 ms (step)\n",
            "Epoch 1 | iter 13 step 13 | loss train: 1.110, val: n/a | iter time: 424.11 ms (step)\n",
            "Epoch 1 | iter 14 step 14 | loss train: 1.104, val: n/a | iter time: 405.23 ms (step)\n",
            "Epoch 1 | iter 15 step 15 | loss train: 1.035, val: n/a | iter time: 424.71 ms (step)\n",
            "Epoch 1 | iter 16 step 16 | loss train: 0.964, val: n/a | iter time: 229.39 ms (step)\n",
            "Epoch 1 | iter 17 step 17 | loss train: 1.218, val: n/a | iter time: 426.33 ms (step)\n",
            "Epoch 1 | iter 18 step 18 | loss train: 0.845, val: n/a | iter time: 423.30 ms (step)\n",
            "Epoch 1 | iter 19 step 19 | loss train: 1.101, val: n/a | iter time: 422.40 ms (step)\n",
            "Epoch 1 | iter 20 step 20 | loss train: 1.089, val: n/a | iter time: 377.18 ms (step)\n",
            "Epoch 1 | iter 21 step 21 | loss train: 1.056, val: n/a | iter time: 419.94 ms (step)\n",
            "Epoch 1 | iter 22 step 22 | loss train: 1.042, val: n/a | iter time: 300.58 ms (step)\n",
            "Epoch 1 | iter 23 step 23 | loss train: 1.207, val: n/a | iter time: 309.49 ms (step)\n",
            "Epoch 1 | iter 24 step 24 | loss train: 1.139, val: n/a | iter time: 406.71 ms (step)\n",
            "Epoch 1 | iter 25 step 25 | loss train: 1.014, val: n/a | iter time: 428.20 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"Comparison of being a human to a robot: The first major difference between humans and robots is that humans can do things that robots can't, such as think, reason, and communicate, and there are still many things that are better/easier/different/more difficult for robots to do. The second major difference between humans and robots is that humans can have feelings, while robots can't. However, it's interesting to note that despite\n",
            "\n",
            "iter 25: val loss 1.1026, val time: 13041.23 ms\n",
            "Epoch 1 | iter 26 step 26 | loss train: 1.177, val: 1.103 | iter time: 395.27 ms (step)\n",
            "Epoch 1 | iter 27 step 27 | loss train: 1.079, val: 1.103 | iter time: 428.50 ms (step)\n",
            "Epoch 1 | iter 28 step 28 | loss train: 1.107, val: 1.103 | iter time: 433.31 ms (step)\n",
            "Epoch 1 | iter 29 step 29 | loss train: 1.083, val: 1.103 | iter time: 427.45 ms (step)\n",
            "Epoch 1 | iter 30 step 30 | loss train: 1.035, val: 1.103 | iter time: 422.07 ms (step)\n",
            "Epoch 1 | iter 31 step 31 | loss train: 1.200, val: 1.103 | iter time: 432.99 ms (step)\n",
            "Epoch 1 | iter 32 step 32 | loss train: 1.102, val: 1.103 | iter time: 427.20 ms (step)\n",
            "Epoch 1 | iter 33 step 33 | loss train: 1.133, val: 1.103 | iter time: 429.35 ms (step)\n",
            "Epoch 1 | iter 34 step 34 | loss train: 1.266, val: 1.103 | iter time: 421.14 ms (step)\n",
            "Epoch 1 | iter 35 step 35 | loss train: 1.097, val: 1.103 | iter time: 352.50 ms (step)\n",
            "Epoch 1 | iter 36 step 36 | loss train: 1.073, val: 1.103 | iter time: 418.24 ms (step)\n",
            "Epoch 1 | iter 37 step 37 | loss train: 1.165, val: 1.103 | iter time: 363.05 ms (step)\n",
            "Epoch 1 | iter 38 step 38 | loss train: 1.098, val: 1.103 | iter time: 420.64 ms (step)\n",
            "Epoch 1 | iter 39 step 39 | loss train: 1.075, val: 1.103 | iter time: 390.90 ms (step)\n",
            "Epoch 1 | iter 40 step 40 | loss train: 1.072, val: 1.103 | iter time: 371.01 ms (step)\n",
            "Epoch 1 | iter 41 step 41 | loss train: 1.211, val: 1.103 | iter time: 315.70 ms (step)\n",
            "Epoch 1 | iter 42 step 42 | loss train: 1.023, val: 1.103 | iter time: 430.66 ms (step)\n",
            "Epoch 1 | iter 43 step 43 | loss train: 1.263, val: 1.103 | iter time: 432.27 ms (step)\n",
            "Epoch 1 | iter 44 step 44 | loss train: 1.165, val: 1.103 | iter time: 309.63 ms (step)\n",
            "Epoch 1 | iter 45 step 45 | loss train: 1.189, val: 1.103 | iter time: 391.15 ms (step)\n",
            "Epoch 1 | iter 46 step 46 | loss train: 1.311, val: 1.103 | iter time: 385.95 ms (step)\n",
            "Epoch 1 | iter 47 step 47 | loss train: 1.273, val: 1.103 | iter time: 430.57 ms (step)\n",
            "Epoch 1 | iter 48 step 48 | loss train: 1.617, val: 1.103 | iter time: 435.97 ms (step)\n",
            "Epoch 1 | iter 49 step 49 | loss train: 1.313, val: 1.103 | iter time: 436.02 ms (step)\n",
            "Epoch 1 | iter 50 step 50 | loss train: 1.252, val: 1.103 | iter time: 321.33 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"Beast of a Winter\"\n",
            "\n",
            "iter 50: val loss 1.2348, val time: 11624.64 ms\n",
            "Epoch 1 | iter 51 step 51 | loss train: 1.301, val: 1.235 | iter time: 425.71 ms (step)\n",
            "Epoch 1 | iter 52 step 52 | loss train: 1.340, val: 1.235 | iter time: 404.86 ms (step)\n",
            "Epoch 1 | iter 53 step 53 | loss train: 1.338, val: 1.235 | iter time: 435.18 ms (step)\n",
            "Epoch 1 | iter 54 step 54 | loss train: 1.271, val: 1.235 | iter time: 361.62 ms (step)\n",
            "Epoch 1 | iter 55 step 55 | loss train: 0.963, val: 1.235 | iter time: 226.80 ms (step)\n",
            "Epoch 1 | iter 56 step 56 | loss train: 1.124, val: 1.235 | iter time: 323.32 ms (step)\n",
            "Epoch 1 | iter 57 step 57 | loss train: 1.337, val: 1.235 | iter time: 326.39 ms (step)\n",
            "Epoch 1 | iter 58 step 58 | loss train: 1.329, val: 1.235 | iter time: 408.44 ms (step)\n",
            "Epoch 1 | iter 59 step 59 | loss train: 1.060, val: 1.235 | iter time: 323.48 ms (step)\n",
            "Epoch 1 | iter 60 step 60 | loss train: 1.466, val: 1.235 | iter time: 435.63 ms (step)\n",
            "Epoch 1 | iter 61 step 61 | loss train: 1.369, val: 1.235 | iter time: 326.04 ms (step)\n",
            "Epoch 1 | iter 62 step 62 | loss train: 1.394, val: 1.235 | iter time: 435.92 ms (step)\n",
            "Epoch 1 | iter 63 step 63 | loss train: 1.187, val: 1.235 | iter time: 403.52 ms (step)\n",
            "Epoch 1 | iter 64 step 64 | loss train: 1.505, val: 1.235 | iter time: 405.64 ms (step)\n",
            "Epoch 1 | iter 65 step 65 | loss train: 1.516, val: 1.235 | iter time: 394.25 ms (step)\n",
            "Epoch 1 | iter 66 step 66 | loss train: 1.456, val: 1.235 | iter time: 371.15 ms (step)\n",
            "Epoch 1 | iter 67 step 67 | loss train: 1.424, val: 1.235 | iter time: 427.76 ms (step)\n",
            "Epoch 1 | iter 68 step 68 | loss train: 1.547, val: 1.235 | iter time: 437.15 ms (step)\n",
            "Epoch 1 | iter 69 step 69 | loss train: 1.230, val: 1.235 | iter time: 333.95 ms (step)\n",
            "Epoch 1 | iter 70 step 70 | loss train: 1.405, val: 1.235 | iter time: 270.06 ms (step)\n",
            "Epoch 1 | iter 71 step 71 | loss train: 1.215, val: 1.235 | iter time: 308.83 ms (step)\n",
            "Epoch 1 | iter 72 step 72 | loss train: 1.319, val: 1.235 | iter time: 425.78 ms (step)\n",
            "Epoch 1 | iter 73 step 73 | loss train: 1.360, val: 1.235 | iter time: 388.05 ms (step)\n",
            "Epoch 1 | iter 74 step 74 | loss train: 1.484, val: 1.235 | iter time: 435.87 ms (step)\n",
            "Epoch 1 | iter 75 step 75 | loss train: 1.503, val: 1.235 | iter time: 435.87 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " From the ground up, to the stars, to the future, to the present: Here are some intricately detailed, natural forces of nature in action. #Videosources: Fusion1, Nature, Earth. #Photosources: NASA, NASA-Mapping, Earthlink\n",
            "\n",
            "Created by: Earthlink\n",
            "\n",
            "iter 75: val loss 1.4216, val time: 12659.57 ms\n",
            "Epoch 1 | iter 76 step 76 | loss train: 1.150, val: 1.422 | iter time: 350.99 ms (step)\n",
            "Epoch 1 | iter 77 step 77 | loss train: 1.583, val: 1.422 | iter time: 348.98 ms (step)\n",
            "Epoch 1 | iter 78 step 78 | loss train: 1.510, val: 1.422 | iter time: 435.47 ms (step)\n",
            "Epoch 1 | iter 79 step 79 | loss train: 1.259, val: 1.422 | iter time: 285.32 ms (step)\n",
            "Epoch 1 | iter 80 step 80 | loss train: 1.542, val: 1.422 | iter time: 428.33 ms (step)\n",
            "Epoch 1 | iter 81 step 81 | loss train: 1.074, val: 1.422 | iter time: 240.21 ms (step)\n",
            "Epoch 1 | iter 82 step 82 | loss train: 1.610, val: 1.422 | iter time: 351.69 ms (step)\n",
            "Epoch 1 | iter 83 step 83 | loss train: 1.208, val: 1.422 | iter time: 416.51 ms (step)\n",
            "Epoch 1 | iter 84 step 84 | loss train: 1.570, val: 1.422 | iter time: 418.55 ms (step)\n",
            "Epoch 1 | iter 85 step 85 | loss train: 1.460, val: 1.422 | iter time: 288.98 ms (step)\n",
            "Epoch 1 | iter 86 step 86 | loss train: 1.502, val: 1.422 | iter time: 432.69 ms (step)\n",
            "Epoch 1 | iter 87 step 87 | loss train: 1.437, val: 1.422 | iter time: 371.88 ms (step)\n",
            "Epoch 1 | iter 88 step 88 | loss train: 1.826, val: 1.422 | iter time: 333.05 ms (step)\n",
            "Epoch 1 | iter 89 step 89 | loss train: 1.430, val: 1.422 | iter time: 287.79 ms (step)\n",
            "Epoch 1 | iter 90 step 90 | loss train: 1.690, val: 1.422 | iter time: 425.78 ms (step)\n",
            "Epoch 1 | iter 91 step 91 | loss train: 1.532, val: 1.422 | iter time: 433.37 ms (step)\n",
            "Epoch 1 | iter 92 step 92 | loss train: 1.332, val: 1.422 | iter time: 323.05 ms (step)\n",
            "Epoch 1 | iter 93 step 93 | loss train: 1.667, val: 1.422 | iter time: 315.20 ms (step)\n",
            "Epoch 1 | iter 94 step 94 | loss train: 1.401, val: 1.422 | iter time: 384.89 ms (step)\n",
            "Epoch 1 | iter 95 step 95 | loss train: 1.627, val: 1.422 | iter time: 392.20 ms (step)\n",
            "Epoch 1 | iter 96 step 96 | loss train: 1.604, val: 1.422 | iter time: 435.97 ms (step)\n",
            "Epoch 1 | iter 97 step 97 | loss train: 1.835, val: 1.422 | iter time: 427.03 ms (step)\n",
            "Epoch 1 | iter 98 step 98 | loss train: 1.671, val: 1.422 | iter time: 432.51 ms (step)\n",
            "Epoch 1 | iter 99 step 99 | loss train: 1.683, val: 1.422 | iter time: 388.41 ms (step)\n",
            "Epoch 1 | iter 100 step 100 | loss train: 1.197, val: 1.422 | iter time: 434.24 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"How to Save Money: Crafting a Tasteful Photoshop Design\"\n",
            "\n",
            "iter 100: val loss 1.6197, val time: 11879.46 ms\n",
            "Saving checkpoint to 'out/finetune/full-tiny-llama-1.1b/step-000100'\n",
            "Epoch 1 | iter 101 step 101 | loss train: 1.436, val: 1.620 | iter time: 233.89 ms (step)\n",
            "Epoch 1 | iter 102 step 102 | loss train: 1.464, val: 1.620 | iter time: 429.04 ms (step)\n",
            "Epoch 1 | iter 103 step 103 | loss train: 1.904, val: 1.620 | iter time: 368.59 ms (step)\n",
            "Epoch 1 | iter 104 step 104 | loss train: 1.842, val: 1.620 | iter time: 370.58 ms (step)\n",
            "Epoch 1 | iter 105 step 105 | loss train: 1.870, val: 1.620 | iter time: 316.92 ms (step)\n",
            "Epoch 1 | iter 106 step 106 | loss train: 1.634, val: 1.620 | iter time: 424.59 ms (step)\n",
            "Epoch 1 | iter 107 step 107 | loss train: 1.637, val: 1.620 | iter time: 275.40 ms (step)\n",
            "Epoch 1 | iter 108 step 108 | loss train: 1.679, val: 1.620 | iter time: 353.65 ms (step)\n",
            "Epoch 1 | iter 109 step 109 | loss train: 1.800, val: 1.620 | iter time: 433.12 ms (step)\n",
            "Epoch 1 | iter 110 step 110 | loss train: 1.439, val: 1.620 | iter time: 289.98 ms (step)\n",
            "Epoch 1 | iter 111 step 111 | loss train: 1.870, val: 1.620 | iter time: 354.85 ms (step)\n",
            "Epoch 1 | iter 112 step 112 | loss train: 1.700, val: 1.620 | iter time: 432.69 ms (step)\n",
            "Epoch 1 | iter 113 step 113 | loss train: 1.380, val: 1.620 | iter time: 432.86 ms (step)\n",
            "Epoch 1 | iter 114 step 114 | loss train: 1.677, val: 1.620 | iter time: 354.03 ms (step)\n",
            "Epoch 1 | iter 115 step 115 | loss train: 1.526, val: 1.620 | iter time: 431.37 ms (step)\n",
            "Epoch 1 | iter 116 step 116 | loss train: 1.705, val: 1.620 | iter time: 433.56 ms (step)\n",
            "Epoch 1 | iter 117 step 117 | loss train: 1.542, val: 1.620 | iter time: 355.05 ms (step)\n",
            "Epoch 1 | iter 118 step 118 | loss train: 1.501, val: 1.620 | iter time: 407.21 ms (step)\n",
            "Epoch 1 | iter 119 step 119 | loss train: 1.546, val: 1.620 | iter time: 432.93 ms (step)\n",
            "Epoch 1 | iter 120 step 120 | loss train: 1.586, val: 1.620 | iter time: 436.39 ms (step)\n",
            "Epoch 1 | iter 121 step 121 | loss train: 1.341, val: 1.620 | iter time: 205.85 ms (step)\n",
            "Epoch 1 | iter 122 step 122 | loss train: 1.693, val: 1.620 | iter time: 428.07 ms (step)\n",
            "Epoch 1 | iter 123 step 123 | loss train: 1.769, val: 1.620 | iter time: 275.29 ms (step)\n",
            "Epoch 1 | iter 124 step 124 | loss train: 1.557, val: 1.620 | iter time: 433.86 ms (step)\n",
            "Epoch 1 | iter 125 step 125 | loss train: 1.511, val: 1.620 | iter time: 357.52 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"Snowflaws are trending #lifestyle # animals # must # have # year # snags # gone # gone # through # the # all.\"\n",
            "\n",
            "iter 125: val loss 1.6115, val time: 12167.38 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  163087\n",
            "| - Tokens w/ Prompt          :  210770\n",
            "| - Total Tokens (w/ Padding) :  437296\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  164.27 s\n",
            "| - Tok/sec                   :  2661.99 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  30.41 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/tiny-llama-full.yaml --train.global_batch_size 8 --train.micro_batch_size 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "064ff_jh04k_"
      },
      "source": [
        "Make a note of the training time and memory, which is printed at the end of the training job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA_LTDzS04k_"
      },
      "source": [
        "### Experiment: Gradient accumulation\n",
        "\n",
        "By using gradient accumulation to “step” only after a few “micro batches”, we can train with a larger effective “global” batch size, with minimal effect on the memory required:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feXg38Qz04k_",
        "outputId": "9b9ffc29-d45e-4f7a-c6a2-8e8a00052ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x70478e11fb00>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-tiny-llama-1.1b'),\r\n",
            " 'precision': '32-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=32,\r\n",
            "                    micro_batch_size=8,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 1,100,048,384\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.704, val: n/a | iter time: 408.07 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.864, val: n/a | iter time: 170.06 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.949, val: n/a | iter time: 315.94 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.879, val: n/a | iter time: 456.32 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.849, val: n/a | iter time: 285.24 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 1.826, val: n/a | iter time: 386.66 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 1.709, val: n/a | iter time: 388.51 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.718, val: n/a | iter time: 441.07 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.743, val: n/a | iter time: 300.07 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.678, val: n/a | iter time: 314.83 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.656, val: n/a | iter time: 362.62 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.617, val: n/a | iter time: 443.32 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.547, val: n/a | iter time: 381.90 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.548, val: n/a | iter time: 368.80 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.500, val: n/a | iter time: 391.79 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.516, val: n/a | iter time: 243.35 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.522, val: n/a | iter time: 382.79 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.385, val: n/a | iter time: 391.22 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.391, val: n/a | iter time: 391.50 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.327, val: n/a | iter time: 395.88 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.256, val: n/a | iter time: 375.49 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.281, val: n/a | iter time: 263.88 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.273, val: n/a | iter time: 271.18 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.233, val: n/a | iter time: 426.02 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.204, val: n/a | iter time: 386.27 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.210, val: n/a | iter time: 360.51 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.143, val: n/a | iter time: 391.63 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.124, val: n/a | iter time: 446.19 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.150, val: n/a | iter time: 382.33 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.092, val: n/a | iter time: 386.29 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.106, val: n/a | iter time: 394.59 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.107, val: n/a | iter time: 442.46 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.099, val: n/a | iter time: 381.54 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.148, val: n/a | iter time: 383.50 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.127, val: n/a | iter time: 314.67 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.091, val: n/a | iter time: 429.97 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.100, val: n/a | iter time: 314.53 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.066, val: n/a | iter time: 382.22 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.049, val: n/a | iter time: 348.94 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.051, val: n/a | iter time: 382.74 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.048, val: n/a | iter time: 265.41 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.014, val: n/a | iter time: 390.98 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.051, val: n/a | iter time: 392.93 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.081, val: n/a | iter time: 321.99 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.069, val: n/a | iter time: 341.53 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.134, val: n/a | iter time: 346.42 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.126, val: n/a | iter time: 393.59 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.211, val: n/a | iter time: 446.10 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.223, val: n/a | iter time: 385.99 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.193, val: n/a | iter time: 280.81 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.186, val: n/a | iter time: 386.61 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.125, val: n/a | iter time: 416.07 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.121, val: n/a | iter time: 381.90 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.115, val: n/a | iter time: 318.30 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.068, val: n/a | iter time: 186.15 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.016, val: n/a | iter time: 331.78 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.008, val: n/a | iter time: 275.60 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 0.999, val: n/a | iter time: 365.01 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 0.996, val: n/a | iter time: 281.92 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.064, val: n/a | iter time: 442.28 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.079, val: n/a | iter time: 274.00 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.100, val: n/a | iter time: 393.57 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.114, val: n/a | iter time: 363.27 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.107, val: n/a | iter time: 420.22 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.124, val: n/a | iter time: 342.22 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.125, val: n/a | iter time: 330.92 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.166, val: n/a | iter time: 385.25 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.158, val: n/a | iter time: 446.66 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.094, val: n/a | iter time: 282.47 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.099, val: n/a | iter time: 226.80 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.052, val: n/a | iter time: 266.86 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.003, val: n/a | iter time: 436.29 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.033, val: n/a | iter time: 336.93 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.015, val: n/a | iter time: 394.69 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.064, val: n/a | iter time: 393.64 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.028, val: n/a | iter time: 362.43 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 0.996, val: n/a | iter time: 300.34 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 1.010, val: n/a | iter time: 393.14 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.956, val: n/a | iter time: 242.64 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 1.004, val: n/a | iter time: 435.47 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.961, val: n/a | iter time: 188.98 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 1.004, val: n/a | iter time: 308.12 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.983, val: n/a | iter time: 374.86 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 1.017, val: n/a | iter time: 427.94 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.081, val: n/a | iter time: 236.28 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 1.022, val: n/a | iter time: 390.10 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.088, val: n/a | iter time: 329.30 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.090, val: n/a | iter time: 344.31 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 1.083, val: n/a | iter time: 237.98 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.116, val: n/a | iter time: 381.34 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.081, val: n/a | iter time: 391.56 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 0.999, val: n/a | iter time: 332.92 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.043, val: n/a | iter time: 263.54 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 0.989, val: n/a | iter time: 343.21 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.029, val: n/a | iter time: 347.96 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.084, val: n/a | iter time: 445.11 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.106, val: n/a | iter time: 375.28 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.118, val: n/a | iter time: 391.54 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 1.132, val: n/a | iter time: 347.38 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.069, val: n/a | iter time: 442.88 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"Combination of aaaand aeeeee.\"\n",
            "\n",
            "iter 100: val loss 1.0542, val time: 11861.06 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 1.012, val: 1.054 | iter time: 182.46 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.998, val: 1.054 | iter time: 388.89 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.028, val: 1.054 | iter time: 328.65 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.137, val: 1.054 | iter time: 382.68 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.176, val: 1.054 | iter time: 267.54 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.168, val: 1.054 | iter time: 386.50 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.084, val: 1.054 | iter time: 232.69 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 1.043, val: 1.054 | iter time: 363.08 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 1.043, val: 1.054 | iter time: 383.68 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 1.055, val: 1.054 | iter time: 246.86 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 1.099, val: 1.054 | iter time: 314.59 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 1.110, val: 1.054 | iter time: 445.18 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.030, val: 1.054 | iter time: 380.66 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.062, val: 1.054 | iter time: 312.59 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.978, val: 1.054 | iter time: 388.68 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 0.964, val: 1.054 | iter time: 441.99 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 1.000, val: 1.054 | iter time: 304.11 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.961, val: 1.054 | iter time: 361.00 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 0.999, val: 1.054 | iter time: 390.14 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.959, val: 1.054 | iter time: 446.54 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 0.948, val: 1.054 | iter time: 155.01 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.002, val: 1.054 | iter time: 386.65 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.053, val: 1.054 | iter time: 232.60 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.059, val: 1.054 | iter time: 446.13 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.047, val: 1.054 | iter time: 305.76 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  163087\n",
            "| - Tokens w/ Prompt          :  210770\n",
            "| - Total Tokens (w/ Padding) :  437296\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  57.92 s\n",
            "| - Tok/sec                   :  7550.02 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  34.82 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/tiny-llama-full.yaml --train.global_batch_size 32 --train.micro_batch_size 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH2JQFek04lA"
      },
      "source": [
        "Make a note of the training time and memory, which is printed at the end of the training job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZRz0Vka04lA"
      },
      "source": [
        "### Experiment: Reduced precision\n",
        "\n",
        "With a “brain float16” format for numbers, instead of “float32”, we can further reduce the memory required, although this representation is less precise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYdExUpY04lA",
        "outputId": "055df119-c770-4f24-caa0-c16d2552493f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7d07f1790f80>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-tiny-llama-1.1b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=32,\r\n",
            "                    micro_batch_size=8,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 1,100,048,384\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.705, val: n/a | iter time: 343.05 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.865, val: n/a | iter time: 99.31 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.950, val: n/a | iter time: 192.03 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.880, val: n/a | iter time: 281.01 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.850, val: n/a | iter time: 166.80 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 1.827, val: n/a | iter time: 227.05 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 1.710, val: n/a | iter time: 228.80 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.719, val: n/a | iter time: 256.92 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.782, val: n/a | iter time: 178.02 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.749, val: n/a | iter time: 185.22 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.763, val: n/a | iter time: 210.26 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.745, val: n/a | iter time: 259.09 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.686, val: n/a | iter time: 225.35 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.743, val: n/a | iter time: 216.01 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.710, val: n/a | iter time: 230.29 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.799, val: n/a | iter time: 144.14 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.819, val: n/a | iter time: 225.82 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.674, val: n/a | iter time: 229.14 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.731, val: n/a | iter time: 228.93 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.671, val: n/a | iter time: 232.06 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.607, val: n/a | iter time: 221.64 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.653, val: n/a | iter time: 152.16 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.653, val: n/a | iter time: 158.01 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.576, val: n/a | iter time: 247.35 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.528, val: n/a | iter time: 227.30 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.534, val: n/a | iter time: 208.85 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.458, val: n/a | iter time: 229.38 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.439, val: n/a | iter time: 259.43 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.479, val: n/a | iter time: 225.80 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.390, val: n/a | iter time: 225.56 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.351, val: n/a | iter time: 232.11 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.384, val: n/a | iter time: 258.50 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.368, val: n/a | iter time: 226.34 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.386, val: n/a | iter time: 223.36 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.381, val: n/a | iter time: 183.60 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.284, val: n/a | iter time: 251.59 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.282, val: n/a | iter time: 185.77 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.247, val: n/a | iter time: 224.04 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.207, val: n/a | iter time: 206.75 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.209, val: n/a | iter time: 224.47 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.187, val: n/a | iter time: 155.68 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.131, val: n/a | iter time: 229.68 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.143, val: n/a | iter time: 231.13 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.159, val: n/a | iter time: 187.05 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.134, val: n/a | iter time: 204.39 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.191, val: n/a | iter time: 204.43 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.182, val: n/a | iter time: 230.38 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.251, val: n/a | iter time: 260.71 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.261, val: n/a | iter time: 227.69 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.232, val: n/a | iter time: 164.71 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.226, val: n/a | iter time: 227.44 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.172, val: n/a | iter time: 240.49 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.162, val: n/a | iter time: 225.19 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.151, val: n/a | iter time: 188.10 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.100, val: n/a | iter time: 110.62 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.046, val: n/a | iter time: 194.54 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.040, val: n/a | iter time: 165.02 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 1.032, val: n/a | iter time: 211.54 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 1.021, val: n/a | iter time: 164.47 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.082, val: n/a | iter time: 258.88 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.094, val: n/a | iter time: 161.84 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.111, val: n/a | iter time: 230.88 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.124, val: n/a | iter time: 211.29 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.120, val: n/a | iter time: 241.68 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.139, val: n/a | iter time: 203.34 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.139, val: n/a | iter time: 192.95 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.181, val: n/a | iter time: 226.26 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.166, val: n/a | iter time: 261.10 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.099, val: n/a | iter time: 168.13 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.108, val: n/a | iter time: 137.53 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.065, val: n/a | iter time: 155.55 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.024, val: n/a | iter time: 253.32 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.053, val: n/a | iter time: 200.64 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.029, val: n/a | iter time: 232.25 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.069, val: n/a | iter time: 231.16 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.028, val: n/a | iter time: 211.88 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 0.994, val: n/a | iter time: 176.59 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 1.008, val: n/a | iter time: 232.59 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.954, val: n/a | iter time: 147.96 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 1.001, val: n/a | iter time: 254.23 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.967, val: n/a | iter time: 118.39 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 1.013, val: n/a | iter time: 181.00 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.991, val: n/a | iter time: 220.14 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 1.028, val: n/a | iter time: 250.78 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.082, val: n/a | iter time: 145.27 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 1.024, val: n/a | iter time: 230.03 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.098, val: n/a | iter time: 192.48 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.097, val: n/a | iter time: 202.64 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 1.089, val: n/a | iter time: 143.84 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.120, val: n/a | iter time: 223.72 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.084, val: n/a | iter time: 229.66 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 1.003, val: n/a | iter time: 192.36 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.045, val: n/a | iter time: 153.95 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 0.990, val: n/a | iter time: 203.86 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.024, val: n/a | iter time: 206.79 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.079, val: n/a | iter time: 260.63 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.096, val: n/a | iter time: 222.92 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.107, val: n/a | iter time: 230.77 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 1.119, val: n/a | iter time: 205.30 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.052, val: n/a | iter time: 258.48 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"Combination of aaaand aeeehhhh.\"\n",
            "\n",
            "iter 100: val loss 1.0460, val time: 7544.98 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 1.009, val: 1.046 | iter time: 113.02 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.991, val: 1.046 | iter time: 230.25 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.024, val: 1.046 | iter time: 191.98 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.139, val: 1.046 | iter time: 220.61 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.167, val: 1.046 | iter time: 157.81 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.164, val: 1.046 | iter time: 229.09 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.081, val: 1.046 | iter time: 142.91 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 1.037, val: 1.046 | iter time: 211.85 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 1.037, val: 1.046 | iter time: 227.07 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 1.051, val: 1.046 | iter time: 150.41 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 1.093, val: 1.046 | iter time: 185.02 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 1.100, val: 1.046 | iter time: 258.87 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.024, val: 1.046 | iter time: 225.84 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.051, val: 1.046 | iter time: 184.15 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.970, val: 1.046 | iter time: 231.50 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 0.959, val: 1.046 | iter time: 258.46 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.990, val: 1.046 | iter time: 180.96 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.951, val: 1.046 | iter time: 210.08 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 0.979, val: 1.046 | iter time: 229.36 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.931, val: 1.046 | iter time: 260.61 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 0.918, val: 1.046 | iter time: 89.61 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 0.969, val: 1.046 | iter time: 228.23 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.027, val: 1.046 | iter time: 141.51 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.034, val: 1.046 | iter time: 259.09 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.022, val: 1.046 | iter time: 181.87 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  163087\n",
            "| - Tokens w/ Prompt          :  210770\n",
            "| - Total Tokens (w/ Padding) :  437296\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  35.61 s\n",
            "| - Tok/sec                   :  12279.05 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  20.10 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/tiny-llama-full.yaml --train.global_batch_size 32 --train.micro_batch_size 8 --precision bf16-true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJm70AcL04lA"
      },
      "source": [
        "Make a note of the training time and memory, which is printed at the end of the training job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oykZCPrE04lA"
      },
      "source": [
        "### Experiment: Mixed precision\n",
        "\n",
        "With mixed precision, we get back some of the lost precision in the results, at the cost of some additional memory and time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu4LwBd904lA",
        "outputId": "168b9e6c-0ea7-4454-ae75-c129ed679c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x77e037f26960>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-tiny-llama-1.1b'),\r\n",
            " 'precision': 'bf16-mixed',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=32,\r\n",
            "                    micro_batch_size=8,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 1,100,048,384\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.704, val: n/a | iter time: 376.14 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.864, val: n/a | iter time: 118.63 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.949, val: n/a | iter time: 209.19 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.879, val: n/a | iter time: 326.03 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.850, val: n/a | iter time: 182.79 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 1.826, val: n/a | iter time: 246.61 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 1.710, val: n/a | iter time: 247.18 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.718, val: n/a | iter time: 300.27 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.743, val: n/a | iter time: 192.21 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.678, val: n/a | iter time: 203.14 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.657, val: n/a | iter time: 228.53 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.617, val: n/a | iter time: 300.45 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.548, val: n/a | iter time: 239.54 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.548, val: n/a | iter time: 234.81 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.500, val: n/a | iter time: 247.67 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.516, val: n/a | iter time: 185.63 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.522, val: n/a | iter time: 241.14 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.385, val: n/a | iter time: 248.16 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.391, val: n/a | iter time: 245.53 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.327, val: n/a | iter time: 272.86 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.256, val: n/a | iter time: 237.84 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.281, val: n/a | iter time: 171.23 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.273, val: n/a | iter time: 176.50 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.233, val: n/a | iter time: 288.67 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.204, val: n/a | iter time: 242.15 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.210, val: n/a | iter time: 227.15 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.143, val: n/a | iter time: 246.71 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.124, val: n/a | iter time: 302.91 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.150, val: n/a | iter time: 240.16 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.092, val: n/a | iter time: 246.64 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.106, val: n/a | iter time: 251.52 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.106, val: n/a | iter time: 300.09 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.099, val: n/a | iter time: 240.54 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.148, val: n/a | iter time: 242.54 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.128, val: n/a | iter time: 203.17 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.091, val: n/a | iter time: 292.86 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.100, val: n/a | iter time: 199.73 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.065, val: n/a | iter time: 243.24 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.049, val: n/a | iter time: 225.73 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.051, val: n/a | iter time: 265.96 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.049, val: n/a | iter time: 171.08 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.014, val: n/a | iter time: 249.73 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.052, val: n/a | iter time: 249.06 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.081, val: n/a | iter time: 227.55 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.070, val: n/a | iter time: 218.74 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.135, val: n/a | iter time: 222.15 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.127, val: n/a | iter time: 250.71 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.211, val: n/a | iter time: 302.38 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.223, val: n/a | iter time: 242.35 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.194, val: n/a | iter time: 182.26 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.186, val: n/a | iter time: 244.41 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.124, val: n/a | iter time: 281.06 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.120, val: n/a | iter time: 239.58 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.115, val: n/a | iter time: 207.97 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.068, val: n/a | iter time: 127.73 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.015, val: n/a | iter time: 236.01 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.008, val: n/a | iter time: 178.73 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 0.998, val: n/a | iter time: 230.84 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 0.996, val: n/a | iter time: 184.27 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.064, val: n/a | iter time: 300.43 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.079, val: n/a | iter time: 177.07 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.100, val: n/a | iter time: 250.41 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.114, val: n/a | iter time: 228.63 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.107, val: n/a | iter time: 282.16 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.123, val: n/a | iter time: 219.15 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.126, val: n/a | iter time: 210.88 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.167, val: n/a | iter time: 244.61 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.159, val: n/a | iter time: 303.76 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.095, val: n/a | iter time: 183.18 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.099, val: n/a | iter time: 155.40 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.052, val: n/a | iter time: 174.16 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.004, val: n/a | iter time: 296.18 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.033, val: n/a | iter time: 215.44 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.015, val: n/a | iter time: 249.64 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.064, val: n/a | iter time: 250.31 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.029, val: n/a | iter time: 253.74 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 0.997, val: n/a | iter time: 192.95 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 1.010, val: n/a | iter time: 250.40 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.956, val: n/a | iter time: 166.25 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 1.004, val: n/a | iter time: 296.90 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.961, val: n/a | iter time: 132.24 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 1.004, val: n/a | iter time: 198.44 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.983, val: n/a | iter time: 238.24 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 1.016, val: n/a | iter time: 292.34 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.080, val: n/a | iter time: 160.71 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 1.022, val: n/a | iter time: 247.66 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.087, val: n/a | iter time: 211.26 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.089, val: n/a | iter time: 243.33 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 1.082, val: n/a | iter time: 159.23 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.116, val: n/a | iter time: 241.91 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.081, val: n/a | iter time: 248.76 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 0.999, val: n/a | iter time: 234.07 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.043, val: n/a | iter time: 168.28 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 0.989, val: n/a | iter time: 221.15 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.029, val: n/a | iter time: 224.79 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.085, val: n/a | iter time: 304.25 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.107, val: n/a | iter time: 238.10 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.118, val: n/a | iter time: 249.99 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 1.132, val: n/a | iter time: 223.14 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.069, val: n/a | iter time: 300.66 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"Combination of aaaand aeeeee.\"\n",
            "\n",
            "iter 100: val loss 1.0542, val time: 8159.63 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 1.012, val: 1.054 | iter time: 127.31 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.997, val: 1.054 | iter time: 248.44 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.027, val: 1.054 | iter time: 210.39 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.137, val: 1.054 | iter time: 263.00 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.176, val: 1.054 | iter time: 172.03 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.168, val: 1.054 | iter time: 246.73 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.084, val: 1.054 | iter time: 160.50 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 1.043, val: 1.054 | iter time: 252.74 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 1.043, val: 1.054 | iter time: 241.87 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 1.055, val: 1.054 | iter time: 169.82 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 1.099, val: 1.054 | iter time: 204.67 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 1.110, val: 1.054 | iter time: 302.04 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.030, val: 1.054 | iter time: 240.34 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.062, val: 1.054 | iter time: 202.99 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.979, val: 1.054 | iter time: 248.85 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 0.964, val: 1.054 | iter time: 299.93 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 1.000, val: 1.054 | iter time: 195.39 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.962, val: 1.054 | iter time: 227.89 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 1.000, val: 1.054 | iter time: 249.43 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.960, val: 1.054 | iter time: 301.29 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 0.949, val: 1.054 | iter time: 103.63 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.002, val: 1.054 | iter time: 246.38 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.053, val: 1.054 | iter time: 160.01 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.059, val: 1.054 | iter time: 302.09 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.048, val: 1.054 | iter time: 198.05 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  163087\n",
            "| - Tokens w/ Prompt          :  210770\n",
            "| - Total Tokens (w/ Padding) :  437296\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  39.18 s\n",
            "| - Tok/sec                   :  11159.86 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  31.32 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/tiny-llama-full.yaml --train.global_batch_size 32 --train.micro_batch_size 8 --precision bf16-mixed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy1y99lT04lA"
      },
      "source": [
        "Make a note of the training time and memory, which is printed at the end of the training job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7qzNGcW04lA"
      },
      "source": [
        "### Experiment: Larger model - 3b\n",
        "\n",
        "We’ve gained so much GPU memory back with these techniques, we can even train a larger model. Let’s switch from the 1.1B to the 3B model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax4Lqamj04lA",
        "outputId": "ae92eaf6-028f-46c4-d925-bbc14d53a066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_3b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x77ba6150ed80>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-open-llama-3b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=32,\r\n",
            "                    micro_batch_size=8,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 3,426,473,600\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.707, val: n/a | iter time: 547.08 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.913, val: n/a | iter time: 261.52 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 2.010, val: n/a | iter time: 439.26 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.924, val: n/a | iter time: 719.65 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.896, val: n/a | iter time: 432.15 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 1.867, val: n/a | iter time: 588.21 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 1.743, val: n/a | iter time: 590.38 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.759, val: n/a | iter time: 677.77 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.831, val: n/a | iter time: 424.11 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.793, val: n/a | iter time: 446.06 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.822, val: n/a | iter time: 502.37 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.800, val: n/a | iter time: 684.57 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.731, val: n/a | iter time: 582.67 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.772, val: n/a | iter time: 522.70 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.713, val: n/a | iter time: 580.12 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.819, val: n/a | iter time: 369.84 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.842, val: n/a | iter time: 583.97 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.706, val: n/a | iter time: 570.65 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.761, val: n/a | iter time: 589.15 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.702, val: n/a | iter time: 587.72 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.640, val: n/a | iter time: 556.54 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.684, val: n/a | iter time: 368.69 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.709, val: n/a | iter time: 369.38 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.618, val: n/a | iter time: 621.06 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.571, val: n/a | iter time: 589.85 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.596, val: n/a | iter time: 483.49 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.508, val: n/a | iter time: 564.57 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.479, val: n/a | iter time: 687.26 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.524, val: n/a | iter time: 551.04 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.419, val: n/a | iter time: 512.63 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.363, val: n/a | iter time: 599.97 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.417, val: n/a | iter time: 660.80 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.391, val: n/a | iter time: 539.58 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.399, val: n/a | iter time: 518.01 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.409, val: n/a | iter time: 436.38 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.307, val: n/a | iter time: 583.06 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.310, val: n/a | iter time: 430.72 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.278, val: n/a | iter time: 515.25 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.232, val: n/a | iter time: 452.45 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.216, val: n/a | iter time: 596.76 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.192, val: n/a | iter time: 372.43 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.118, val: n/a | iter time: 596.48 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.110, val: n/a | iter time: 594.39 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.129, val: n/a | iter time: 473.11 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.104, val: n/a | iter time: 500.78 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.169, val: n/a | iter time: 505.49 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.172, val: n/a | iter time: 547.92 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.245, val: n/a | iter time: 686.20 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.241, val: n/a | iter time: 586.49 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.228, val: n/a | iter time: 405.02 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.209, val: n/a | iter time: 558.54 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.158, val: n/a | iter time: 616.53 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.156, val: n/a | iter time: 545.49 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.125, val: n/a | iter time: 507.21 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.092, val: n/a | iter time: 268.30 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.029, val: n/a | iter time: 474.41 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.025, val: n/a | iter time: 399.34 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 1.034, val: n/a | iter time: 505.57 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 1.024, val: n/a | iter time: 406.93 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.085, val: n/a | iter time: 651.05 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.098, val: n/a | iter time: 375.87 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.113, val: n/a | iter time: 571.51 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.128, val: n/a | iter time: 506.16 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.121, val: n/a | iter time: 601.63 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.137, val: n/a | iter time: 500.45 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.135, val: n/a | iter time: 514.43 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.159, val: n/a | iter time: 555.83 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.141, val: n/a | iter time: 689.49 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.072, val: n/a | iter time: 403.16 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.075, val: n/a | iter time: 313.45 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.050, val: n/a | iter time: 389.17 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.016, val: n/a | iter time: 609.19 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.043, val: n/a | iter time: 469.34 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.026, val: n/a | iter time: 594.52 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.065, val: n/a | iter time: 597.34 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.025, val: n/a | iter time: 522.03 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 0.998, val: n/a | iter time: 418.32 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 1.006, val: n/a | iter time: 573.11 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.953, val: n/a | iter time: 340.66 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 0.994, val: n/a | iter time: 610.44 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.953, val: n/a | iter time: 269.52 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 1.003, val: n/a | iter time: 435.85 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.982, val: n/a | iter time: 486.59 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 1.022, val: n/a | iter time: 604.16 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.068, val: n/a | iter time: 375.25 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 1.018, val: n/a | iter time: 591.44 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.092, val: n/a | iter time: 437.88 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.089, val: n/a | iter time: 528.14 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 1.094, val: n/a | iter time: 361.60 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.113, val: n/a | iter time: 569.76 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.074, val: n/a | iter time: 590.26 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 0.990, val: n/a | iter time: 497.42 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.023, val: n/a | iter time: 357.60 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 0.974, val: n/a | iter time: 448.35 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.008, val: n/a | iter time: 445.77 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.069, val: n/a | iter time: 686.62 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.090, val: n/a | iter time: 516.92 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.083, val: n/a | iter time: 569.20 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 1.089, val: n/a | iter time: 512.79 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.019, val: n/a | iter time: 677.92 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"No, I'm not a robot. I'm a human being.\"\n",
            "\n",
            "iter 100: val loss 1.0329, val time: 18245.82 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 0.979, val: 1.033 | iter time: 258.60 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.969, val: 1.033 | iter time: 590.92 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.013, val: 1.033 | iter time: 482.97 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.136, val: 1.033 | iter time: 572.81 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.157, val: 1.033 | iter time: 399.13 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.168, val: 1.033 | iter time: 523.13 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.080, val: 1.033 | iter time: 344.75 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 1.026, val: 1.033 | iter time: 523.74 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 1.040, val: 1.033 | iter time: 536.39 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 1.052, val: 1.033 | iter time: 363.91 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 1.086, val: 1.033 | iter time: 486.15 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 1.095, val: 1.033 | iter time: 658.89 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.006, val: 1.033 | iter time: 577.54 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.022, val: 1.033 | iter time: 441.42 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.971, val: 1.033 | iter time: 569.71 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 0.955, val: 1.033 | iter time: 650.33 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.991, val: 1.033 | iter time: 473.43 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.965, val: 1.033 | iter time: 478.09 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 0.968, val: 1.033 | iter time: 589.40 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.932, val: 1.033 | iter time: 685.96 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 0.921, val: 1.033 | iter time: 215.74 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 0.981, val: 1.033 | iter time: 532.33 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.042, val: 1.033 | iter time: 366.17 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.054, val: 1.033 | iter time: 612.35 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.035, val: 1.033 | iter time: 429.64 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  151019\n",
            "| - Tokens w/ Prompt          :  195803\n",
            "| - Total Tokens (w/ Padding) :  409088\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  84.40 s\n",
            "| - Tok/sec                   :  4847.07 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  46.89 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/open-llama-3b-full.yaml --train.global_batch_size 32 --train.micro_batch_size 8 --precision bf16-true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz1OrUPM04lA"
      },
      "source": [
        "Make a note of the training time and memory, which is printed at the end of the training job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v2Bb7cT04lA"
      },
      "source": [
        "### Experiment: Larger model - 7b\n",
        "\n",
        "If we reduce the batch size again, we can even train a 7b model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXDzChOl04lA",
        "outputId": "96c71ae7-33db-4a3e-e08a-0a7e9a23f585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_7b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x719e0af3d1f0>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-open-llama-7b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=16,\r\n",
            "                    micro_batch_size=4,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 6,738,415,616\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.788, val: n/a | iter time: 472.84 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.633, val: n/a | iter time: 357.86 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.828, val: n/a | iter time: 173.44 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.853, val: n/a | iter time: 506.13 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.882, val: n/a | iter time: 386.88 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 2.152, val: n/a | iter time: 140.92 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 2.077, val: n/a | iter time: 339.25 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.938, val: n/a | iter time: 719.74 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.789, val: n/a | iter time: 390.77 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.580, val: n/a | iter time: 341.86 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.738, val: n/a | iter time: 160.18 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.821, val: n/a | iter time: 721.68 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.870, val: n/a | iter time: 520.09 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.852, val: n/a | iter time: 513.93 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.673, val: n/a | iter time: 442.49 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.602, val: n/a | iter time: 720.17 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.669, val: n/a | iter time: 385.03 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.645, val: n/a | iter time: 282.03 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.582, val: n/a | iter time: 420.93 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.664, val: n/a | iter time: 518.86 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.667, val: n/a | iter time: 149.16 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.618, val: n/a | iter time: 451.96 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.550, val: n/a | iter time: 553.27 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.491, val: n/a | iter time: 659.43 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.375, val: n/a | iter time: 333.67 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.343, val: n/a | iter time: 550.77 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.375, val: n/a | iter time: 496.32 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.416, val: n/a | iter time: 332.68 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.351, val: n/a | iter time: 524.51 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.412, val: n/a | iter time: 231.49 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.366, val: n/a | iter time: 279.21 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.286, val: n/a | iter time: 434.87 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.349, val: n/a | iter time: 532.27 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.291, val: n/a | iter time: 285.13 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.331, val: n/a | iter time: 173.45 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.191, val: n/a | iter time: 710.60 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.132, val: n/a | iter time: 204.94 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.090, val: n/a | iter time: 545.93 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.017, val: n/a | iter time: 147.54 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.078, val: n/a | iter time: 619.24 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.068, val: n/a | iter time: 508.81 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.026, val: n/a | iter time: 413.98 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 0.986, val: n/a | iter time: 342.47 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.016, val: n/a | iter time: 510.43 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.058, val: n/a | iter time: 144.42 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.134, val: n/a | iter time: 347.01 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.153, val: n/a | iter time: 498.69 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.135, val: n/a | iter time: 595.70 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.075, val: n/a | iter time: 377.56 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.005, val: n/a | iter time: 554.27 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.009, val: n/a | iter time: 270.52 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.032, val: n/a | iter time: 611.81 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 0.989, val: n/a | iter time: 149.00 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.023, val: n/a | iter time: 533.44 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 0.983, val: n/a | iter time: 416.05 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 0.936, val: n/a | iter time: 725.03 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 0.985, val: n/a | iter time: 265.37 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 0.990, val: n/a | iter time: 521.23 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 0.982, val: n/a | iter time: 485.92 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 0.990, val: n/a | iter time: 563.77 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.011, val: n/a | iter time: 425.23 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 0.991, val: n/a | iter time: 556.06 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.049, val: n/a | iter time: 535.35 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.006, val: n/a | iter time: 394.37 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.002, val: n/a | iter time: 492.47 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 0.956, val: n/a | iter time: 320.13 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 0.971, val: n/a | iter time: 415.69 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.024, val: n/a | iter time: 656.15 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.007, val: n/a | iter time: 381.64 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.065, val: n/a | iter time: 357.92 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.010, val: n/a | iter time: 445.94 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 0.969, val: n/a | iter time: 616.87 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 0.971, val: n/a | iter time: 172.42 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 0.977, val: n/a | iter time: 413.91 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.010, val: n/a | iter time: 495.79 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 0.964, val: n/a | iter time: 344.92 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 0.957, val: n/a | iter time: 395.18 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 0.929, val: n/a | iter time: 359.02 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.860, val: n/a | iter time: 455.63 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 0.929, val: n/a | iter time: 469.13 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.859, val: n/a | iter time: 175.94 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 0.932, val: n/a | iter time: 355.37 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.880, val: n/a | iter time: 164.29 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 0.843, val: n/a | iter time: 730.02 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.027, val: n/a | iter time: 360.09 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 0.894, val: n/a | iter time: 551.62 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 0.991, val: n/a | iter time: 352.80 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.022, val: n/a | iter time: 514.53 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 0.947, val: n/a | iter time: 429.31 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 0.997, val: n/a | iter time: 412.08 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.037, val: n/a | iter time: 450.86 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 1.029, val: n/a | iter time: 484.40 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.025, val: n/a | iter time: 487.69 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 1.011, val: n/a | iter time: 231.49 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.148, val: n/a | iter time: 556.41 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.129, val: n/a | iter time: 611.61 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.125, val: n/a | iter time: 526.87 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.148, val: n/a | iter time: 422.25 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 0.973, val: n/a | iter time: 355.17 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.002, val: n/a | iter time: 549.53 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"It's not your eyesight — it's your judgment, sucka!\"\n",
            "\n",
            "iter 100: val loss 0.9819, val time: 14575.27 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 0.976, val: 0.982 | iter time: 495.31 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.945, val: 0.982 | iter time: 324.26 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 0.967, val: 0.982 | iter time: 327.32 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.008, val: 0.982 | iter time: 667.12 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.028, val: 0.982 | iter time: 357.66 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.073, val: 0.982 | iter time: 517.17 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.054, val: 0.982 | iter time: 448.39 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 0.976, val: 0.982 | iter time: 488.58 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 0.892, val: 0.982 | iter time: 121.72 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 0.864, val: 0.982 | iter time: 269.90 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 0.878, val: 0.982 | iter time: 351.10 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 0.854, val: 0.982 | iter time: 402.81 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 0.971, val: 0.982 | iter time: 209.20 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 0.970, val: 0.982 | iter time: 390.98 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.942, val: 0.982 | iter time: 414.04 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 0.997, val: 0.982 | iter time: 620.86 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.878, val: 0.982 | iter time: 138.53 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.879, val: 0.982 | iter time: 389.80 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 0.944, val: 0.982 | iter time: 522.13 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.966, val: 0.982 | iter time: 462.40 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 1.046, val: 0.982 | iter time: 331.08 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.097, val: 0.982 | iter time: 292.29 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.023, val: 0.982 | iter time: 525.37 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.052, val: 0.982 | iter time: 565.32 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.029, val: 0.982 | iter time: 296.83 ms\n",
            "Epoch 1 | iter 126 step 31 | loss train: 0.972, val: 0.982 | iter time: 447.42 ms\n",
            "Epoch 1 | iter 127 step 31 | loss train: 1.030, val: 0.982 | iter time: 440.49 ms\n",
            "Epoch 1 | iter 128 step 32 | loss train: 1.026, val: 0.982 | iter time: 628.61 ms (step)\n",
            "Epoch 1 | iter 129 step 32 | loss train: 1.068, val: 0.982 | iter time: 354.47 ms\n",
            "Epoch 1 | iter 130 step 32 | loss train: 1.141, val: 0.982 | iter time: 461.03 ms\n",
            "Epoch 1 | iter 131 step 32 | loss train: 1.116, val: 0.982 | iter time: 486.66 ms\n",
            "Epoch 1 | iter 132 step 33 | loss train: 1.072, val: 0.982 | iter time: 576.42 ms (step)\n",
            "Epoch 1 | iter 133 step 33 | loss train: 1.051, val: 0.982 | iter time: 421.61 ms\n",
            "Epoch 1 | iter 134 step 33 | loss train: 0.976, val: 0.982 | iter time: 512.70 ms\n",
            "Epoch 1 | iter 135 step 33 | loss train: 0.980, val: 0.982 | iter time: 510.56 ms\n",
            "Epoch 1 | iter 136 step 34 | loss train: 1.017, val: 0.982 | iter time: 730.89 ms (step)\n",
            "Epoch 1 | iter 137 step 34 | loss train: 0.961, val: 0.982 | iter time: 366.47 ms\n",
            "Epoch 1 | iter 138 step 34 | loss train: 0.974, val: 0.982 | iter time: 380.39 ms\n",
            "Epoch 1 | iter 139 step 34 | loss train: 0.956, val: 0.982 | iter time: 307.67 ms\n",
            "Epoch 1 | iter 140 step 35 | loss train: 0.935, val: 0.982 | iter time: 471.81 ms (step)\n",
            "Epoch 1 | iter 141 step 35 | loss train: 0.961, val: 0.982 | iter time: 334.88 ms\n",
            "Epoch 1 | iter 142 step 35 | loss train: 0.905, val: 0.982 | iter time: 144.72 ms\n",
            "Epoch 1 | iter 143 step 35 | loss train: 0.905, val: 0.982 | iter time: 492.76 ms\n",
            "Epoch 1 | iter 144 step 36 | loss train: 0.848, val: 0.982 | iter time: 510.30 ms (step)\n",
            "Epoch 1 | iter 145 step 36 | loss train: 0.840, val: 0.982 | iter time: 302.77 ms\n",
            "Epoch 1 | iter 146 step 36 | loss train: 0.948, val: 0.982 | iter time: 437.09 ms\n",
            "Epoch 1 | iter 147 step 36 | loss train: 0.986, val: 0.982 | iter time: 553.60 ms\n",
            "Epoch 1 | iter 148 step 37 | loss train: 0.993, val: 0.982 | iter time: 586.49 ms (step)\n",
            "Epoch 1 | iter 149 step 37 | loss train: 1.019, val: 0.982 | iter time: 381.06 ms\n",
            "Epoch 1 | iter 150 step 37 | loss train: 1.017, val: 0.982 | iter time: 556.27 ms\n",
            "Epoch 1 | iter 151 step 37 | loss train: 0.958, val: 0.982 | iter time: 406.27 ms\n",
            "Epoch 1 | iter 152 step 38 | loss train: 0.933, val: 0.982 | iter time: 549.06 ms (step)\n",
            "Epoch 1 | iter 153 step 38 | loss train: 0.905, val: 0.982 | iter time: 383.76 ms\n",
            "Epoch 1 | iter 154 step 38 | loss train: 0.863, val: 0.982 | iter time: 221.95 ms\n",
            "Epoch 1 | iter 155 step 38 | loss train: 0.884, val: 0.982 | iter time: 536.81 ms\n",
            "Epoch 1 | iter 156 step 39 | loss train: 0.972, val: 0.982 | iter time: 687.08 ms (step)\n",
            "Epoch 1 | iter 157 step 39 | loss train: 1.010, val: 0.982 | iter time: 293.21 ms\n",
            "Epoch 1 | iter 158 step 39 | loss train: 0.952, val: 0.982 | iter time: 220.55 ms\n",
            "Epoch 1 | iter 159 step 39 | loss train: 0.944, val: 0.982 | iter time: 493.26 ms\n",
            "Epoch 1 | iter 160 step 40 | loss train: 0.898, val: 0.982 | iter time: 620.11 ms (step)\n",
            "Epoch 1 | iter 161 step 40 | loss train: 0.845, val: 0.982 | iter time: 254.97 ms\n",
            "Epoch 1 | iter 162 step 40 | loss train: 0.830, val: 0.982 | iter time: 222.87 ms\n",
            "Epoch 1 | iter 163 step 40 | loss train: 0.893, val: 0.982 | iter time: 413.15 ms\n",
            "Epoch 1 | iter 164 step 41 | loss train: 0.972, val: 0.982 | iter time: 485.01 ms (step)\n",
            "Epoch 1 | iter 165 step 41 | loss train: 0.987, val: 0.982 | iter time: 420.76 ms\n",
            "Epoch 1 | iter 166 step 41 | loss train: 0.970, val: 0.982 | iter time: 173.29 ms\n",
            "Epoch 1 | iter 167 step 41 | loss train: 0.944, val: 0.982 | iter time: 492.21 ms\n",
            "Epoch 1 | iter 168 step 42 | loss train: 0.924, val: 0.982 | iter time: 518.38 ms (step)\n",
            "Epoch 1 | iter 169 step 42 | loss train: 0.931, val: 0.982 | iter time: 336.15 ms\n",
            "Epoch 1 | iter 170 step 42 | loss train: 1.012, val: 0.982 | iter time: 301.19 ms\n",
            "Epoch 1 | iter 171 step 42 | loss train: 0.938, val: 0.982 | iter time: 186.59 ms\n",
            "Epoch 1 | iter 172 step 43 | loss train: 0.936, val: 0.982 | iter time: 721.06 ms (step)\n",
            "Epoch 1 | iter 173 step 43 | loss train: 0.918, val: 0.982 | iter time: 386.83 ms\n",
            "Epoch 1 | iter 174 step 43 | loss train: 1.013, val: 0.982 | iter time: 270.83 ms\n",
            "Epoch 1 | iter 175 step 43 | loss train: 1.111, val: 0.982 | iter time: 356.16 ms\n",
            "Epoch 1 | iter 176 step 44 | loss train: 1.093, val: 0.982 | iter time: 583.70 ms (step)\n",
            "Epoch 1 | iter 177 step 44 | loss train: 1.144, val: 0.982 | iter time: 325.14 ms\n",
            "Epoch 1 | iter 178 step 44 | loss train: 1.041, val: 0.982 | iter time: 281.76 ms\n",
            "Epoch 1 | iter 179 step 44 | loss train: 1.017, val: 0.982 | iter time: 536.23 ms\n",
            "Epoch 1 | iter 180 step 45 | loss train: 1.025, val: 0.982 | iter time: 589.49 ms (step)\n",
            "Epoch 1 | iter 181 step 45 | loss train: 0.978, val: 0.982 | iter time: 520.07 ms\n",
            "Epoch 1 | iter 182 step 45 | loss train: 1.005, val: 0.982 | iter time: 343.36 ms\n",
            "Epoch 1 | iter 183 step 45 | loss train: 0.890, val: 0.982 | iter time: 355.17 ms\n",
            "Epoch 1 | iter 184 step 46 | loss train: 0.852, val: 0.982 | iter time: 558.24 ms (step)\n",
            "Epoch 1 | iter 185 step 46 | loss train: 0.907, val: 0.982 | iter time: 323.20 ms\n",
            "Epoch 1 | iter 186 step 46 | loss train: 0.957, val: 0.982 | iter time: 302.39 ms\n",
            "Epoch 1 | iter 187 step 46 | loss train: 1.031, val: 0.982 | iter time: 276.89 ms\n",
            "Epoch 1 | iter 188 step 47 | loss train: 1.023, val: 0.982 | iter time: 586.70 ms (step)\n",
            "Epoch 1 | iter 189 step 47 | loss train: 1.040, val: 0.982 | iter time: 393.73 ms\n",
            "Epoch 1 | iter 190 step 47 | loss train: 0.977, val: 0.982 | iter time: 410.84 ms\n",
            "Epoch 1 | iter 191 step 47 | loss train: 0.985, val: 0.982 | iter time: 446.37 ms\n",
            "Epoch 1 | iter 192 step 48 | loss train: 1.028, val: 0.982 | iter time: 723.03 ms (step)\n",
            "Epoch 1 | iter 193 step 48 | loss train: 0.957, val: 0.982 | iter time: 295.34 ms\n",
            "Epoch 1 | iter 194 step 48 | loss train: 1.052, val: 0.982 | iter time: 501.97 ms\n",
            "Epoch 1 | iter 195 step 48 | loss train: 1.032, val: 0.982 | iter time: 536.53 ms\n",
            "Epoch 1 | iter 196 step 49 | loss train: 0.987, val: 0.982 | iter time: 338.57 ms (step)\n",
            "Epoch 1 | iter 197 step 49 | loss train: 1.045, val: 0.982 | iter time: 361.67 ms\n",
            "Epoch 1 | iter 198 step 49 | loss train: 0.984, val: 0.982 | iter time: 460.58 ms\n",
            "Epoch 1 | iter 199 step 49 | loss train: 0.960, val: 0.982 | iter time: 146.73 ms\n",
            "Epoch 1 | iter 200 step 50 | loss train: 0.925, val: 0.982 | iter time: 722.81 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " Her eyes are burning with passion and her cheeks are flushed with excitement. She doesn't even notice the looks of astonishment on the faces around her. This is the moment that she has been working towards for a very long time. She's finally become the supreme ruler of her domain. Tonight, she's the queen of the ball.\n",
            "\n",
            "### Photo:\n",
            "Queen\n",
            "\n",
            "iter 200: val loss 1.0177, val time: 16137.14 ms\n",
            "Epoch 1 | iter 201 step 50 | loss train: 0.938, val: 1.018 | iter time: 243.85 ms\n",
            "Epoch 1 | iter 202 step 50 | loss train: 0.879, val: 1.018 | iter time: 202.97 ms\n",
            "Epoch 1 | iter 203 step 50 | loss train: 0.963, val: 1.018 | iter time: 199.40 ms\n",
            "Epoch 1 | iter 204 step 51 | loss train: 0.983, val: 1.018 | iter time: 722.14 ms (step)\n",
            "Epoch 1 | iter 205 step 51 | loss train: 1.004, val: 1.018 | iter time: 412.34 ms\n",
            "Epoch 1 | iter 206 step 51 | loss train: 1.131, val: 1.018 | iter time: 316.83 ms\n",
            "Epoch 1 | iter 207 step 51 | loss train: 1.071, val: 1.018 | iter time: 319.39 ms\n",
            "Epoch 1 | iter 208 step 52 | loss train: 1.216, val: 1.018 | iter time: 616.30 ms (step)\n",
            "Epoch 1 | iter 209 step 52 | loss train: 1.203, val: 1.018 | iter time: 321.55 ms\n",
            "Epoch 1 | iter 210 step 52 | loss train: 1.172, val: 1.018 | iter time: 386.71 ms\n",
            "Epoch 1 | iter 211 step 52 | loss train: 1.225, val: 1.018 | iter time: 266.88 ms\n",
            "Epoch 1 | iter 212 step 53 | loss train: 1.095, val: 1.018 | iter time: 664.52 ms (step)\n",
            "Epoch 1 | iter 213 step 53 | loss train: 1.058, val: 1.018 | iter time: 295.22 ms\n",
            "Epoch 1 | iter 214 step 53 | loss train: 0.993, val: 1.018 | iter time: 306.97 ms\n",
            "Epoch 1 | iter 215 step 53 | loss train: 0.983, val: 1.018 | iter time: 305.80 ms\n",
            "Epoch 1 | iter 216 step 54 | loss train: 1.031, val: 1.018 | iter time: 575.71 ms (step)\n",
            "Epoch 1 | iter 217 step 54 | loss train: 1.081, val: 1.018 | iter time: 491.24 ms\n",
            "Epoch 1 | iter 218 step 54 | loss train: 1.133, val: 1.018 | iter time: 330.91 ms\n",
            "Epoch 1 | iter 219 step 54 | loss train: 1.086, val: 1.018 | iter time: 339.25 ms\n",
            "Epoch 1 | iter 220 step 55 | loss train: 1.091, val: 1.018 | iter time: 378.77 ms (step)\n",
            "Epoch 1 | iter 221 step 55 | loss train: 1.072, val: 1.018 | iter time: 309.63 ms\n",
            "Epoch 1 | iter 222 step 55 | loss train: 1.055, val: 1.018 | iter time: 441.96 ms\n",
            "Epoch 1 | iter 223 step 55 | loss train: 1.113, val: 1.018 | iter time: 533.77 ms\n",
            "Epoch 1 | iter 224 step 56 | loss train: 1.105, val: 1.018 | iter time: 611.69 ms (step)\n",
            "Epoch 1 | iter 225 step 56 | loss train: 1.017, val: 1.018 | iter time: 521.43 ms\n",
            "Epoch 1 | iter 226 step 56 | loss train: 0.970, val: 1.018 | iter time: 224.19 ms\n",
            "Epoch 1 | iter 227 step 56 | loss train: 0.955, val: 1.018 | iter time: 391.02 ms\n",
            "Epoch 1 | iter 228 step 57 | loss train: 0.958, val: 1.018 | iter time: 580.38 ms (step)\n",
            "Epoch 1 | iter 229 step 57 | loss train: 1.027, val: 1.018 | iter time: 195.94 ms\n",
            "Epoch 1 | iter 230 step 57 | loss train: 0.982, val: 1.018 | iter time: 540.10 ms\n",
            "Epoch 1 | iter 231 step 57 | loss train: 0.913, val: 1.018 | iter time: 201.33 ms\n",
            "Epoch 1 | iter 232 step 58 | loss train: 0.941, val: 1.018 | iter time: 691.79 ms (step)\n",
            "Epoch 1 | iter 233 step 58 | loss train: 0.921, val: 1.018 | iter time: 422.90 ms\n",
            "Epoch 1 | iter 234 step 58 | loss train: 0.992, val: 1.018 | iter time: 399.16 ms\n",
            "Epoch 1 | iter 235 step 58 | loss train: 0.971, val: 1.018 | iter time: 264.72 ms\n",
            "Epoch 1 | iter 236 step 59 | loss train: 0.966, val: 1.018 | iter time: 607.54 ms (step)\n",
            "Epoch 1 | iter 237 step 59 | loss train: 0.956, val: 1.018 | iter time: 529.33 ms\n",
            "Epoch 1 | iter 238 step 59 | loss train: 0.882, val: 1.018 | iter time: 203.99 ms\n",
            "Epoch 1 | iter 239 step 59 | loss train: 0.962, val: 1.018 | iter time: 553.35 ms\n",
            "Epoch 1 | iter 240 step 60 | loss train: 0.882, val: 1.018 | iter time: 709.88 ms (step)\n",
            "Epoch 1 | iter 241 step 60 | loss train: 0.892, val: 1.018 | iter time: 142.28 ms\n",
            "Epoch 1 | iter 242 step 60 | loss train: 0.981, val: 1.018 | iter time: 226.45 ms\n",
            "Epoch 1 | iter 243 step 60 | loss train: 1.065, val: 1.018 | iter time: 505.23 ms\n",
            "Epoch 1 | iter 244 step 61 | loss train: 1.097, val: 1.018 | iter time: 580.53 ms (step)\n",
            "Epoch 1 | iter 245 step 61 | loss train: 1.134, val: 1.018 | iter time: 318.15 ms\n",
            "Epoch 1 | iter 246 step 61 | loss train: 1.150, val: 1.018 | iter time: 285.01 ms\n",
            "Epoch 1 | iter 247 step 61 | loss train: 1.019, val: 1.018 | iter time: 488.11 ms\n",
            "Epoch 1 | iter 248 step 62 | loss train: 1.057, val: 1.018 | iter time: 667.20 ms (step)\n",
            "Epoch 1 | iter 249 step 62 | loss train: 1.016, val: 1.018 | iter time: 385.87 ms\n",
            "Epoch 1 | iter 250 step 62 | loss train: 0.964, val: 1.018 | iter time: 408.10 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  151019\n",
            "| - Tokens w/ Prompt          :  195803\n",
            "| - Total Tokens (w/ Padding) :  336060\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  138.72 s\n",
            "| - Tok/sec                   :  2422.59 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  69.71 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/open-llama-7b-full.yaml --train.global_batch_size 16 --train.micro_batch_size 4 --precision bf16-true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gVXXFeM04lA"
      },
      "source": [
        "Make a note of the training time and memory, which is printed at the end of the training job."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSSHLhZP04lA"
      },
      "source": [
        "### Experiment: Larger model - 13b\n",
        "\n",
        "Even with the smallest possible batch size, we can’t train a 13B model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBIgJnne04lA",
        "outputId": "7dfb3b6a-4896-4940-b756-4075dc1b40e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_13b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x773916f38e90>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/full-open-llama-13b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=1,\r\n",
            "                    micro_batch_size=1,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 13,015,864,320\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/conda/bin/litgpt\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/__main__.py\", line 71, in main\n",
            "    CLI(parser_data)\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/jsonargparse/_cli.py\", line 119, in CLI\n",
            "    return _run_component(component, init.get(subcommand))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/jsonargparse/_cli.py\", line 204, in _run_component\n",
            "    return component(**cfg)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/finetune/full.py\", line 122, in setup\n",
            "    fabric.launch(main, devices, resume, seed, config, data, checkpoint_dir, out_dir, train, eval, optimizer)\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/fabric.py\", line 837, in launch\n",
            "    return self._wrap_and_launch(function, self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/fabric.py\", line 923, in _wrap_and_launch\n",
            "    return to_run(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/fabric.py\", line 928, in _wrap_with_setup\n",
            "    return to_run(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/finetune/full.py\", line 171, in main\n",
            "    token_counts = fit(fabric, state, train_dataloader, val_dataloader, devices, resume, checkpoint_dir, out_dir, train, eval, data)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/litgpt/finetune/full.py\", line 272, in fit\n",
            "    optimizer.step()\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/optim/lr_scheduler.py\", line 137, in wrapper\n",
            "    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/wrappers.py\", line 87, in step\n",
            "    output = self._strategy.optimizer_step(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/strategies/strategy.py\", line 204, in optimizer_step\n",
            "    return self.precision.optimizer_step(optimizer, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/lightning/fabric/plugins/precision/precision.py\", line 124, in optimizer_step\n",
            "    return optimizer.step(**kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
            "    out = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
            "    ret = func(self, *args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py\", line 209, in step\n",
            "    has_complex = self._init_group(\n",
            "                  ^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.12/site-packages/torch/optim/adamw.py\", line 152, in _init_group\n",
            "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
            "                          ^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 150.00 MiB. GPU 0 has a total capacity of 79.25 GiB of which 62.75 MiB is free. Process 31936 has 79.18 GiB memory in use. Of the allocated memory 78.64 GiB is allocated by PyTorch, and 38.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/open-llama-13b-full.yaml --train.global_batch_size 1 --train.micro_batch_size 1 --precision bf16-true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afAjqgbA04lA"
      },
      "source": [
        "this will fail with an “out of memory” error. But, if we switch from the Adam optimizer (which has two state values per parameter) to SGD, we can train a 13B model. It’s *verrrrry* slow, though, so we won’t even train it for a full epoch - just 25 “steps”, so we can get an idea of the memory required:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d06SLG_k04lA",
        "outputId": "112efc48-3ddd-4cd5-cdd2-3307a5f8e074"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/conda/lib/python3.12/site-packages/litgpt/args.py:49: UserWarning: `--train.lr_warmup_steps` should be less than `--train.max_steps`. Got 100 lr_warmup_steps and 25 max_steps.\r\n",
            "  warnings.warn(\r\n",
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_13b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7fa86d5c4530>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': 'SGD',\r\n",
            " 'out_dir': PosixPath('out/finetune/full-open-llama-13b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'resume': False,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=1,\r\n",
            "                    micro_batch_size=1,\r\n",
            "                    lr_warmup_steps=100,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=25,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 13,015,864,320\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 1 | loss train: 1.313, val: n/a | iter time: 444.12 ms (step)\n",
            "Epoch 1 | iter 2 step 2 | loss train: 2.139, val: n/a | iter time: 179.40 ms (step)\n",
            "Epoch 1 | iter 3 step 3 | loss train: 1.856, val: n/a | iter time: 242.47 ms (step)\n",
            "Epoch 1 | iter 4 step 4 | loss train: 2.023, val: n/a | iter time: 172.67 ms (step)\n",
            "Epoch 1 | iter 5 step 5 | loss train: 1.300, val: n/a | iter time: 207.94 ms (step)\n",
            "Epoch 1 | iter 6 step 6 | loss train: 1.884, val: n/a | iter time: 166.50 ms (step)\n",
            "Epoch 1 | iter 7 step 7 | loss train: 2.886, val: n/a | iter time: 150.55 ms (step)\n",
            "Epoch 1 | iter 8 step 8 | loss train: 1.041, val: n/a | iter time: 252.37 ms (step)\n",
            "Epoch 1 | iter 9 step 9 | loss train: 1.602, val: n/a | iter time: 166.04 ms (step)\n",
            "Epoch 1 | iter 10 step 10 | loss train: 2.405, val: n/a | iter time: 167.61 ms (step)\n",
            "Epoch 1 | iter 11 step 11 | loss train: 2.256, val: n/a | iter time: 156.90 ms (step)\n",
            "Epoch 1 | iter 12 step 12 | loss train: 2.107, val: n/a | iter time: 168.83 ms (step)\n",
            "Epoch 1 | iter 13 step 13 | loss train: 1.715, val: n/a | iter time: 188.72 ms (step)\n",
            "Epoch 1 | iter 14 step 14 | loss train: 1.705, val: n/a | iter time: 166.89 ms (step)\n",
            "Epoch 1 | iter 15 step 15 | loss train: 2.205, val: n/a | iter time: 166.95 ms (step)\n",
            "Epoch 1 | iter 16 step 16 | loss train: 1.535, val: n/a | iter time: 198.51 ms (step)\n",
            "Epoch 1 | iter 17 step 17 | loss train: 1.640, val: n/a | iter time: 186.30 ms (step)\n",
            "Epoch 1 | iter 18 step 18 | loss train: 2.297, val: n/a | iter time: 158.63 ms (step)\n",
            "Epoch 1 | iter 19 step 19 | loss train: 1.535, val: n/a | iter time: 268.26 ms (step)\n",
            "Epoch 1 | iter 20 step 20 | loss train: 2.101, val: n/a | iter time: 161.14 ms (step)\n",
            "Epoch 1 | iter 21 step 21 | loss train: 2.402, val: n/a | iter time: 158.53 ms (step)\n",
            "Epoch 1 | iter 22 step 22 | loss train: 2.061, val: n/a | iter time: 160.32 ms (step)\n",
            "Epoch 1 | iter 23 step 23 | loss train: 2.200, val: n/a | iter time: 159.78 ms (step)\n",
            "Epoch 1 | iter 24 step 24 | loss train: 1.637, val: n/a | iter time: 158.42 ms (step)\n",
            "Epoch 1 | iter 25 step 25 | loss train: 1.446, val: n/a | iter time: 246.84 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            "A cat in a box with a pointy hat strapped to his back watching a fluffy dog\n",
            "\n",
            "### Instruction:\n",
            "Tell me _______!\n",
            "\n",
            "### Response:\n",
            "When fall arrives: I love Halloween.\n",
            "\n",
            "### Instruction:\n",
            "Instructions: choose the best option\n",
            "\n",
            "**1.** Write an email to your friend\n",
            "2. Write a blog post\n",
            "3. Make some illustrations\n",
            "\n",
            "### Response:\n",
            "1. Write an email\n",
            "\n",
            "iter 25: val loss 1.6274, val time: 9082.25 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :   2592\n",
            "| - Tokens w/ Prompt          :   3739\n",
            "| - Total Tokens (w/ Padding) :   3739\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  15.56 s\n",
            "| - Tok/sec                   :  240.22 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  52.38 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune_full --config config/open-llama-13b-full.yaml --train.global_batch_size 1 --train.micro_batch_size 1 --precision bf16-true --optimizer SGD --train.max_steps 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a4QQN0104lA"
      },
      "source": [
        "### Experiment: Parameter efficient fine tuning\n",
        "\n",
        "If we are only fine-tuning, not training a model from scratch, we can also consider LoRA and QLoRA. Let’s try it first with our 1.1B model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaWr9Zn_04lF",
        "outputId": "8a0e9b54-00a8-4502-8b3e-94996ac1c77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x78267c3b3170>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'lora_alpha': 16,\r\n",
            " 'lora_dropout': 0.05,\r\n",
            " 'lora_head': True,\r\n",
            " 'lora_key': True,\r\n",
            " 'lora_mlp': True,\r\n",
            " 'lora_projection': True,\r\n",
            " 'lora_query': True,\r\n",
            " 'lora_r': 32,\r\n",
            " 'lora_value': True,\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/lora-tiny-llama-1.1b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'quantize': None,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=16,\r\n",
            "                    micro_batch_size=4,\r\n",
            "                    lr_warmup_steps=10,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 26,320,896\n",
            "Number of non-trainable parameters: 1,100,048,384\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.871, val: n/a | iter time: 355.82 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.701, val: n/a | iter time: 107.24 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.864, val: n/a | iter time: 119.40 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.873, val: n/a | iter time: 146.95 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.895, val: n/a | iter time: 112.27 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 2.138, val: n/a | iter time: 101.14 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 2.084, val: n/a | iter time: 97.78 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.984, val: n/a | iter time: 139.07 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.862, val: n/a | iter time: 100.97 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.666, val: n/a | iter time: 100.70 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.802, val: n/a | iter time: 100.07 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.862, val: n/a | iter time: 138.64 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.891, val: n/a | iter time: 134.71 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.885, val: n/a | iter time: 138.12 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.732, val: n/a | iter time: 117.49 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.682, val: n/a | iter time: 138.59 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.763, val: n/a | iter time: 104.57 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.774, val: n/a | iter time: 99.85 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.724, val: n/a | iter time: 106.72 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.824, val: n/a | iter time: 103.76 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.884, val: n/a | iter time: 99.34 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.819, val: n/a | iter time: 130.14 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.765, val: n/a | iter time: 135.75 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.724, val: n/a | iter time: 125.38 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.592, val: n/a | iter time: 98.26 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.608, val: n/a | iter time: 136.47 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.699, val: n/a | iter time: 131.73 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.820, val: n/a | iter time: 104.30 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.748, val: n/a | iter time: 136.67 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.864, val: n/a | iter time: 100.15 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.846, val: n/a | iter time: 102.26 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.762, val: n/a | iter time: 103.96 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.816, val: n/a | iter time: 136.48 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.735, val: n/a | iter time: 100.34 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.813, val: n/a | iter time: 99.85 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.605, val: n/a | iter time: 139.32 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.646, val: n/a | iter time: 98.70 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.600, val: n/a | iter time: 135.23 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.572, val: n/a | iter time: 99.24 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.645, val: n/a | iter time: 123.60 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.537, val: n/a | iter time: 136.07 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.504, val: n/a | iter time: 100.12 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.340, val: n/a | iter time: 100.32 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.417, val: n/a | iter time: 104.35 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.526, val: n/a | iter time: 98.99 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.548, val: n/a | iter time: 101.08 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.540, val: n/a | iter time: 132.01 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.441, val: n/a | iter time: 108.83 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.309, val: n/a | iter time: 102.40 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.212, val: n/a | iter time: 137.53 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.264, val: n/a | iter time: 100.97 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.295, val: n/a | iter time: 131.00 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.295, val: n/a | iter time: 95.56 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.340, val: n/a | iter time: 137.32 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.250, val: n/a | iter time: 108.96 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.210, val: n/a | iter time: 140.67 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.208, val: n/a | iter time: 98.51 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 1.214, val: n/a | iter time: 135.90 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 1.189, val: n/a | iter time: 137.15 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.185, val: n/a | iter time: 103.96 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.202, val: n/a | iter time: 121.22 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.171, val: n/a | iter time: 136.57 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.233, val: n/a | iter time: 135.73 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.189, val: n/a | iter time: 103.54 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.178, val: n/a | iter time: 138.07 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.155, val: n/a | iter time: 100.57 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.165, val: n/a | iter time: 118.78 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.195, val: n/a | iter time: 138.54 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.168, val: n/a | iter time: 105.21 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.188, val: n/a | iter time: 100.15 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.121, val: n/a | iter time: 133.04 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.065, val: n/a | iter time: 131.37 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.065, val: n/a | iter time: 98.01 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.085, val: n/a | iter time: 117.74 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.122, val: n/a | iter time: 134.43 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.103, val: n/a | iter time: 99.21 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 1.081, val: n/a | iter time: 122.45 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 1.045, val: n/a | iter time: 101.22 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.994, val: n/a | iter time: 119.97 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 1.040, val: n/a | iter time: 106.35 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 1.006, val: n/a | iter time: 97.45 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 1.063, val: n/a | iter time: 98.54 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.985, val: n/a | iter time: 100.74 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 0.976, val: n/a | iter time: 139.43 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.140, val: n/a | iter time: 99.03 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 1.036, val: n/a | iter time: 137.29 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.159, val: n/a | iter time: 100.88 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.169, val: n/a | iter time: 103.21 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 1.097, val: n/a | iter time: 120.50 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.124, val: n/a | iter time: 105.35 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.162, val: n/a | iter time: 121.46 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 1.162, val: n/a | iter time: 105.05 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.155, val: n/a | iter time: 136.51 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 1.168, val: n/a | iter time: 100.74 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.269, val: n/a | iter time: 136.78 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.241, val: n/a | iter time: 123.02 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.240, val: n/a | iter time: 135.51 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.249, val: n/a | iter time: 104.74 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 1.091, val: n/a | iter time: 100.63 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.130, val: n/a | iter time: 103.59 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " The caption is \"A boss can admit he's wrong, but not when the mistake is on camera.\"\n",
            "\n",
            "iter 100: val loss 1.0922, val time: 5434.55 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 1.123, val: 1.092 | iter time: 135.09 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 1.100, val: 1.092 | iter time: 102.13 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.124, val: 1.092 | iter time: 102.23 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.151, val: 1.092 | iter time: 133.34 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.134, val: 1.092 | iter time: 98.91 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.171, val: 1.092 | iter time: 136.39 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.155, val: 1.092 | iter time: 105.54 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 1.102, val: 1.092 | iter time: 104.25 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 1.047, val: 1.092 | iter time: 99.35 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 1.000, val: 1.092 | iter time: 101.25 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 1.001, val: 1.092 | iter time: 102.14 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 0.964, val: 1.092 | iter time: 102.28 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.063, val: 1.092 | iter time: 99.73 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.067, val: 1.092 | iter time: 102.26 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 1.062, val: 1.092 | iter time: 105.73 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 1.100, val: 1.092 | iter time: 134.80 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.975, val: 1.092 | iter time: 95.57 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.970, val: 1.092 | iter time: 103.55 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 1.007, val: 1.092 | iter time: 136.74 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 1.069, val: 1.092 | iter time: 105.25 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 1.148, val: 1.092 | iter time: 100.73 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.211, val: 1.092 | iter time: 100.99 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.143, val: 1.092 | iter time: 139.64 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.136, val: 1.092 | iter time: 105.34 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.118, val: 1.092 | iter time: 95.83 ms\n",
            "Epoch 1 | iter 126 step 31 | loss train: 1.042, val: 1.092 | iter time: 129.97 ms\n",
            "Epoch 1 | iter 127 step 31 | loss train: 1.106, val: 1.092 | iter time: 121.43 ms\n",
            "Epoch 1 | iter 128 step 32 | loss train: 1.110, val: 1.092 | iter time: 130.11 ms (step)\n",
            "Epoch 1 | iter 129 step 32 | loss train: 1.144, val: 1.092 | iter time: 99.79 ms\n",
            "Epoch 1 | iter 130 step 32 | loss train: 1.238, val: 1.092 | iter time: 124.16 ms\n",
            "Epoch 1 | iter 131 step 32 | loss train: 1.226, val: 1.092 | iter time: 119.68 ms\n",
            "Epoch 1 | iter 132 step 33 | loss train: 1.167, val: 1.092 | iter time: 108.90 ms (step)\n",
            "Epoch 1 | iter 133 step 33 | loss train: 1.185, val: 1.092 | iter time: 117.13 ms\n",
            "Epoch 1 | iter 134 step 33 | loss train: 1.120, val: 1.092 | iter time: 136.35 ms\n",
            "Epoch 1 | iter 135 step 33 | loss train: 1.114, val: 1.092 | iter time: 138.51 ms\n",
            "Epoch 1 | iter 136 step 34 | loss train: 1.160, val: 1.092 | iter time: 140.83 ms (step)\n",
            "Epoch 1 | iter 137 step 34 | loss train: 1.082, val: 1.092 | iter time: 100.16 ms\n",
            "Epoch 1 | iter 138 step 34 | loss train: 1.079, val: 1.092 | iter time: 101.00 ms\n",
            "Epoch 1 | iter 139 step 34 | loss train: 1.067, val: 1.092 | iter time: 103.94 ms\n",
            "Epoch 1 | iter 140 step 35 | loss train: 1.062, val: 1.092 | iter time: 106.23 ms (step)\n",
            "Epoch 1 | iter 141 step 35 | loss train: 1.093, val: 1.092 | iter time: 96.72 ms\n",
            "Epoch 1 | iter 142 step 35 | loss train: 1.022, val: 1.092 | iter time: 101.04 ms\n",
            "Epoch 1 | iter 143 step 35 | loss train: 1.010, val: 1.092 | iter time: 140.06 ms\n",
            "Epoch 1 | iter 144 step 36 | loss train: 0.948, val: 1.092 | iter time: 105.37 ms (step)\n",
            "Epoch 1 | iter 145 step 36 | loss train: 0.919, val: 1.092 | iter time: 102.03 ms\n",
            "Epoch 1 | iter 146 step 36 | loss train: 1.038, val: 1.092 | iter time: 119.49 ms\n",
            "Epoch 1 | iter 147 step 36 | loss train: 1.065, val: 1.092 | iter time: 137.02 ms\n",
            "Epoch 1 | iter 148 step 37 | loss train: 1.048, val: 1.092 | iter time: 121.29 ms (step)\n",
            "Epoch 1 | iter 149 step 37 | loss train: 1.078, val: 1.092 | iter time: 118.84 ms\n",
            "Epoch 1 | iter 150 step 37 | loss train: 1.063, val: 1.092 | iter time: 137.19 ms\n",
            "Epoch 1 | iter 151 step 37 | loss train: 1.000, val: 1.092 | iter time: 105.03 ms\n",
            "Epoch 1 | iter 152 step 38 | loss train: 0.980, val: 1.092 | iter time: 105.07 ms (step)\n",
            "Epoch 1 | iter 153 step 38 | loss train: 0.952, val: 1.092 | iter time: 104.41 ms\n",
            "Epoch 1 | iter 154 step 38 | loss train: 0.916, val: 1.092 | iter time: 102.60 ms\n",
            "Epoch 1 | iter 155 step 38 | loss train: 0.971, val: 1.092 | iter time: 135.95 ms\n",
            "Epoch 1 | iter 156 step 39 | loss train: 1.045, val: 1.092 | iter time: 140.06 ms (step)\n",
            "Epoch 1 | iter 157 step 39 | loss train: 1.070, val: 1.092 | iter time: 96.00 ms\n",
            "Epoch 1 | iter 158 step 39 | loss train: 1.012, val: 1.092 | iter time: 103.29 ms\n",
            "Epoch 1 | iter 159 step 39 | loss train: 0.995, val: 1.092 | iter time: 135.31 ms\n",
            "Epoch 1 | iter 160 step 40 | loss train: 0.974, val: 1.092 | iter time: 133.07 ms (step)\n",
            "Epoch 1 | iter 161 step 40 | loss train: 0.938, val: 1.092 | iter time: 100.38 ms\n",
            "Epoch 1 | iter 162 step 40 | loss train: 0.929, val: 1.092 | iter time: 101.82 ms\n",
            "Epoch 1 | iter 163 step 40 | loss train: 0.990, val: 1.092 | iter time: 103.68 ms\n",
            "Epoch 1 | iter 164 step 41 | loss train: 1.061, val: 1.092 | iter time: 106.56 ms (step)\n",
            "Epoch 1 | iter 165 step 41 | loss train: 1.059, val: 1.092 | iter time: 134.33 ms\n",
            "Epoch 1 | iter 166 step 41 | loss train: 1.036, val: 1.092 | iter time: 101.84 ms\n",
            "Epoch 1 | iter 167 step 41 | loss train: 1.000, val: 1.092 | iter time: 132.56 ms\n",
            "Epoch 1 | iter 168 step 42 | loss train: 0.985, val: 1.092 | iter time: 105.00 ms (step)\n",
            "Epoch 1 | iter 169 step 42 | loss train: 1.027, val: 1.092 | iter time: 100.22 ms\n",
            "Epoch 1 | iter 170 step 42 | loss train: 1.112, val: 1.092 | iter time: 108.98 ms\n",
            "Epoch 1 | iter 171 step 42 | loss train: 1.039, val: 1.092 | iter time: 97.82 ms\n",
            "Epoch 1 | iter 172 step 43 | loss train: 1.019, val: 1.092 | iter time: 139.61 ms (step)\n",
            "Epoch 1 | iter 173 step 43 | loss train: 0.984, val: 1.092 | iter time: 117.06 ms\n",
            "Epoch 1 | iter 174 step 43 | loss train: 1.106, val: 1.092 | iter time: 102.63 ms\n",
            "Epoch 1 | iter 175 step 43 | loss train: 1.186, val: 1.092 | iter time: 98.65 ms\n",
            "Epoch 1 | iter 176 step 44 | loss train: 1.194, val: 1.092 | iter time: 105.83 ms (step)\n",
            "Epoch 1 | iter 177 step 44 | loss train: 1.241, val: 1.092 | iter time: 98.32 ms\n",
            "Epoch 1 | iter 178 step 44 | loss train: 1.092, val: 1.092 | iter time: 100.61 ms\n",
            "Epoch 1 | iter 179 step 44 | loss train: 1.087, val: 1.092 | iter time: 140.35 ms\n",
            "Epoch 1 | iter 180 step 45 | loss train: 1.095, val: 1.092 | iter time: 111.22 ms (step)\n",
            "Epoch 1 | iter 181 step 45 | loss train: 1.046, val: 1.092 | iter time: 135.10 ms\n",
            "Epoch 1 | iter 182 step 45 | loss train: 1.090, val: 1.092 | iter time: 102.02 ms\n",
            "Epoch 1 | iter 183 step 45 | loss train: 0.972, val: 1.092 | iter time: 101.46 ms\n",
            "Epoch 1 | iter 184 step 46 | loss train: 0.918, val: 1.092 | iter time: 101.67 ms (step)\n",
            "Epoch 1 | iter 185 step 46 | loss train: 0.975, val: 1.092 | iter time: 98.61 ms\n",
            "Epoch 1 | iter 186 step 46 | loss train: 1.013, val: 1.092 | iter time: 102.41 ms\n",
            "Epoch 1 | iter 187 step 46 | loss train: 1.087, val: 1.092 | iter time: 97.77 ms\n",
            "Epoch 1 | iter 188 step 47 | loss train: 1.071, val: 1.092 | iter time: 131.28 ms (step)\n",
            "Epoch 1 | iter 189 step 47 | loss train: 1.074, val: 1.092 | iter time: 122.33 ms\n",
            "Epoch 1 | iter 190 step 47 | loss train: 1.038, val: 1.092 | iter time: 106.66 ms\n",
            "Epoch 1 | iter 191 step 47 | loss train: 1.043, val: 1.092 | iter time: 121.93 ms\n",
            "Epoch 1 | iter 192 step 48 | loss train: 1.094, val: 1.092 | iter time: 140.05 ms (step)\n",
            "Epoch 1 | iter 193 step 48 | loss train: 1.030, val: 1.092 | iter time: 96.20 ms\n",
            "Epoch 1 | iter 194 step 48 | loss train: 1.116, val: 1.092 | iter time: 135.60 ms\n",
            "Epoch 1 | iter 195 step 48 | loss train: 1.117, val: 1.092 | iter time: 140.61 ms\n",
            "Epoch 1 | iter 196 step 49 | loss train: 1.066, val: 1.092 | iter time: 105.13 ms (step)\n",
            "Epoch 1 | iter 197 step 49 | loss train: 1.127, val: 1.092 | iter time: 100.23 ms\n",
            "Epoch 1 | iter 198 step 49 | loss train: 1.071, val: 1.092 | iter time: 121.26 ms\n",
            "Epoch 1 | iter 199 step 49 | loss train: 1.012, val: 1.092 | iter time: 101.69 ms\n",
            "Epoch 1 | iter 200 step 50 | loss train: 0.988, val: 1.092 | iter time: 139.58 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " Here's a caption for this photo: \"She is more than just a pretty face! What a beauty!\"\n",
            "\n",
            "iter 200: val loss 1.0610, val time: 5390.09 ms\n",
            "Epoch 1 | iter 201 step 50 | loss train: 1.011, val: 1.061 | iter time: 109.27 ms\n",
            "Epoch 1 | iter 202 step 50 | loss train: 0.968, val: 1.061 | iter time: 105.41 ms\n",
            "Epoch 1 | iter 203 step 50 | loss train: 1.037, val: 1.061 | iter time: 99.87 ms\n",
            "Epoch 1 | iter 204 step 51 | loss train: 1.050, val: 1.061 | iter time: 140.90 ms (step)\n",
            "Epoch 1 | iter 205 step 51 | loss train: 1.047, val: 1.061 | iter time: 116.80 ms\n",
            "Epoch 1 | iter 206 step 51 | loss train: 1.144, val: 1.061 | iter time: 103.60 ms\n",
            "Epoch 1 | iter 207 step 51 | loss train: 1.122, val: 1.061 | iter time: 104.82 ms\n",
            "Epoch 1 | iter 208 step 52 | loss train: 1.274, val: 1.061 | iter time: 121.98 ms (step)\n",
            "Epoch 1 | iter 209 step 52 | loss train: 1.265, val: 1.061 | iter time: 100.14 ms\n",
            "Epoch 1 | iter 210 step 52 | loss train: 1.213, val: 1.061 | iter time: 102.13 ms\n",
            "Epoch 1 | iter 211 step 52 | loss train: 1.218, val: 1.061 | iter time: 110.89 ms\n",
            "Epoch 1 | iter 212 step 53 | loss train: 1.073, val: 1.061 | iter time: 141.79 ms (step)\n",
            "Epoch 1 | iter 213 step 53 | loss train: 1.036, val: 1.061 | iter time: 99.68 ms\n",
            "Epoch 1 | iter 214 step 53 | loss train: 0.969, val: 1.061 | iter time: 102.01 ms\n",
            "Epoch 1 | iter 215 step 53 | loss train: 0.995, val: 1.061 | iter time: 110.07 ms\n",
            "Epoch 1 | iter 216 step 54 | loss train: 1.046, val: 1.061 | iter time: 107.02 ms (step)\n",
            "Epoch 1 | iter 217 step 54 | loss train: 1.089, val: 1.061 | iter time: 136.66 ms\n",
            "Epoch 1 | iter 218 step 54 | loss train: 1.142, val: 1.061 | iter time: 97.55 ms\n",
            "Epoch 1 | iter 219 step 54 | loss train: 1.085, val: 1.061 | iter time: 97.72 ms\n",
            "Epoch 1 | iter 220 step 55 | loss train: 1.080, val: 1.061 | iter time: 105.15 ms (step)\n",
            "Epoch 1 | iter 221 step 55 | loss train: 1.060, val: 1.061 | iter time: 99.37 ms\n",
            "Epoch 1 | iter 222 step 55 | loss train: 1.058, val: 1.061 | iter time: 105.27 ms\n",
            "Epoch 1 | iter 223 step 55 | loss train: 1.116, val: 1.061 | iter time: 136.25 ms\n",
            "Epoch 1 | iter 224 step 56 | loss train: 1.132, val: 1.061 | iter time: 109.97 ms (step)\n",
            "Epoch 1 | iter 225 step 56 | loss train: 1.062, val: 1.061 | iter time: 135.04 ms\n",
            "Epoch 1 | iter 226 step 56 | loss train: 1.026, val: 1.061 | iter time: 97.41 ms\n",
            "Epoch 1 | iter 227 step 56 | loss train: 1.010, val: 1.061 | iter time: 100.75 ms\n",
            "Epoch 1 | iter 228 step 57 | loss train: 1.002, val: 1.061 | iter time: 106.40 ms (step)\n",
            "Epoch 1 | iter 229 step 57 | loss train: 1.073, val: 1.061 | iter time: 106.31 ms\n",
            "Epoch 1 | iter 230 step 57 | loss train: 0.997, val: 1.061 | iter time: 136.08 ms\n",
            "Epoch 1 | iter 231 step 57 | loss train: 0.940, val: 1.061 | iter time: 100.71 ms\n",
            "Epoch 1 | iter 232 step 58 | loss train: 0.963, val: 1.061 | iter time: 140.22 ms (step)\n",
            "Epoch 1 | iter 233 step 58 | loss train: 0.939, val: 1.061 | iter time: 105.38 ms\n",
            "Epoch 1 | iter 234 step 58 | loss train: 1.007, val: 1.061 | iter time: 101.93 ms\n",
            "Epoch 1 | iter 235 step 58 | loss train: 0.966, val: 1.061 | iter time: 101.59 ms\n",
            "Epoch 1 | iter 236 step 59 | loss train: 0.929, val: 1.061 | iter time: 132.15 ms (step)\n",
            "Epoch 1 | iter 237 step 59 | loss train: 0.915, val: 1.061 | iter time: 135.99 ms\n",
            "Epoch 1 | iter 238 step 59 | loss train: 0.857, val: 1.061 | iter time: 97.33 ms\n",
            "Epoch 1 | iter 239 step 59 | loss train: 0.929, val: 1.061 | iter time: 138.29 ms\n",
            "Epoch 1 | iter 240 step 60 | loss train: 0.869, val: 1.061 | iter time: 139.64 ms (step)\n",
            "Epoch 1 | iter 241 step 60 | loss train: 0.846, val: 1.061 | iter time: 99.63 ms\n",
            "Epoch 1 | iter 242 step 60 | loss train: 0.927, val: 1.061 | iter time: 105.44 ms\n",
            "Epoch 1 | iter 243 step 60 | loss train: 1.001, val: 1.061 | iter time: 136.00 ms\n",
            "Epoch 1 | iter 244 step 61 | loss train: 1.041, val: 1.061 | iter time: 106.24 ms (step)\n",
            "Epoch 1 | iter 245 step 61 | loss train: 1.102, val: 1.061 | iter time: 95.39 ms\n",
            "Epoch 1 | iter 246 step 61 | loss train: 1.134, val: 1.061 | iter time: 100.74 ms\n",
            "Epoch 1 | iter 247 step 61 | loss train: 1.015, val: 1.061 | iter time: 132.68 ms\n",
            "Epoch 1 | iter 248 step 62 | loss train: 1.033, val: 1.061 | iter time: 141.88 ms (step)\n",
            "Epoch 1 | iter 249 step 62 | loss train: 1.003, val: 1.061 | iter time: 105.11 ms\n",
            "Epoch 1 | iter 250 step 62 | loss train: 0.923, val: 1.061 | iter time: 106.40 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  163087\n",
            "| - Tokens w/ Prompt          :  210770\n",
            "| - Total Tokens (w/ Padding) :  360976\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  41.98 s\n",
            "| - Tok/sec                   :  8598.77 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  8.00 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n",
            "Saving LoRA weights to 'out/finetune/lora-tiny-llama-1.1b/final/lit_model.pth.lora'\n",
            "{'checkpoint_dir': PosixPath('out/finetune/lora-tiny-llama-1.1b/final'),\n",
            " 'precision': None,\n",
            " 'pretrained_checkpoint_dir': None}\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / \"lit_model.pth\"), mmap=True)\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_checkpoint = torch.load(str(lora_path), mmap=True)\n",
            "Saved merged weights to 'out/finetune/lora-tiny-llama-1.1b/final/lit_model.pth'\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune --config config/tiny-llama-lora.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFhP1ZCJ04lF"
      },
      "source": [
        "The memory required is *shockingly* small! We can see it with our 3B and 7B models, too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cygq_Fo04lF",
        "outputId": "5eb4c16e-997d-4431-cda0-7eeb29b87410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_3b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7472f0b26c90>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'lora_alpha': 16,\r\n",
            " 'lora_dropout': 0.05,\r\n",
            " 'lora_head': True,\r\n",
            " 'lora_key': True,\r\n",
            " 'lora_mlp': True,\r\n",
            " 'lora_projection': True,\r\n",
            " 'lora_query': True,\r\n",
            " 'lora_r': 32,\r\n",
            " 'lora_value': True,\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/lora-open-llama-3b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'quantize': None,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=16,\r\n",
            "                    micro_batch_size=4,\r\n",
            "                    lr_warmup_steps=10,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 51,978,240\n",
            "Number of non-trainable parameters: 3,426,473,600\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.873, val: n/a | iter time: 438.58 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.704, val: n/a | iter time: 244.94 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.892, val: n/a | iter time: 113.69 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.922, val: n/a | iter time: 222.19 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.953, val: n/a | iter time: 264.00 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 2.237, val: n/a | iter time: 109.58 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 2.167, val: n/a | iter time: 223.98 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 2.037, val: n/a | iter time: 351.84 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.890, val: n/a | iter time: 264.82 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.675, val: n/a | iter time: 228.36 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.853, val: n/a | iter time: 107.39 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.928, val: n/a | iter time: 352.71 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.981, val: n/a | iter time: 344.75 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.974, val: n/a | iter time: 328.70 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.789, val: n/a | iter time: 296.29 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.729, val: n/a | iter time: 353.34 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.818, val: n/a | iter time: 261.35 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.815, val: n/a | iter time: 174.51 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.765, val: n/a | iter time: 269.16 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.878, val: n/a | iter time: 237.10 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.953, val: n/a | iter time: 105.82 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.913, val: n/a | iter time: 300.05 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.855, val: n/a | iter time: 350.45 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.797, val: n/a | iter time: 313.23 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.639, val: n/a | iter time: 237.98 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.626, val: n/a | iter time: 348.89 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.690, val: n/a | iter time: 314.85 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.864, val: n/a | iter time: 111.83 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.786, val: n/a | iter time: 343.48 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.926, val: n/a | iter time: 150.88 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.944, val: n/a | iter time: 173.36 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.848, val: n/a | iter time: 168.67 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.906, val: n/a | iter time: 350.72 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.817, val: n/a | iter time: 175.73 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.926, val: n/a | iter time: 111.83 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.676, val: n/a | iter time: 345.89 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.715, val: n/a | iter time: 148.84 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.654, val: n/a | iter time: 347.63 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.626, val: n/a | iter time: 107.65 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.717, val: n/a | iter time: 301.43 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.613, val: n/a | iter time: 335.08 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.591, val: n/a | iter time: 266.22 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.372, val: n/a | iter time: 228.05 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.461, val: n/a | iter time: 232.23 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.576, val: n/a | iter time: 105.54 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.604, val: n/a | iter time: 233.34 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.608, val: n/a | iter time: 313.72 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.485, val: n/a | iter time: 279.28 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.343, val: n/a | iter time: 250.42 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.234, val: n/a | iter time: 351.33 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.304, val: n/a | iter time: 166.70 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.346, val: n/a | iter time: 299.64 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.329, val: n/a | iter time: 111.09 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.376, val: n/a | iter time: 339.31 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.254, val: n/a | iter time: 266.17 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.187, val: n/a | iter time: 357.09 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.203, val: n/a | iter time: 173.68 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 1.202, val: n/a | iter time: 334.74 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 1.180, val: n/a | iter time: 305.47 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.191, val: n/a | iter time: 258.30 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.198, val: n/a | iter time: 297.11 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.158, val: n/a | iter time: 351.10 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.233, val: n/a | iter time: 342.77 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.190, val: n/a | iter time: 151.11 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.187, val: n/a | iter time: 324.39 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.159, val: n/a | iter time: 213.01 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.144, val: n/a | iter time: 264.58 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.180, val: n/a | iter time: 309.65 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.148, val: n/a | iter time: 260.52 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.193, val: n/a | iter time: 238.77 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.145, val: n/a | iter time: 294.92 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.090, val: n/a | iter time: 296.92 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.113, val: n/a | iter time: 126.86 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.117, val: n/a | iter time: 265.88 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.147, val: n/a | iter time: 306.88 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.110, val: n/a | iter time: 113.10 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 1.077, val: n/a | iter time: 273.06 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 1.040, val: n/a | iter time: 240.64 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.967, val: n/a | iter time: 303.09 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 1.034, val: n/a | iter time: 184.80 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.976, val: n/a | iter time: 128.41 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 1.051, val: n/a | iter time: 241.10 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 1.000, val: n/a | iter time: 107.77 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 0.958, val: n/a | iter time: 357.14 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.137, val: n/a | iter time: 242.93 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 0.999, val: n/a | iter time: 350.81 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.112, val: n/a | iter time: 236.59 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.145, val: n/a | iter time: 232.05 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 1.074, val: n/a | iter time: 301.16 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.130, val: n/a | iter time: 263.52 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.153, val: n/a | iter time: 300.41 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 1.159, val: n/a | iter time: 212.80 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.147, val: n/a | iter time: 321.86 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 1.140, val: n/a | iter time: 151.47 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.264, val: n/a | iter time: 352.76 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.224, val: n/a | iter time: 295.82 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.222, val: n/a | iter time: 350.01 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.228, val: n/a | iter time: 270.10 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 1.065, val: n/a | iter time: 237.83 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.124, val: n/a | iter time: 246.79 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            "\n",
            "There are many things that are 'frightening' about bats. One is the way they can fly so fast in the dark and without a flapping wing, and with such relative ease. Then comes the fact they can fly through confined spaces and at night without being seen, and that's a scary thought itself.\n",
            "\n",
            "iter 100: val loss 1.0814, val time: 12926.44 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 1.097, val: 1.081 | iter time: 329.87 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 1.081, val: 1.081 | iter time: 216.99 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.112, val: 1.081 | iter time: 216.53 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.135, val: 1.081 | iter time: 318.17 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.146, val: 1.081 | iter time: 241.24 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.185, val: 1.081 | iter time: 332.40 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.152, val: 1.081 | iter time: 301.99 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 1.073, val: 1.081 | iter time: 216.30 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 1.022, val: 1.081 | iter time: 104.62 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 0.983, val: 1.081 | iter time: 165.34 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 0.999, val: 1.081 | iter time: 238.71 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 0.973, val: 1.081 | iter time: 156.94 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.067, val: 1.081 | iter time: 153.89 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.069, val: 1.081 | iter time: 249.91 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 1.051, val: 1.081 | iter time: 267.43 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 1.102, val: 1.081 | iter time: 302.02 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.979, val: 1.081 | iter time: 102.86 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.974, val: 1.081 | iter time: 248.90 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 1.029, val: 1.081 | iter time: 334.13 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 1.062, val: 1.081 | iter time: 180.37 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 1.142, val: 1.081 | iter time: 238.77 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.208, val: 1.081 | iter time: 175.80 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.135, val: 1.081 | iter time: 334.66 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.150, val: 1.081 | iter time: 253.70 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.122, val: 1.081 | iter time: 214.25 ms\n",
            "Epoch 1 | iter 126 step 31 | loss train: 1.052, val: 1.081 | iter time: 301.91 ms\n",
            "Epoch 1 | iter 127 step 31 | loss train: 1.110, val: 1.081 | iter time: 290.65 ms\n",
            "Epoch 1 | iter 128 step 32 | loss train: 1.101, val: 1.081 | iter time: 309.34 ms (step)\n",
            "Epoch 1 | iter 129 step 32 | loss train: 1.143, val: 1.081 | iter time: 241.92 ms\n",
            "Epoch 1 | iter 130 step 32 | loss train: 1.223, val: 1.081 | iter time: 306.60 ms\n",
            "Epoch 1 | iter 131 step 32 | loss train: 1.203, val: 1.081 | iter time: 305.38 ms\n",
            "Epoch 1 | iter 132 step 33 | loss train: 1.167, val: 1.081 | iter time: 266.84 ms (step)\n",
            "Epoch 1 | iter 133 step 33 | loss train: 1.168, val: 1.081 | iter time: 293.98 ms\n",
            "Epoch 1 | iter 134 step 33 | loss train: 1.087, val: 1.081 | iter time: 327.76 ms\n",
            "Epoch 1 | iter 135 step 33 | loss train: 1.075, val: 1.081 | iter time: 326.20 ms\n",
            "Epoch 1 | iter 136 step 34 | loss train: 1.102, val: 1.081 | iter time: 358.42 ms (step)\n",
            "Epoch 1 | iter 137 step 34 | loss train: 1.036, val: 1.081 | iter time: 249.94 ms\n",
            "Epoch 1 | iter 138 step 34 | loss train: 1.050, val: 1.081 | iter time: 243.58 ms\n",
            "Epoch 1 | iter 139 step 34 | loss train: 1.049, val: 1.081 | iter time: 187.93 ms\n",
            "Epoch 1 | iter 140 step 35 | loss train: 1.039, val: 1.081 | iter time: 186.46 ms (step)\n",
            "Epoch 1 | iter 141 step 35 | loss train: 1.069, val: 1.081 | iter time: 241.43 ms\n",
            "Epoch 1 | iter 142 step 35 | loss train: 1.006, val: 1.081 | iter time: 107.96 ms\n",
            "Epoch 1 | iter 143 step 35 | loss train: 0.996, val: 1.081 | iter time: 311.44 ms\n",
            "Epoch 1 | iter 144 step 36 | loss train: 0.938, val: 1.081 | iter time: 230.83 ms (step)\n",
            "Epoch 1 | iter 145 step 36 | loss train: 0.910, val: 1.081 | iter time: 218.33 ms\n",
            "Epoch 1 | iter 146 step 36 | loss train: 1.019, val: 1.081 | iter time: 291.77 ms\n",
            "Epoch 1 | iter 147 step 36 | loss train: 1.046, val: 1.081 | iter time: 353.83 ms\n",
            "Epoch 1 | iter 148 step 37 | loss train: 1.045, val: 1.081 | iter time: 272.70 ms (step)\n",
            "Epoch 1 | iter 149 step 37 | loss train: 1.076, val: 1.081 | iter time: 261.73 ms\n",
            "Epoch 1 | iter 150 step 37 | loss train: 1.065, val: 1.081 | iter time: 354.05 ms\n",
            "Epoch 1 | iter 151 step 37 | loss train: 1.005, val: 1.081 | iter time: 262.63 ms\n",
            "Epoch 1 | iter 152 step 38 | loss train: 0.974, val: 1.081 | iter time: 246.94 ms (step)\n",
            "Epoch 1 | iter 153 step 38 | loss train: 0.949, val: 1.081 | iter time: 259.79 ms\n",
            "Epoch 1 | iter 154 step 38 | loss train: 0.915, val: 1.081 | iter time: 146.12 ms\n",
            "Epoch 1 | iter 155 step 38 | loss train: 0.949, val: 1.081 | iter time: 339.23 ms\n",
            "Epoch 1 | iter 156 step 39 | loss train: 1.042, val: 1.081 | iter time: 334.09 ms (step)\n",
            "Epoch 1 | iter 157 step 39 | loss train: 1.067, val: 1.081 | iter time: 210.79 ms\n",
            "Epoch 1 | iter 158 step 39 | loss train: 1.012, val: 1.081 | iter time: 143.63 ms\n",
            "Epoch 1 | iter 159 step 39 | loss train: 1.001, val: 1.081 | iter time: 310.68 ms\n",
            "Epoch 1 | iter 160 step 40 | loss train: 0.964, val: 1.081 | iter time: 307.31 ms (step)\n",
            "Epoch 1 | iter 161 step 40 | loss train: 0.922, val: 1.081 | iter time: 172.74 ms\n",
            "Epoch 1 | iter 162 step 40 | loss train: 0.900, val: 1.081 | iter time: 146.24 ms\n",
            "Epoch 1 | iter 163 step 40 | loss train: 0.976, val: 1.081 | iter time: 264.97 ms\n",
            "Epoch 1 | iter 164 step 41 | loss train: 1.043, val: 1.081 | iter time: 214.62 ms (step)\n",
            "Epoch 1 | iter 165 step 41 | loss train: 1.047, val: 1.081 | iter time: 293.97 ms\n",
            "Epoch 1 | iter 166 step 41 | loss train: 1.038, val: 1.081 | iter time: 108.94 ms\n",
            "Epoch 1 | iter 167 step 41 | loss train: 1.001, val: 1.081 | iter time: 306.13 ms\n",
            "Epoch 1 | iter 168 step 42 | loss train: 0.983, val: 1.081 | iter time: 237.67 ms (step)\n",
            "Epoch 1 | iter 169 step 42 | loss train: 1.003, val: 1.081 | iter time: 237.93 ms\n",
            "Epoch 1 | iter 170 step 42 | loss train: 1.087, val: 1.081 | iter time: 181.25 ms\n",
            "Epoch 1 | iter 171 step 42 | loss train: 1.021, val: 1.081 | iter time: 120.85 ms\n",
            "Epoch 1 | iter 172 step 43 | loss train: 1.013, val: 1.081 | iter time: 354.10 ms (step)\n",
            "Epoch 1 | iter 173 step 43 | loss train: 1.010, val: 1.081 | iter time: 262.58 ms\n",
            "Epoch 1 | iter 174 step 43 | loss train: 1.112, val: 1.081 | iter time: 166.77 ms\n",
            "Epoch 1 | iter 175 step 43 | loss train: 1.188, val: 1.081 | iter time: 240.47 ms\n",
            "Epoch 1 | iter 176 step 44 | loss train: 1.181, val: 1.081 | iter time: 270.70 ms (step)\n",
            "Epoch 1 | iter 177 step 44 | loss train: 1.214, val: 1.081 | iter time: 231.56 ms\n",
            "Epoch 1 | iter 178 step 44 | loss train: 1.088, val: 1.081 | iter time: 174.63 ms\n",
            "Epoch 1 | iter 179 step 44 | loss train: 1.080, val: 1.081 | iter time: 339.80 ms\n",
            "Epoch 1 | iter 180 step 45 | loss train: 1.088, val: 1.081 | iter time: 277.26 ms (step)\n",
            "Epoch 1 | iter 181 step 45 | loss train: 1.029, val: 1.081 | iter time: 348.20 ms\n",
            "Epoch 1 | iter 182 step 45 | loss train: 1.076, val: 1.081 | iter time: 229.04 ms\n",
            "Epoch 1 | iter 183 step 45 | loss train: 0.948, val: 1.081 | iter time: 238.64 ms\n",
            "Epoch 1 | iter 184 step 46 | loss train: 0.895, val: 1.081 | iter time: 250.49 ms (step)\n",
            "Epoch 1 | iter 185 step 46 | loss train: 0.947, val: 1.081 | iter time: 230.37 ms\n",
            "Epoch 1 | iter 186 step 46 | loss train: 0.984, val: 1.081 | iter time: 182.93 ms\n",
            "Epoch 1 | iter 187 step 46 | loss train: 1.066, val: 1.081 | iter time: 170.77 ms\n",
            "Epoch 1 | iter 188 step 47 | loss train: 1.065, val: 1.081 | iter time: 275.43 ms (step)\n",
            "Epoch 1 | iter 189 step 47 | loss train: 1.096, val: 1.081 | iter time: 267.67 ms\n",
            "Epoch 1 | iter 190 step 47 | loss train: 1.031, val: 1.081 | iter time: 265.58 ms\n",
            "Epoch 1 | iter 191 step 47 | loss train: 1.038, val: 1.081 | iter time: 296.32 ms\n",
            "Epoch 1 | iter 192 step 48 | loss train: 1.083, val: 1.081 | iter time: 355.13 ms (step)\n",
            "Epoch 1 | iter 193 step 48 | loss train: 1.008, val: 1.081 | iter time: 212.32 ms\n",
            "Epoch 1 | iter 194 step 48 | loss train: 1.105, val: 1.081 | iter time: 318.80 ms\n",
            "Epoch 1 | iter 195 step 48 | loss train: 1.088, val: 1.081 | iter time: 339.98 ms\n",
            "Epoch 1 | iter 196 step 49 | loss train: 1.033, val: 1.081 | iter time: 111.92 ms (step)\n",
            "Epoch 1 | iter 197 step 49 | loss train: 1.091, val: 1.081 | iter time: 244.31 ms\n",
            "Epoch 1 | iter 198 step 49 | loss train: 1.038, val: 1.081 | iter time: 305.63 ms\n",
            "Epoch 1 | iter 199 step 49 | loss train: 0.990, val: 1.081 | iter time: 107.63 ms\n",
            "Epoch 1 | iter 200 step 50 | loss train: 0.961, val: 1.081 | iter time: 354.50 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " Here's a dog on a plane: Can't believe my luck!\n",
            "\n",
            "\n",
            "iter 200: val loss 1.0497, val time: 11047.24 ms\n",
            "Epoch 1 | iter 201 step 50 | loss train: 0.984, val: 1.050 | iter time: 166.91 ms\n",
            "Epoch 1 | iter 202 step 50 | loss train: 0.940, val: 1.050 | iter time: 134.48 ms\n",
            "Epoch 1 | iter 203 step 50 | loss train: 1.019, val: 1.050 | iter time: 130.82 ms\n",
            "Epoch 1 | iter 204 step 51 | loss train: 1.033, val: 1.050 | iter time: 353.54 ms (step)\n",
            "Epoch 1 | iter 205 step 51 | loss train: 1.043, val: 1.050 | iter time: 289.52 ms\n",
            "Epoch 1 | iter 206 step 51 | loss train: 1.143, val: 1.050 | iter time: 210.15 ms\n",
            "Epoch 1 | iter 207 step 51 | loss train: 1.111, val: 1.050 | iter time: 213.01 ms\n",
            "Epoch 1 | iter 208 step 52 | loss train: 1.275, val: 1.050 | iter time: 297.76 ms (step)\n",
            "Epoch 1 | iter 209 step 52 | loss train: 1.259, val: 1.050 | iter time: 231.47 ms\n",
            "Epoch 1 | iter 210 step 52 | loss train: 1.196, val: 1.050 | iter time: 246.96 ms\n",
            "Epoch 1 | iter 211 step 52 | loss train: 1.228, val: 1.050 | iter time: 164.49 ms\n",
            "Epoch 1 | iter 212 step 53 | loss train: 1.078, val: 1.050 | iter time: 315.27 ms (step)\n",
            "Epoch 1 | iter 213 step 53 | loss train: 1.024, val: 1.050 | iter time: 212.81 ms\n",
            "Epoch 1 | iter 214 step 53 | loss train: 0.982, val: 1.050 | iter time: 188.26 ms\n",
            "Epoch 1 | iter 215 step 53 | loss train: 0.983, val: 1.050 | iter time: 186.08 ms\n",
            "Epoch 1 | iter 216 step 54 | loss train: 1.022, val: 1.050 | iter time: 265.94 ms (step)\n",
            "Epoch 1 | iter 217 step 54 | loss train: 1.083, val: 1.050 | iter time: 323.31 ms\n",
            "Epoch 1 | iter 218 step 54 | loss train: 1.126, val: 1.050 | iter time: 221.20 ms\n",
            "Epoch 1 | iter 219 step 54 | loss train: 1.082, val: 1.050 | iter time: 226.52 ms\n",
            "Epoch 1 | iter 220 step 55 | loss train: 1.097, val: 1.050 | iter time: 136.61 ms (step)\n",
            "Epoch 1 | iter 221 step 55 | loss train: 1.073, val: 1.050 | iter time: 221.66 ms\n",
            "Epoch 1 | iter 222 step 55 | loss train: 1.052, val: 1.050 | iter time: 294.93 ms\n",
            "Epoch 1 | iter 223 step 55 | loss train: 1.106, val: 1.050 | iter time: 340.53 ms\n",
            "Epoch 1 | iter 224 step 56 | loss train: 1.099, val: 1.050 | iter time: 294.91 ms (step)\n",
            "Epoch 1 | iter 225 step 56 | loss train: 1.017, val: 1.050 | iter time: 348.52 ms\n",
            "Epoch 1 | iter 226 step 56 | loss train: 1.000, val: 1.050 | iter time: 146.57 ms\n",
            "Epoch 1 | iter 227 step 56 | loss train: 0.979, val: 1.050 | iter time: 246.44 ms\n",
            "Epoch 1 | iter 228 step 57 | loss train: 0.978, val: 1.050 | iter time: 267.04 ms (step)\n",
            "Epoch 1 | iter 229 step 57 | loss train: 1.068, val: 1.050 | iter time: 143.71 ms\n",
            "Epoch 1 | iter 230 step 57 | loss train: 1.023, val: 1.050 | iter time: 342.34 ms\n",
            "Epoch 1 | iter 231 step 57 | loss train: 0.952, val: 1.050 | iter time: 130.69 ms\n",
            "Epoch 1 | iter 232 step 58 | loss train: 0.975, val: 1.050 | iter time: 336.76 ms (step)\n",
            "Epoch 1 | iter 233 step 58 | loss train: 0.935, val: 1.050 | iter time: 293.56 ms\n",
            "Epoch 1 | iter 234 step 58 | loss train: 0.982, val: 1.050 | iter time: 255.89 ms\n",
            "Epoch 1 | iter 235 step 58 | loss train: 0.959, val: 1.050 | iter time: 164.84 ms\n",
            "Epoch 1 | iter 236 step 59 | loss train: 0.943, val: 1.050 | iter time: 293.86 ms (step)\n",
            "Epoch 1 | iter 237 step 59 | loss train: 0.934, val: 1.050 | iter time: 348.91 ms\n",
            "Epoch 1 | iter 238 step 59 | loss train: 0.870, val: 1.050 | iter time: 134.33 ms\n",
            "Epoch 1 | iter 239 step 59 | loss train: 0.943, val: 1.050 | iter time: 352.13 ms\n",
            "Epoch 1 | iter 240 step 60 | loss train: 0.873, val: 1.050 | iter time: 347.00 ms (step)\n",
            "Epoch 1 | iter 241 step 60 | loss train: 0.852, val: 1.050 | iter time: 103.86 ms\n",
            "Epoch 1 | iter 242 step 60 | loss train: 0.929, val: 1.050 | iter time: 147.15 ms\n",
            "Epoch 1 | iter 243 step 60 | loss train: 1.029, val: 1.050 | iter time: 319.08 ms\n",
            "Epoch 1 | iter 244 step 61 | loss train: 1.054, val: 1.050 | iter time: 268.15 ms (step)\n",
            "Epoch 1 | iter 245 step 61 | loss train: 1.123, val: 1.050 | iter time: 226.80 ms\n",
            "Epoch 1 | iter 246 step 61 | loss train: 1.151, val: 1.050 | iter time: 173.69 ms\n",
            "Epoch 1 | iter 247 step 61 | loss train: 1.010, val: 1.050 | iter time: 307.92 ms\n",
            "Epoch 1 | iter 248 step 62 | loss train: 1.047, val: 1.050 | iter time: 314.79 ms (step)\n",
            "Epoch 1 | iter 249 step 62 | loss train: 0.980, val: 1.050 | iter time: 263.22 ms\n",
            "Epoch 1 | iter 250 step 62 | loss train: 0.914, val: 1.050 | iter time: 261.20 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  151019\n",
            "| - Tokens w/ Prompt          :  195803\n",
            "| - Total Tokens (w/ Padding) :  336060\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  89.06 s\n",
            "| - Tok/sec                   :  3773.44 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  17.02 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n",
            "Saving LoRA weights to 'out/finetune/lora-open-llama-3b/final/lit_model.pth.lora'\n",
            "{'checkpoint_dir': PosixPath('out/finetune/lora-open-llama-3b/final'),\n",
            " 'precision': None,\n",
            " 'pretrained_checkpoint_dir': None}\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / \"lit_model.pth\"), mmap=True)\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_checkpoint = torch.load(str(lora_path), mmap=True)\n",
            "Saved merged weights to 'out/finetune/lora-open-llama-3b/final/lit_model.pth'\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune --config config/open-llama-3b-lora.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtoIapbb04lF",
        "outputId": "6146988f-1e66-41db-9988-10a3cc4c8381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_7b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7df3bf9761b0>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'lora_alpha': 16,\r\n",
            " 'lora_dropout': 0.05,\r\n",
            " 'lora_head': True,\r\n",
            " 'lora_key': True,\r\n",
            " 'lora_mlp': True,\r\n",
            " 'lora_projection': True,\r\n",
            " 'lora_query': True,\r\n",
            " 'lora_r': 32,\r\n",
            " 'lora_value': True,\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/lora-open-llama-7b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'quantize': None,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=16,\r\n",
            "                    micro_batch_size=4,\r\n",
            "                    lr_warmup_steps=10,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 81,108,992\n",
            "Number of non-trainable parameters: 6,738,415,616\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.788, val: n/a | iter time: 555.83 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.633, val: n/a | iter time: 368.14 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.828, val: n/a | iter time: 170.68 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.853, val: n/a | iter time: 335.17 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.882, val: n/a | iter time: 424.14 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 2.152, val: n/a | iter time: 134.11 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 2.077, val: n/a | iter time: 342.05 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.938, val: n/a | iter time: 570.83 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.791, val: n/a | iter time: 426.89 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.585, val: n/a | iter time: 350.64 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.751, val: n/a | iter time: 155.22 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.837, val: n/a | iter time: 573.34 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.896, val: n/a | iter time: 562.95 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.888, val: n/a | iter time: 532.85 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.716, val: n/a | iter time: 460.12 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.653, val: n/a | iter time: 571.12 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.740, val: n/a | iter time: 420.89 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.738, val: n/a | iter time: 287.67 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.681, val: n/a | iter time: 430.04 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.792, val: n/a | iter time: 363.23 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.866, val: n/a | iter time: 169.36 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.821, val: n/a | iter time: 464.17 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.763, val: n/a | iter time: 568.96 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.709, val: n/a | iter time: 511.94 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.560, val: n/a | iter time: 365.96 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.543, val: n/a | iter time: 568.79 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.618, val: n/a | iter time: 519.36 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.788, val: n/a | iter time: 164.69 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.699, val: n/a | iter time: 563.96 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.850, val: n/a | iter time: 231.13 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.848, val: n/a | iter time: 285.00 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.745, val: n/a | iter time: 272.54 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.805, val: n/a | iter time: 570.22 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.710, val: n/a | iter time: 286.06 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.808, val: n/a | iter time: 173.67 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.573, val: n/a | iter time: 560.73 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.599, val: n/a | iter time: 229.88 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.528, val: n/a | iter time: 566.08 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.494, val: n/a | iter time: 142.72 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.571, val: n/a | iter time: 467.39 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.469, val: n/a | iter time: 549.13 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.441, val: n/a | iter time: 426.44 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.251, val: n/a | iter time: 351.66 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.328, val: n/a | iter time: 354.20 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.431, val: n/a | iter time: 163.93 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.467, val: n/a | iter time: 356.71 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.467, val: n/a | iter time: 513.52 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.368, val: n/a | iter time: 445.71 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.241, val: n/a | iter time: 405.94 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.143, val: n/a | iter time: 574.50 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.191, val: n/a | iter time: 272.16 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.222, val: n/a | iter time: 461.65 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.207, val: n/a | iter time: 170.15 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.241, val: n/a | iter time: 557.68 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.143, val: n/a | iter time: 425.70 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 1.082, val: n/a | iter time: 576.46 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.100, val: n/a | iter time: 291.82 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 1.097, val: n/a | iter time: 544.83 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 1.077, val: n/a | iter time: 500.32 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.081, val: n/a | iter time: 414.43 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.082, val: n/a | iter time: 463.66 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 1.048, val: n/a | iter time: 572.86 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.104, val: n/a | iter time: 557.89 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 1.059, val: n/a | iter time: 229.89 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 1.058, val: n/a | iter time: 531.12 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 1.010, val: n/a | iter time: 327.83 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 1.016, val: n/a | iter time: 424.63 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.065, val: n/a | iter time: 505.18 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 1.050, val: n/a | iter time: 416.74 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.112, val: n/a | iter time: 367.44 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 1.061, val: n/a | iter time: 459.44 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 1.013, val: n/a | iter time: 461.00 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 1.021, val: n/a | iter time: 192.98 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 1.025, val: n/a | iter time: 427.48 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 1.053, val: n/a | iter time: 507.26 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 1.021, val: n/a | iter time: 177.89 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 1.013, val: n/a | iter time: 433.92 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 0.979, val: n/a | iter time: 370.06 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.912, val: n/a | iter time: 472.46 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 0.971, val: n/a | iter time: 307.94 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.903, val: n/a | iter time: 197.78 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 0.984, val: n/a | iter time: 368.75 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.945, val: n/a | iter time: 160.39 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 0.905, val: n/a | iter time: 577.42 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 1.067, val: n/a | iter time: 393.44 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 0.928, val: n/a | iter time: 567.70 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 1.015, val: n/a | iter time: 363.96 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 1.049, val: n/a | iter time: 356.81 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 0.985, val: n/a | iter time: 470.35 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 1.041, val: n/a | iter time: 422.25 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.080, val: n/a | iter time: 465.15 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 1.075, val: n/a | iter time: 325.23 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.067, val: n/a | iter time: 527.54 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 1.054, val: n/a | iter time: 231.50 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.184, val: n/a | iter time: 574.34 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.158, val: n/a | iter time: 458.24 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.151, val: n/a | iter time: 569.02 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.163, val: n/a | iter time: 431.53 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 0.991, val: n/a | iter time: 365.82 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 1.030, val: n/a | iter time: 395.92 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"I heard Tom Holland is a fan of double denim, he'll be wearing this all night!\"\n",
            "\n",
            "iter 100: val loss 1.0088, val time: 18044.66 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 1.005, val: 1.009 | iter time: 536.31 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.987, val: 1.009 | iter time: 333.41 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 1.014, val: 1.009 | iter time: 334.26 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 1.057, val: 1.009 | iter time: 518.48 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 1.068, val: 1.009 | iter time: 390.00 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.106, val: 1.009 | iter time: 536.92 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.082, val: 1.009 | iter time: 468.08 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 0.997, val: 1.009 | iter time: 331.93 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 0.934, val: 1.009 | iter time: 143.38 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 0.903, val: 1.009 | iter time: 272.43 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 0.923, val: 1.009 | iter time: 365.68 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 0.901, val: 1.009 | iter time: 240.62 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 1.009, val: 1.009 | iter time: 234.96 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 1.010, val: 1.009 | iter time: 403.89 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.983, val: 1.009 | iter time: 426.94 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 1.031, val: 1.009 | iter time: 468.88 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.908, val: 1.009 | iter time: 157.62 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.900, val: 1.009 | iter time: 402.00 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 0.954, val: 1.009 | iter time: 544.78 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.979, val: 1.009 | iter time: 300.73 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 1.057, val: 1.009 | iter time: 365.72 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.116, val: 1.009 | iter time: 295.29 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 1.041, val: 1.009 | iter time: 545.40 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.064, val: 1.009 | iter time: 410.19 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 1.049, val: 1.009 | iter time: 327.79 ms\n",
            "Epoch 1 | iter 126 step 31 | loss train: 0.986, val: 1.009 | iter time: 467.03 ms\n",
            "Epoch 1 | iter 127 step 31 | loss train: 1.046, val: 1.009 | iter time: 451.87 ms\n",
            "Epoch 1 | iter 128 step 32 | loss train: 1.042, val: 1.009 | iter time: 478.89 ms (step)\n",
            "Epoch 1 | iter 129 step 32 | loss train: 1.076, val: 1.009 | iter time: 389.34 ms\n",
            "Epoch 1 | iter 130 step 32 | loss train: 1.156, val: 1.009 | iter time: 476.66 ms\n",
            "Epoch 1 | iter 131 step 32 | loss train: 1.134, val: 1.009 | iter time: 499.12 ms\n",
            "Epoch 1 | iter 132 step 33 | loss train: 1.096, val: 1.009 | iter time: 424.56 ms (step)\n",
            "Epoch 1 | iter 133 step 33 | loss train: 1.074, val: 1.009 | iter time: 457.94 ms\n",
            "Epoch 1 | iter 134 step 33 | loss train: 0.993, val: 1.009 | iter time: 532.10 ms\n",
            "Epoch 1 | iter 135 step 33 | loss train: 0.985, val: 1.009 | iter time: 530.52 ms\n",
            "Epoch 1 | iter 136 step 34 | loss train: 1.013, val: 1.009 | iter time: 580.20 ms (step)\n",
            "Epoch 1 | iter 137 step 34 | loss train: 0.948, val: 1.009 | iter time: 404.91 ms\n",
            "Epoch 1 | iter 138 step 34 | loss train: 0.964, val: 1.009 | iter time: 392.94 ms\n",
            "Epoch 1 | iter 139 step 34 | loss train: 0.955, val: 1.009 | iter time: 312.05 ms\n",
            "Epoch 1 | iter 140 step 35 | loss train: 0.940, val: 1.009 | iter time: 309.80 ms (step)\n",
            "Epoch 1 | iter 141 step 35 | loss train: 0.980, val: 1.009 | iter time: 368.12 ms\n",
            "Epoch 1 | iter 142 step 35 | loss train: 0.926, val: 1.009 | iter time: 142.87 ms\n",
            "Epoch 1 | iter 143 step 35 | loss train: 0.927, val: 1.009 | iter time: 512.30 ms\n",
            "Epoch 1 | iter 144 step 36 | loss train: 0.871, val: 1.009 | iter time: 352.83 ms (step)\n",
            "Epoch 1 | iter 145 step 36 | loss train: 0.858, val: 1.009 | iter time: 336.27 ms\n",
            "Epoch 1 | iter 146 step 36 | loss train: 0.958, val: 1.009 | iter time: 452.44 ms\n",
            "Epoch 1 | iter 147 step 36 | loss train: 0.984, val: 1.009 | iter time: 572.74 ms\n",
            "Epoch 1 | iter 148 step 37 | loss train: 0.980, val: 1.009 | iter time: 436.23 ms (step)\n",
            "Epoch 1 | iter 149 step 37 | loss train: 1.005, val: 1.009 | iter time: 418.39 ms\n",
            "Epoch 1 | iter 150 step 37 | loss train: 1.000, val: 1.009 | iter time: 573.61 ms\n",
            "Epoch 1 | iter 151 step 37 | loss train: 0.945, val: 1.009 | iter time: 421.51 ms\n",
            "Epoch 1 | iter 152 step 38 | loss train: 0.928, val: 1.009 | iter time: 396.16 ms (step)\n",
            "Epoch 1 | iter 153 step 38 | loss train: 0.902, val: 1.009 | iter time: 416.37 ms\n",
            "Epoch 1 | iter 154 step 38 | loss train: 0.866, val: 1.009 | iter time: 223.28 ms\n",
            "Epoch 1 | iter 155 step 38 | loss train: 0.890, val: 1.009 | iter time: 554.36 ms\n",
            "Epoch 1 | iter 156 step 39 | loss train: 0.970, val: 1.009 | iter time: 540.26 ms (step)\n",
            "Epoch 1 | iter 157 step 39 | loss train: 0.996, val: 1.009 | iter time: 324.85 ms\n",
            "Epoch 1 | iter 158 step 39 | loss train: 0.931, val: 1.009 | iter time: 219.38 ms\n",
            "Epoch 1 | iter 159 step 39 | loss train: 0.921, val: 1.009 | iter time: 510.32 ms\n",
            "Epoch 1 | iter 160 step 40 | loss train: 0.882, val: 1.009 | iter time: 475.27 ms (step)\n",
            "Epoch 1 | iter 161 step 40 | loss train: 0.835, val: 1.009 | iter time: 283.77 ms\n",
            "Epoch 1 | iter 162 step 40 | loss train: 0.834, val: 1.009 | iter time: 223.88 ms\n",
            "Epoch 1 | iter 163 step 40 | loss train: 0.902, val: 1.009 | iter time: 424.16 ms\n",
            "Epoch 1 | iter 164 step 41 | loss train: 0.983, val: 1.009 | iter time: 327.59 ms (step)\n",
            "Epoch 1 | iter 165 step 41 | loss train: 0.996, val: 1.009 | iter time: 457.29 ms\n",
            "Epoch 1 | iter 166 step 41 | loss train: 0.981, val: 1.009 | iter time: 172.59 ms\n",
            "Epoch 1 | iter 167 step 41 | loss train: 0.954, val: 1.009 | iter time: 502.51 ms\n",
            "Epoch 1 | iter 168 step 42 | loss train: 0.920, val: 1.009 | iter time: 363.45 ms (step)\n",
            "Epoch 1 | iter 169 step 42 | loss train: 0.934, val: 1.009 | iter time: 365.05 ms\n",
            "Epoch 1 | iter 170 step 42 | loss train: 1.020, val: 1.009 | iter time: 302.71 ms\n",
            "Epoch 1 | iter 171 step 42 | loss train: 0.956, val: 1.009 | iter time: 186.01 ms\n",
            "Epoch 1 | iter 172 step 43 | loss train: 0.956, val: 1.009 | iter time: 574.66 ms (step)\n",
            "Epoch 1 | iter 173 step 43 | loss train: 0.949, val: 1.009 | iter time: 419.23 ms\n",
            "Epoch 1 | iter 174 step 43 | loss train: 1.031, val: 1.009 | iter time: 272.92 ms\n",
            "Epoch 1 | iter 175 step 43 | loss train: 1.112, val: 1.009 | iter time: 370.25 ms\n",
            "Epoch 1 | iter 176 step 44 | loss train: 1.101, val: 1.009 | iter time: 430.39 ms (step)\n",
            "Epoch 1 | iter 177 step 44 | loss train: 1.135, val: 1.009 | iter time: 356.15 ms\n",
            "Epoch 1 | iter 178 step 44 | loss train: 1.029, val: 1.009 | iter time: 285.31 ms\n",
            "Epoch 1 | iter 179 step 44 | loss train: 1.006, val: 1.009 | iter time: 552.48 ms\n",
            "Epoch 1 | iter 180 step 45 | loss train: 1.014, val: 1.009 | iter time: 440.09 ms (step)\n",
            "Epoch 1 | iter 181 step 45 | loss train: 0.965, val: 1.009 | iter time: 564.29 ms\n",
            "Epoch 1 | iter 182 step 45 | loss train: 1.001, val: 1.009 | iter time: 352.39 ms\n",
            "Epoch 1 | iter 183 step 45 | loss train: 0.885, val: 1.009 | iter time: 364.80 ms\n",
            "Epoch 1 | iter 184 step 46 | loss train: 0.834, val: 1.009 | iter time: 404.78 ms (step)\n",
            "Epoch 1 | iter 185 step 46 | loss train: 0.878, val: 1.009 | iter time: 353.05 ms\n",
            "Epoch 1 | iter 186 step 46 | loss train: 0.921, val: 1.009 | iter time: 304.94 ms\n",
            "Epoch 1 | iter 187 step 46 | loss train: 0.990, val: 1.009 | iter time: 277.66 ms\n",
            "Epoch 1 | iter 188 step 47 | loss train: 0.987, val: 1.009 | iter time: 436.94 ms (step)\n",
            "Epoch 1 | iter 189 step 47 | loss train: 1.017, val: 1.009 | iter time: 428.58 ms\n",
            "Epoch 1 | iter 190 step 47 | loss train: 0.949, val: 1.009 | iter time: 421.99 ms\n",
            "Epoch 1 | iter 191 step 47 | loss train: 0.961, val: 1.009 | iter time: 460.52 ms\n",
            "Epoch 1 | iter 192 step 48 | loss train: 1.007, val: 1.009 | iter time: 578.97 ms (step)\n",
            "Epoch 1 | iter 193 step 48 | loss train: 0.939, val: 1.009 | iter time: 325.59 ms\n",
            "Epoch 1 | iter 194 step 48 | loss train: 1.029, val: 1.009 | iter time: 521.86 ms\n",
            "Epoch 1 | iter 195 step 48 | loss train: 1.012, val: 1.009 | iter time: 554.85 ms\n",
            "Epoch 1 | iter 196 step 49 | loss train: 0.966, val: 1.009 | iter time: 170.67 ms (step)\n",
            "Epoch 1 | iter 197 step 49 | loss train: 1.022, val: 1.009 | iter time: 397.60 ms\n",
            "Epoch 1 | iter 198 step 49 | loss train: 0.964, val: 1.009 | iter time: 476.06 ms\n",
            "Epoch 1 | iter 199 step 49 | loss train: 0.926, val: 1.009 | iter time: 144.88 ms\n",
            "Epoch 1 | iter 200 step 50 | loss train: 0.886, val: 1.009 | iter time: 571.96 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " \"The only thing I'm sure of is that I'm definitely not a cat.\"\n",
            "\n",
            "iter 200: val loss 0.9798, val time: 17831.06 ms\n",
            "Epoch 1 | iter 201 step 50 | loss train: 0.898, val: 0.980 | iter time: 272.29 ms\n",
            "Epoch 1 | iter 202 step 50 | loss train: 0.869, val: 0.980 | iter time: 208.07 ms\n",
            "Epoch 1 | iter 203 step 50 | loss train: 0.956, val: 0.980 | iter time: 199.75 ms\n",
            "Epoch 1 | iter 204 step 51 | loss train: 0.975, val: 0.980 | iter time: 574.34 ms (step)\n",
            "Epoch 1 | iter 205 step 51 | loss train: 0.989, val: 0.980 | iter time: 450.05 ms\n",
            "Epoch 1 | iter 206 step 51 | loss train: 1.080, val: 0.980 | iter time: 324.18 ms\n",
            "Epoch 1 | iter 207 step 51 | loss train: 1.030, val: 0.980 | iter time: 327.29 ms\n",
            "Epoch 1 | iter 208 step 52 | loss train: 1.182, val: 0.980 | iter time: 464.45 ms (step)\n",
            "Epoch 1 | iter 209 step 52 | loss train: 1.171, val: 0.980 | iter time: 355.01 ms\n",
            "Epoch 1 | iter 210 step 52 | loss train: 1.111, val: 0.980 | iter time: 400.52 ms\n",
            "Epoch 1 | iter 211 step 52 | loss train: 1.156, val: 0.980 | iter time: 267.53 ms\n",
            "Epoch 1 | iter 212 step 53 | loss train: 1.019, val: 0.980 | iter time: 516.61 ms (step)\n",
            "Epoch 1 | iter 213 step 53 | loss train: 0.972, val: 0.980 | iter time: 326.77 ms\n",
            "Epoch 1 | iter 214 step 53 | loss train: 0.933, val: 0.980 | iter time: 316.06 ms\n",
            "Epoch 1 | iter 215 step 53 | loss train: 0.931, val: 0.980 | iter time: 310.87 ms\n",
            "Epoch 1 | iter 216 step 54 | loss train: 0.964, val: 0.980 | iter time: 425.11 ms (step)\n",
            "Epoch 1 | iter 217 step 54 | loss train: 1.014, val: 0.980 | iter time: 526.92 ms\n",
            "Epoch 1 | iter 218 step 54 | loss train: 1.056, val: 0.980 | iter time: 339.38 ms\n",
            "Epoch 1 | iter 219 step 54 | loss train: 1.006, val: 0.980 | iter time: 348.16 ms\n",
            "Epoch 1 | iter 220 step 55 | loss train: 1.027, val: 0.980 | iter time: 211.66 ms (step)\n",
            "Epoch 1 | iter 221 step 55 | loss train: 1.007, val: 0.980 | iter time: 342.30 ms\n",
            "Epoch 1 | iter 222 step 55 | loss train: 0.985, val: 0.980 | iter time: 456.22 ms\n",
            "Epoch 1 | iter 223 step 55 | loss train: 1.043, val: 0.980 | iter time: 554.78 ms\n",
            "Epoch 1 | iter 224 step 56 | loss train: 1.033, val: 0.980 | iter time: 460.11 ms (step)\n",
            "Epoch 1 | iter 225 step 56 | loss train: 0.954, val: 0.980 | iter time: 564.13 ms\n",
            "Epoch 1 | iter 226 step 56 | loss train: 0.939, val: 0.980 | iter time: 224.18 ms\n",
            "Epoch 1 | iter 227 step 56 | loss train: 0.911, val: 0.980 | iter time: 398.79 ms\n",
            "Epoch 1 | iter 228 step 57 | loss train: 0.914, val: 0.980 | iter time: 426.88 ms (step)\n",
            "Epoch 1 | iter 229 step 57 | loss train: 0.992, val: 0.980 | iter time: 219.15 ms\n",
            "Epoch 1 | iter 230 step 57 | loss train: 0.945, val: 0.980 | iter time: 557.40 ms\n",
            "Epoch 1 | iter 231 step 57 | loss train: 0.882, val: 0.980 | iter time: 201.01 ms\n",
            "Epoch 1 | iter 232 step 58 | loss train: 0.902, val: 0.980 | iter time: 549.80 ms (step)\n",
            "Epoch 1 | iter 233 step 58 | loss train: 0.871, val: 0.980 | iter time: 457.18 ms\n",
            "Epoch 1 | iter 234 step 58 | loss train: 0.925, val: 0.980 | iter time: 412.89 ms\n",
            "Epoch 1 | iter 235 step 58 | loss train: 0.901, val: 0.980 | iter time: 266.49 ms\n",
            "Epoch 1 | iter 236 step 59 | loss train: 0.890, val: 0.980 | iter time: 455.71 ms (step)\n",
            "Epoch 1 | iter 237 step 59 | loss train: 0.874, val: 0.980 | iter time: 568.13 ms\n",
            "Epoch 1 | iter 238 step 59 | loss train: 0.796, val: 0.980 | iter time: 206.97 ms\n",
            "Epoch 1 | iter 239 step 59 | loss train: 0.869, val: 0.980 | iter time: 570.87 ms\n",
            "Epoch 1 | iter 240 step 60 | loss train: 0.803, val: 0.980 | iter time: 564.99 ms (step)\n",
            "Epoch 1 | iter 241 step 60 | loss train: 0.797, val: 0.980 | iter time: 162.53 ms\n",
            "Epoch 1 | iter 242 step 60 | loss train: 0.883, val: 0.980 | iter time: 225.77 ms\n",
            "Epoch 1 | iter 243 step 60 | loss train: 0.979, val: 0.980 | iter time: 521.41 ms\n",
            "Epoch 1 | iter 244 step 61 | loss train: 1.002, val: 0.980 | iter time: 427.00 ms (step)\n",
            "Epoch 1 | iter 245 step 61 | loss train: 1.061, val: 0.980 | iter time: 348.72 ms\n",
            "Epoch 1 | iter 246 step 61 | loss train: 1.081, val: 0.980 | iter time: 286.18 ms\n",
            "Epoch 1 | iter 247 step 61 | loss train: 0.940, val: 0.980 | iter time: 503.48 ms\n",
            "Epoch 1 | iter 248 step 62 | loss train: 0.968, val: 0.980 | iter time: 515.49 ms (step)\n",
            "Epoch 1 | iter 249 step 62 | loss train: 0.908, val: 0.980 | iter time: 421.58 ms\n",
            "Epoch 1 | iter 250 step 62 | loss train: 0.850, val: 0.980 | iter time: 419.38 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  151019\n",
            "| - Tokens w/ Prompt          :  195803\n",
            "| - Total Tokens (w/ Padding) :  336060\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  138.00 s\n",
            "| - Tok/sec                   :  2435.21 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  29.99 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n",
            "Saving LoRA weights to 'out/finetune/lora-open-llama-7b/final/lit_model.pth.lora'\n",
            "{'checkpoint_dir': PosixPath('out/finetune/lora-open-llama-7b/final'),\n",
            " 'precision': None,\n",
            " 'pretrained_checkpoint_dir': None}\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / \"lit_model.pth\"), mmap=True)\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_checkpoint = torch.load(str(lora_path), mmap=True)\n",
            "Saved merged weights to 'out/finetune/lora-open-llama-7b/final/lit_model.pth'\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune --config config/open-llama-7b-lora.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA9XkEUK04lF"
      },
      "source": [
        "We can also further reduce the memory required with quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvwUGDHM04lF",
        "outputId": "96b14032-c998-49e9-a833-c4118acef5ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_7b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x747be4784f80>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'lora_alpha': 16,\r\n",
            " 'lora_dropout': 0.05,\r\n",
            " 'lora_head': True,\r\n",
            " 'lora_key': True,\r\n",
            " 'lora_mlp': True,\r\n",
            " 'lora_projection': True,\r\n",
            " 'lora_query': True,\r\n",
            " 'lora_r': 32,\r\n",
            " 'lora_value': True,\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/lora-open-llama-7b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'quantize': 'bnb.nf4',\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=16,\r\n",
            "                    micro_batch_size=4,\r\n",
            "                    lr_warmup_steps=10,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\r\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/finetune/lora.py:135: UserWarning: LitGPT only supports bitsandbytes v0.42.0. This may result in errors when using quantization.\r\n",
            "  warnings.warn(\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 81,108,992\n",
            "Number of non-trainable parameters: 6,738,415,616\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 5.248, val: n/a | iter time: 691.16 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 5.505, val: n/a | iter time: 492.99 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 5.567, val: n/a | iter time: 299.58 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 5.463, val: n/a | iter time: 669.32 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 5.530, val: n/a | iter time: 548.82 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 5.522, val: n/a | iter time: 262.28 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 5.417, val: n/a | iter time: 472.34 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 5.450, val: n/a | iter time: 725.88 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 5.441, val: n/a | iter time: 550.64 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 5.279, val: n/a | iter time: 479.27 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 5.321, val: n/a | iter time: 281.28 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 5.393, val: n/a | iter time: 726.31 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 5.419, val: n/a | iter time: 692.20 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 5.552, val: n/a | iter time: 665.59 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 5.429, val: n/a | iter time: 590.31 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 5.364, val: n/a | iter time: 728.35 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 5.193, val: n/a | iter time: 548.24 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 5.067, val: n/a | iter time: 414.29 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 5.123, val: n/a | iter time: 559.44 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 5.017, val: n/a | iter time: 516.88 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 5.040, val: n/a | iter time: 294.65 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 4.989, val: n/a | iter time: 594.70 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 4.915, val: n/a | iter time: 704.14 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 4.922, val: n/a | iter time: 664.92 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 4.816, val: n/a | iter time: 496.58 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 4.753, val: n/a | iter time: 699.61 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 4.727, val: n/a | iter time: 649.22 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 4.702, val: n/a | iter time: 314.25 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 4.750, val: n/a | iter time: 695.04 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 4.655, val: n/a | iter time: 357.61 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 4.601, val: n/a | iter time: 411.67 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 4.515, val: n/a | iter time: 420.58 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 4.494, val: n/a | iter time: 700.25 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 4.635, val: n/a | iter time: 413.66 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 4.571, val: n/a | iter time: 298.89 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 4.494, val: n/a | iter time: 715.44 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 4.325, val: n/a | iter time: 354.48 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 4.297, val: n/a | iter time: 696.21 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 4.254, val: n/a | iter time: 269.70 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 4.291, val: n/a | iter time: 622.93 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 4.289, val: n/a | iter time: 675.63 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 4.100, val: n/a | iter time: 555.87 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 3.976, val: n/a | iter time: 481.11 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 3.939, val: n/a | iter time: 506.82 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 3.941, val: n/a | iter time: 289.78 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 3.979, val: n/a | iter time: 484.83 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 4.033, val: n/a | iter time: 646.70 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 3.972, val: n/a | iter time: 597.22 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 3.995, val: n/a | iter time: 533.99 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 4.012, val: n/a | iter time: 706.75 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 4.012, val: n/a | iter time: 399.25 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 3.992, val: n/a | iter time: 616.71 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 3.835, val: n/a | iter time: 294.75 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 3.746, val: n/a | iter time: 684.85 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 3.764, val: n/a | iter time: 556.84 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 3.700, val: n/a | iter time: 731.76 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 3.727, val: n/a | iter time: 417.16 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 3.680, val: n/a | iter time: 674.08 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 3.531, val: n/a | iter time: 633.13 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 3.504, val: n/a | iter time: 565.74 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 3.497, val: n/a | iter time: 592.39 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 3.461, val: n/a | iter time: 703.26 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 3.631, val: n/a | iter time: 689.10 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 3.577, val: n/a | iter time: 379.41 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 3.597, val: n/a | iter time: 659.85 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 3.659, val: n/a | iter time: 457.26 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 3.521, val: n/a | iter time: 556.00 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 3.566, val: n/a | iter time: 659.77 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 3.466, val: n/a | iter time: 544.56 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 3.283, val: n/a | iter time: 495.61 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 3.294, val: n/a | iter time: 590.49 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 3.211, val: n/a | iter time: 613.50 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 3.203, val: n/a | iter time: 319.28 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 3.268, val: n/a | iter time: 552.77 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 3.226, val: n/a | iter time: 640.64 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 3.127, val: n/a | iter time: 330.04 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 3.087, val: n/a | iter time: 562.25 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 3.059, val: n/a | iter time: 500.95 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 3.025, val: n/a | iter time: 603.17 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 3.011, val: n/a | iter time: 456.14 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 2.925, val: n/a | iter time: 324.14 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 2.891, val: n/a | iter time: 496.97 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 2.686, val: n/a | iter time: 285.40 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 2.830, val: n/a | iter time: 736.41 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 2.999, val: n/a | iter time: 521.92 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 2.972, val: n/a | iter time: 700.48 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 3.189, val: n/a | iter time: 495.59 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 3.154, val: n/a | iter time: 508.26 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 3.066, val: n/a | iter time: 601.69 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 3.052, val: n/a | iter time: 551.95 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 2.998, val: n/a | iter time: 596.21 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 2.918, val: n/a | iter time: 477.85 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 2.868, val: n/a | iter time: 655.71 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 2.915, val: n/a | iter time: 357.60 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 3.077, val: n/a | iter time: 704.65 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 3.078, val: n/a | iter time: 612.88 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 3.120, val: n/a | iter time: 699.82 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 3.089, val: n/a | iter time: 561.86 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 2.780, val: n/a | iter time: 494.80 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 2.836, val: n/a | iter time: 547.78 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "\n",
            "1. Write a response that appropriately completes the request.\n",
            "\n",
            "### Response:\n",
            "### Response:\n",
            "### Response: A response\n",
            "### Response:\n",
            "### Response:\n",
            "### Response\n",
            "### Response:\n",
            "### Response\n",
            "1. Write a response that appropriately completes the request. Write a description that appropriately completes the request using appropriate language.\n",
            "### Response:\n",
            "###\n",
            "\n",
            "iter 100: val loss 2.8190, val time: 28101.52 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 2.902, val: 2.819 | iter time: 665.51 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 2.811, val: 2.819 | iter time: 461.79 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 2.848, val: 2.819 | iter time: 461.66 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 2.935, val: 2.819 | iter time: 672.42 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 2.751, val: 2.819 | iter time: 515.35 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 2.834, val: 2.819 | iter time: 667.42 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 2.844, val: 2.819 | iter time: 597.47 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 2.642, val: 2.819 | iter time: 481.85 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 2.460, val: 2.819 | iter time: 269.19 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 2.285, val: 2.819 | iter time: 396.13 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 2.197, val: 2.819 | iter time: 493.74 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 2.098, val: 2.819 | iter time: 389.49 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 2.304, val: 2.819 | iter time: 360.76 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 2.372, val: 2.819 | iter time: 530.12 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 2.454, val: 2.819 | iter time: 558.48 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 2.572, val: 2.819 | iter time: 620.49 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 2.366, val: 2.819 | iter time: 282.10 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 2.381, val: 2.819 | iter time: 526.88 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 2.405, val: 2.819 | iter time: 682.29 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 2.444, val: 2.819 | iter time: 449.98 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 2.618, val: 2.819 | iter time: 496.77 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 2.657, val: 2.819 | iter time: 421.81 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 2.618, val: 2.819 | iter time: 674.89 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 2.640, val: 2.819 | iter time: 562.19 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 2.627, val: 2.819 | iter time: 455.47 ms\n",
            "Epoch 1 | iter 126 step 31 | loss train: 2.469, val: 2.819 | iter time: 596.70 ms\n",
            "Epoch 1 | iter 127 step 31 | loss train: 2.529, val: 2.819 | iter time: 585.65 ms\n",
            "Epoch 1 | iter 128 step 32 | loss train: 2.512, val: 2.819 | iter time: 634.89 ms (step)\n",
            "Epoch 1 | iter 129 step 32 | loss train: 2.515, val: 2.819 | iter time: 516.90 ms\n",
            "Epoch 1 | iter 130 step 32 | loss train: 2.662, val: 2.819 | iter time: 607.36 ms\n",
            "Epoch 1 | iter 131 step 32 | loss train: 2.645, val: 2.819 | iter time: 635.23 ms\n",
            "Epoch 1 | iter 132 step 33 | loss train: 2.563, val: 2.819 | iter time: 576.67 ms (step)\n",
            "Epoch 1 | iter 133 step 33 | loss train: 2.636, val: 2.819 | iter time: 587.01 ms\n",
            "Epoch 1 | iter 134 step 33 | loss train: 2.617, val: 2.819 | iter time: 664.46 ms\n",
            "Epoch 1 | iter 135 step 33 | loss train: 2.608, val: 2.819 | iter time: 663.66 ms\n",
            "Epoch 1 | iter 136 step 34 | loss train: 2.704, val: 2.819 | iter time: 734.55 ms (step)\n",
            "Epoch 1 | iter 137 step 34 | loss train: 2.563, val: 2.819 | iter time: 528.93 ms\n",
            "Epoch 1 | iter 138 step 34 | loss train: 2.528, val: 2.819 | iter time: 522.39 ms\n",
            "Epoch 1 | iter 139 step 34 | loss train: 2.519, val: 2.819 | iter time: 440.87 ms\n",
            "Epoch 1 | iter 140 step 35 | loss train: 2.484, val: 2.819 | iter time: 458.70 ms (step)\n",
            "Epoch 1 | iter 141 step 35 | loss train: 2.593, val: 2.819 | iter time: 496.17 ms\n",
            "Epoch 1 | iter 142 step 35 | loss train: 2.393, val: 2.819 | iter time: 268.66 ms\n",
            "Epoch 1 | iter 143 step 35 | loss train: 2.343, val: 2.819 | iter time: 641.68 ms\n",
            "Epoch 1 | iter 144 step 36 | loss train: 2.207, val: 2.819 | iter time: 504.10 ms (step)\n",
            "Epoch 1 | iter 145 step 36 | loss train: 2.045, val: 2.819 | iter time: 463.73 ms\n",
            "Epoch 1 | iter 146 step 36 | loss train: 2.304, val: 2.819 | iter time: 583.08 ms\n",
            "Epoch 1 | iter 147 step 36 | loss train: 2.384, val: 2.819 | iter time: 706.87 ms\n",
            "Epoch 1 | iter 148 step 37 | loss train: 2.368, val: 2.819 | iter time: 589.83 ms (step)\n",
            "Epoch 1 | iter 149 step 37 | loss train: 2.504, val: 2.819 | iter time: 546.93 ms\n",
            "Epoch 1 | iter 150 step 37 | loss train: 2.532, val: 2.819 | iter time: 704.03 ms\n",
            "Epoch 1 | iter 151 step 37 | loss train: 2.476, val: 2.819 | iter time: 548.54 ms\n",
            "Epoch 1 | iter 152 step 38 | loss train: 2.511, val: 2.819 | iter time: 548.60 ms (step)\n",
            "Epoch 1 | iter 153 step 38 | loss train: 2.512, val: 2.819 | iter time: 544.82 ms\n",
            "Epoch 1 | iter 154 step 38 | loss train: 2.377, val: 2.819 | iter time: 350.01 ms\n",
            "Epoch 1 | iter 155 step 38 | loss train: 2.425, val: 2.819 | iter time: 686.50 ms\n",
            "Epoch 1 | iter 156 step 39 | loss train: 2.521, val: 2.819 | iter time: 696.27 ms (step)\n",
            "Epoch 1 | iter 157 step 39 | loss train: 2.400, val: 2.819 | iter time: 452.39 ms\n",
            "Epoch 1 | iter 158 step 39 | loss train: 2.322, val: 2.819 | iter time: 346.13 ms\n",
            "Epoch 1 | iter 159 step 39 | loss train: 2.253, val: 2.819 | iter time: 641.59 ms\n",
            "Epoch 1 | iter 160 step 40 | loss train: 2.241, val: 2.819 | iter time: 627.23 ms (step)\n",
            "Epoch 1 | iter 161 step 40 | loss train: 2.209, val: 2.819 | iter time: 410.10 ms\n",
            "Epoch 1 | iter 162 step 40 | loss train: 2.190, val: 2.819 | iter time: 350.07 ms\n",
            "Epoch 1 | iter 163 step 40 | loss train: 2.281, val: 2.819 | iter time: 552.10 ms\n",
            "Epoch 1 | iter 164 step 41 | loss train: 2.225, val: 2.819 | iter time: 479.65 ms (step)\n",
            "Epoch 1 | iter 165 step 41 | loss train: 2.327, val: 2.819 | iter time: 587.40 ms\n",
            "Epoch 1 | iter 166 step 41 | loss train: 2.285, val: 2.819 | iter time: 298.93 ms\n",
            "Epoch 1 | iter 167 step 41 | loss train: 2.212, val: 2.819 | iter time: 637.33 ms\n",
            "Epoch 1 | iter 168 step 42 | loss train: 2.273, val: 2.819 | iter time: 515.11 ms (step)\n",
            "Epoch 1 | iter 169 step 42 | loss train: 2.260, val: 2.819 | iter time: 495.92 ms\n",
            "Epoch 1 | iter 170 step 42 | loss train: 2.452, val: 2.819 | iter time: 430.87 ms\n",
            "Epoch 1 | iter 171 step 42 | loss train: 2.326, val: 2.819 | iter time: 313.94 ms\n",
            "Epoch 1 | iter 172 step 43 | loss train: 2.305, val: 2.819 | iter time: 729.35 ms (step)\n",
            "Epoch 1 | iter 173 step 43 | loss train: 2.309, val: 2.819 | iter time: 547.50 ms\n",
            "Epoch 1 | iter 174 step 43 | loss train: 2.325, val: 2.819 | iter time: 399.02 ms\n",
            "Epoch 1 | iter 175 step 43 | loss train: 2.441, val: 2.819 | iter time: 498.55 ms\n",
            "Epoch 1 | iter 176 step 44 | loss train: 2.441, val: 2.819 | iter time: 582.74 ms (step)\n",
            "Epoch 1 | iter 177 step 44 | loss train: 2.546, val: 2.819 | iter time: 482.95 ms\n",
            "Epoch 1 | iter 178 step 44 | loss train: 2.472, val: 2.819 | iter time: 411.46 ms\n",
            "Epoch 1 | iter 179 step 44 | loss train: 2.507, val: 2.819 | iter time: 684.77 ms\n",
            "Epoch 1 | iter 180 step 45 | loss train: 2.471, val: 2.819 | iter time: 593.06 ms (step)\n",
            "Epoch 1 | iter 181 step 45 | loss train: 2.430, val: 2.819 | iter time: 693.30 ms\n",
            "Epoch 1 | iter 182 step 45 | loss train: 2.501, val: 2.819 | iter time: 479.65 ms\n",
            "Epoch 1 | iter 183 step 45 | loss train: 2.335, val: 2.819 | iter time: 495.04 ms\n",
            "Epoch 1 | iter 184 step 46 | loss train: 2.271, val: 2.819 | iter time: 556.25 ms (step)\n",
            "Epoch 1 | iter 185 step 46 | loss train: 2.274, val: 2.819 | iter time: 481.29 ms\n",
            "Epoch 1 | iter 186 step 46 | loss train: 2.266, val: 2.819 | iter time: 431.94 ms\n",
            "Epoch 1 | iter 187 step 46 | loss train: 2.324, val: 2.819 | iter time: 404.39 ms\n",
            "Epoch 1 | iter 188 step 47 | loss train: 2.362, val: 2.819 | iter time: 587.97 ms (step)\n",
            "Epoch 1 | iter 189 step 47 | loss train: 2.358, val: 2.819 | iter time: 554.93 ms\n",
            "Epoch 1 | iter 190 step 47 | loss train: 2.311, val: 2.819 | iter time: 552.08 ms\n",
            "Epoch 1 | iter 191 step 47 | loss train: 2.375, val: 2.819 | iter time: 591.18 ms\n",
            "Epoch 1 | iter 192 step 48 | loss train: 2.455, val: 2.819 | iter time: 730.03 ms (step)\n",
            "Epoch 1 | iter 193 step 48 | loss train: 2.336, val: 2.819 | iter time: 454.16 ms\n",
            "Epoch 1 | iter 194 step 48 | loss train: 2.539, val: 2.819 | iter time: 653.89 ms\n",
            "Epoch 1 | iter 195 step 48 | loss train: 2.574, val: 2.819 | iter time: 685.15 ms\n",
            "Epoch 1 | iter 196 step 49 | loss train: 2.389, val: 2.819 | iter time: 322.40 ms (step)\n",
            "Epoch 1 | iter 197 step 49 | loss train: 2.459, val: 2.819 | iter time: 526.07 ms\n",
            "Epoch 1 | iter 198 step 49 | loss train: 2.352, val: 2.819 | iter time: 605.43 ms\n",
            "Epoch 1 | iter 199 step 49 | loss train: 2.054, val: 2.819 | iter time: 271.06 ms\n",
            "Epoch 1 | iter 200 step 50 | loss train: 2.291, val: 2.819 | iter time: 729.32 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " A funny video clip with funny captions, funny captions, funny captions and puns.\n",
            "\n",
            "iter 200: val loss 2.4150, val time: 24263.14 ms\n",
            "Epoch 1 | iter 201 step 50 | loss train: 2.266, val: 2.415 | iter time: 398.70 ms\n",
            "Epoch 1 | iter 202 step 50 | loss train: 2.060, val: 2.415 | iter time: 333.25 ms\n",
            "Epoch 1 | iter 203 step 50 | loss train: 2.260, val: 2.415 | iter time: 326.41 ms\n",
            "Epoch 1 | iter 204 step 51 | loss train: 2.153, val: 2.415 | iter time: 728.59 ms (step)\n",
            "Epoch 1 | iter 205 step 51 | loss train: 2.198, val: 2.415 | iter time: 577.99 ms\n",
            "Epoch 1 | iter 206 step 51 | loss train: 2.429, val: 2.415 | iter time: 452.33 ms\n",
            "Epoch 1 | iter 207 step 51 | loss train: 2.356, val: 2.415 | iter time: 455.81 ms\n",
            "Epoch 1 | iter 208 step 52 | loss train: 2.513, val: 2.415 | iter time: 618.10 ms (step)\n",
            "Epoch 1 | iter 209 step 52 | loss train: 2.517, val: 2.415 | iter time: 482.89 ms\n",
            "Epoch 1 | iter 210 step 52 | loss train: 2.420, val: 2.415 | iter time: 527.29 ms\n",
            "Epoch 1 | iter 211 step 52 | loss train: 2.432, val: 2.415 | iter time: 391.82 ms\n",
            "Epoch 1 | iter 212 step 53 | loss train: 2.297, val: 2.415 | iter time: 670.75 ms (step)\n",
            "Epoch 1 | iter 213 step 53 | loss train: 2.194, val: 2.415 | iter time: 455.19 ms\n",
            "Epoch 1 | iter 214 step 53 | loss train: 2.192, val: 2.415 | iter time: 441.53 ms\n",
            "Epoch 1 | iter 215 step 53 | loss train: 2.290, val: 2.415 | iter time: 435.89 ms\n",
            "Epoch 1 | iter 216 step 54 | loss train: 2.241, val: 2.415 | iter time: 776.69 ms (step)\n",
            "Epoch 1 | iter 217 step 54 | loss train: 2.361, val: 2.415 | iter time: 658.05 ms\n",
            "Epoch 1 | iter 218 step 54 | loss train: 2.312, val: 2.415 | iter time: 467.10 ms\n",
            "Epoch 1 | iter 219 step 54 | loss train: 2.266, val: 2.415 | iter time: 477.02 ms\n",
            "Epoch 1 | iter 220 step 55 | loss train: 2.231, val: 2.415 | iter time: 360.33 ms (step)\n",
            "Epoch 1 | iter 221 step 55 | loss train: 2.250, val: 2.415 | iter time: 468.91 ms\n",
            "Epoch 1 | iter 222 step 55 | loss train: 2.292, val: 2.415 | iter time: 587.94 ms\n",
            "Epoch 1 | iter 223 step 55 | loss train: 2.340, val: 2.415 | iter time: 683.81 ms\n",
            "Epoch 1 | iter 224 step 56 | loss train: 2.439, val: 2.415 | iter time: 616.69 ms (step)\n",
            "Epoch 1 | iter 225 step 56 | loss train: 2.331, val: 2.415 | iter time: 694.80 ms\n",
            "Epoch 1 | iter 226 step 56 | loss train: 2.266, val: 2.415 | iter time: 352.02 ms\n",
            "Epoch 1 | iter 227 step 56 | loss train: 2.280, val: 2.415 | iter time: 526.78 ms\n",
            "Epoch 1 | iter 228 step 57 | loss train: 2.210, val: 2.415 | iter time: 580.78 ms (step)\n",
            "Epoch 1 | iter 229 step 57 | loss train: 2.228, val: 2.415 | iter time: 343.84 ms\n",
            "Epoch 1 | iter 230 step 57 | loss train: 2.238, val: 2.415 | iter time: 689.81 ms\n",
            "Epoch 1 | iter 231 step 57 | loss train: 2.163, val: 2.415 | iter time: 327.19 ms\n",
            "Epoch 1 | iter 232 step 58 | loss train: 2.245, val: 2.415 | iter time: 702.72 ms (step)\n",
            "Epoch 1 | iter 233 step 58 | loss train: 2.277, val: 2.415 | iter time: 587.30 ms\n",
            "Epoch 1 | iter 234 step 58 | loss train: 2.267, val: 2.415 | iter time: 539.60 ms\n",
            "Epoch 1 | iter 235 step 58 | loss train: 2.198, val: 2.415 | iter time: 392.08 ms\n",
            "Epoch 1 | iter 236 step 59 | loss train: 2.178, val: 2.415 | iter time: 609.84 ms (step)\n",
            "Epoch 1 | iter 237 step 59 | loss train: 2.271, val: 2.415 | iter time: 696.91 ms\n",
            "Epoch 1 | iter 238 step 59 | loss train: 2.176, val: 2.415 | iter time: 333.12 ms\n",
            "Epoch 1 | iter 239 step 59 | loss train: 2.323, val: 2.415 | iter time: 701.29 ms\n",
            "Epoch 1 | iter 240 step 60 | loss train: 2.338, val: 2.415 | iter time: 717.95 ms (step)\n",
            "Epoch 1 | iter 241 step 60 | loss train: 2.105, val: 2.415 | iter time: 285.85 ms\n",
            "Epoch 1 | iter 242 step 60 | loss train: 2.206, val: 2.415 | iter time: 352.87 ms\n",
            "Epoch 1 | iter 243 step 60 | loss train: 2.359, val: 2.415 | iter time: 653.59 ms\n",
            "Epoch 1 | iter 244 step 61 | loss train: 2.312, val: 2.415 | iter time: 579.00 ms (step)\n",
            "Epoch 1 | iter 245 step 61 | loss train: 2.500, val: 2.415 | iter time: 477.41 ms\n",
            "Epoch 1 | iter 246 step 61 | loss train: 2.591, val: 2.415 | iter time: 411.29 ms\n",
            "Epoch 1 | iter 247 step 61 | loss train: 2.514, val: 2.415 | iter time: 635.06 ms\n",
            "Epoch 1 | iter 248 step 62 | loss train: 2.567, val: 2.415 | iter time: 670.46 ms (step)\n",
            "Epoch 1 | iter 249 step 62 | loss train: 2.487, val: 2.415 | iter time: 547.94 ms\n",
            "Epoch 1 | iter 250 step 62 | loss train: 2.419, val: 2.415 | iter time: 547.15 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  151019\n",
            "| - Tokens w/ Prompt          :  195803\n",
            "| - Total Tokens (w/ Padding) :  336060\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  188.68 s\n",
            "| - Tok/sec                   :  1781.11 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  21.25 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n",
            "Saving LoRA weights to 'out/finetune/lora-open-llama-7b/final/lit_model.pth.lora'\n",
            "{'checkpoint_dir': PosixPath('out/finetune/lora-open-llama-7b/final'),\n",
            " 'precision': None,\n",
            " 'pretrained_checkpoint_dir': None}\n",
            "LoRA weights have already been merged in this checkpoint.\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune --config config/open-llama-7b-lora.yaml --quantize bnb.nf4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NAdxDe04lF"
      },
      "source": [
        "Even the 13B model can be trained quickly with minimal memory required, using LoRA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0I0Lhwk04lF",
        "outputId": "14dc33b0-93ce-4361-cda0-ed2914b47f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'access_token': None,\r\n",
            " 'checkpoint_dir': PosixPath('checkpoints/openlm-research/open_llama_13b'),\r\n",
            " 'data': Alpaca2k(mask_prompt=False,\r\n",
            "                  val_split_fraction=0.5,\r\n",
            "                  prompt_style=<litgpt.prompts.Alpaca object at 0x7c801011e8d0>,\r\n",
            "                  ignore_index=-100,\r\n",
            "                  seed=42,\r\n",
            "                  num_workers=4,\r\n",
            "                  download_dir=PosixPath('data/alpaca2k')),\r\n",
            " 'devices': 1,\r\n",
            " 'eval': EvalArgs(interval=25,\r\n",
            "                  max_new_tokens=100,\r\n",
            "                  max_iters=100,\r\n",
            "                  initial_validation=False,\r\n",
            "                  final_validation=False,\r\n",
            "                  evaluate_example='first'),\r\n",
            " 'logger_name': 'csv',\r\n",
            " 'lora_alpha': 16,\r\n",
            " 'lora_dropout': 0.05,\r\n",
            " 'lora_head': True,\r\n",
            " 'lora_key': True,\r\n",
            " 'lora_mlp': True,\r\n",
            " 'lora_projection': True,\r\n",
            " 'lora_query': True,\r\n",
            " 'lora_r': 32,\r\n",
            " 'lora_value': True,\r\n",
            " 'num_nodes': 1,\r\n",
            " 'optimizer': {'class_path': 'torch.optim.AdamW',\r\n",
            "               'init_args': {'betas': [0.9, 0.95],\r\n",
            "                             'lr': 0.0002,\r\n",
            "                             'weight_decay': 0.0}},\r\n",
            " 'out_dir': PosixPath('out/finetune/lora-open-llama-13b'),\r\n",
            " 'precision': 'bf16-true',\r\n",
            " 'quantize': None,\r\n",
            " 'seed': 1337,\r\n",
            " 'train': TrainArgs(save_interval=100,\r\n",
            "                    log_interval=1,\r\n",
            "                    global_batch_size=16,\r\n",
            "                    micro_batch_size=4,\r\n",
            "                    lr_warmup_steps=10,\r\n",
            "                    lr_warmup_fraction=None,\r\n",
            "                    epochs=1,\r\n",
            "                    max_tokens=None,\r\n",
            "                    max_steps=None,\r\n",
            "                    max_seq_length=512,\r\n",
            "                    tie_embeddings=None,\r\n",
            "                    max_norm=None,\r\n",
            "                    min_lr=6e-05)}\n",
            "Seed set to 1337\n",
            "Number of trainable parameters: 126,361,600\n",
            "Number of non-trainable parameters: 13,015,864,320\n",
            "The longest sequence length in the train data is 512, the model's maximum sequence length is 512 and context length is 2048\n",
            "Verifying settings ...\n",
            "Epoch 1 | iter 1 step 0 | loss train: 1.703, val: n/a | iter time: 782.44 ms\n",
            "Epoch 1 | iter 2 step 0 | loss train: 1.553, val: n/a | iter time: 605.92 ms\n",
            "Epoch 1 | iter 3 step 0 | loss train: 1.747, val: n/a | iter time: 274.65 ms\n",
            "Epoch 1 | iter 4 step 1 | loss train: 1.778, val: n/a | iter time: 518.97 ms (step)\n",
            "Epoch 1 | iter 5 step 1 | loss train: 1.811, val: n/a | iter time: 715.37 ms\n",
            "Epoch 1 | iter 6 step 1 | loss train: 2.085, val: n/a | iter time: 217.40 ms\n",
            "Epoch 1 | iter 7 step 1 | loss train: 2.002, val: n/a | iter time: 577.30 ms\n",
            "Epoch 1 | iter 8 step 2 | loss train: 1.868, val: n/a | iter time: 938.77 ms (step)\n",
            "Epoch 1 | iter 9 step 2 | loss train: 1.718, val: n/a | iter time: 724.17 ms\n",
            "Epoch 1 | iter 10 step 2 | loss train: 1.507, val: n/a | iter time: 590.90 ms\n",
            "Epoch 1 | iter 11 step 2 | loss train: 1.686, val: n/a | iter time: 246.58 ms\n",
            "Epoch 1 | iter 12 step 3 | loss train: 1.770, val: n/a | iter time: 938.97 ms (step)\n",
            "Epoch 1 | iter 13 step 3 | loss train: 1.831, val: n/a | iter time: 922.11 ms\n",
            "Epoch 1 | iter 14 step 3 | loss train: 1.823, val: n/a | iter time: 893.72 ms\n",
            "Epoch 1 | iter 15 step 3 | loss train: 1.646, val: n/a | iter time: 785.38 ms\n",
            "Epoch 1 | iter 16 step 4 | loss train: 1.586, val: n/a | iter time: 939.15 ms (step)\n",
            "Epoch 1 | iter 17 step 4 | loss train: 1.672, val: n/a | iter time: 715.40 ms\n",
            "Epoch 1 | iter 18 step 4 | loss train: 1.672, val: n/a | iter time: 462.65 ms\n",
            "Epoch 1 | iter 19 step 4 | loss train: 1.613, val: n/a | iter time: 735.61 ms\n",
            "Epoch 1 | iter 20 step 5 | loss train: 1.717, val: n/a | iter time: 608.97 ms (step)\n",
            "Epoch 1 | iter 21 step 5 | loss train: 1.777, val: n/a | iter time: 270.14 ms\n",
            "Epoch 1 | iter 22 step 5 | loss train: 1.728, val: n/a | iter time: 794.36 ms\n",
            "Epoch 1 | iter 23 step 5 | loss train: 1.669, val: n/a | iter time: 939.47 ms\n",
            "Epoch 1 | iter 24 step 6 | loss train: 1.604, val: n/a | iter time: 839.22 ms (step)\n",
            "Epoch 1 | iter 25 step 6 | loss train: 1.461, val: n/a | iter time: 616.51 ms\n",
            "Epoch 1 | iter 26 step 6 | loss train: 1.440, val: n/a | iter time: 935.09 ms\n",
            "Epoch 1 | iter 27 step 6 | loss train: 1.505, val: n/a | iter time: 849.92 ms\n",
            "Epoch 1 | iter 28 step 7 | loss train: 1.653, val: n/a | iter time: 258.47 ms (step)\n",
            "Epoch 1 | iter 29 step 7 | loss train: 1.560, val: n/a | iter time: 928.19 ms\n",
            "Epoch 1 | iter 30 step 7 | loss train: 1.690, val: n/a | iter time: 394.10 ms\n",
            "Epoch 1 | iter 31 step 7 | loss train: 1.683, val: n/a | iter time: 456.13 ms\n",
            "Epoch 1 | iter 32 step 8 | loss train: 1.573, val: n/a | iter time: 440.23 ms (step)\n",
            "Epoch 1 | iter 33 step 8 | loss train: 1.638, val: n/a | iter time: 939.49 ms\n",
            "Epoch 1 | iter 34 step 8 | loss train: 1.547, val: n/a | iter time: 459.80 ms\n",
            "Epoch 1 | iter 35 step 8 | loss train: 1.629, val: n/a | iter time: 277.44 ms\n",
            "Epoch 1 | iter 36 step 9 | loss train: 1.421, val: n/a | iter time: 918.19 ms (step)\n",
            "Epoch 1 | iter 37 step 9 | loss train: 1.412, val: n/a | iter time: 392.66 ms\n",
            "Epoch 1 | iter 38 step 9 | loss train: 1.349, val: n/a | iter time: 923.82 ms\n",
            "Epoch 1 | iter 39 step 9 | loss train: 1.301, val: n/a | iter time: 230.90 ms\n",
            "Epoch 1 | iter 40 step 10 | loss train: 1.360, val: n/a | iter time: 795.13 ms (step)\n",
            "Epoch 1 | iter 41 step 10 | loss train: 1.293, val: n/a | iter time: 916.26 ms\n",
            "Epoch 1 | iter 42 step 10 | loss train: 1.257, val: n/a | iter time: 723.27 ms\n",
            "Epoch 1 | iter 43 step 10 | loss train: 1.090, val: n/a | iter time: 592.96 ms\n",
            "Epoch 1 | iter 44 step 11 | loss train: 1.149, val: n/a | iter time: 592.85 ms (step)\n",
            "Epoch 1 | iter 45 step 11 | loss train: 1.214, val: n/a | iter time: 259.30 ms\n",
            "Epoch 1 | iter 46 step 11 | loss train: 1.256, val: n/a | iter time: 597.90 ms\n",
            "Epoch 1 | iter 47 step 11 | loss train: 1.268, val: n/a | iter time: 848.19 ms\n",
            "Epoch 1 | iter 48 step 12 | loss train: 1.200, val: n/a | iter time: 752.52 ms (step)\n",
            "Epoch 1 | iter 49 step 12 | loss train: 1.112, val: n/a | iter time: 701.91 ms\n",
            "Epoch 1 | iter 50 step 12 | loss train: 1.029, val: n/a | iter time: 937.73 ms\n",
            "Epoch 1 | iter 51 step 12 | loss train: 1.057, val: n/a | iter time: 436.62 ms\n",
            "Epoch 1 | iter 52 step 13 | loss train: 1.085, val: n/a | iter time: 783.29 ms (step)\n",
            "Epoch 1 | iter 53 step 13 | loss train: 1.059, val: n/a | iter time: 268.56 ms\n",
            "Epoch 1 | iter 54 step 13 | loss train: 1.091, val: n/a | iter time: 905.76 ms\n",
            "Epoch 1 | iter 55 step 13 | loss train: 1.023, val: n/a | iter time: 723.46 ms\n",
            "Epoch 1 | iter 56 step 14 | loss train: 0.969, val: n/a | iter time: 945.29 ms (step)\n",
            "Epoch 1 | iter 57 step 14 | loss train: 1.003, val: n/a | iter time: 467.31 ms\n",
            "Epoch 1 | iter 58 step 14 | loss train: 1.014, val: n/a | iter time: 908.02 ms\n",
            "Epoch 1 | iter 59 step 14 | loss train: 0.996, val: n/a | iter time: 823.44 ms\n",
            "Epoch 1 | iter 60 step 15 | loss train: 1.001, val: n/a | iter time: 706.07 ms (step)\n",
            "Epoch 1 | iter 61 step 15 | loss train: 1.007, val: n/a | iter time: 791.14 ms\n",
            "Epoch 1 | iter 62 step 15 | loss train: 0.975, val: n/a | iter time: 935.32 ms\n",
            "Epoch 1 | iter 63 step 15 | loss train: 1.035, val: n/a | iter time: 910.58 ms\n",
            "Epoch 1 | iter 64 step 16 | loss train: 0.999, val: n/a | iter time: 389.70 ms (step)\n",
            "Epoch 1 | iter 65 step 16 | loss train: 0.997, val: n/a | iter time: 884.39 ms\n",
            "Epoch 1 | iter 66 step 16 | loss train: 0.946, val: n/a | iter time: 550.33 ms\n",
            "Epoch 1 | iter 67 step 16 | loss train: 0.956, val: n/a | iter time: 727.35 ms\n",
            "Epoch 1 | iter 68 step 17 | loss train: 1.000, val: n/a | iter time: 831.84 ms (step)\n",
            "Epoch 1 | iter 69 step 17 | loss train: 0.979, val: n/a | iter time: 706.67 ms\n",
            "Epoch 1 | iter 70 step 17 | loss train: 1.045, val: n/a | iter time: 614.61 ms\n",
            "Epoch 1 | iter 71 step 17 | loss train: 0.986, val: n/a | iter time: 783.13 ms\n",
            "Epoch 1 | iter 72 step 18 | loss train: 0.937, val: n/a | iter time: 789.04 ms (step)\n",
            "Epoch 1 | iter 73 step 18 | loss train: 0.947, val: n/a | iter time: 315.87 ms\n",
            "Epoch 1 | iter 74 step 18 | loss train: 0.942, val: n/a | iter time: 722.91 ms\n",
            "Epoch 1 | iter 75 step 18 | loss train: 0.983, val: n/a | iter time: 835.37 ms\n",
            "Epoch 1 | iter 76 step 19 | loss train: 0.952, val: n/a | iter time: 280.42 ms (step)\n",
            "Epoch 1 | iter 77 step 19 | loss train: 0.932, val: n/a | iter time: 733.41 ms\n",
            "Epoch 1 | iter 78 step 19 | loss train: 0.908, val: n/a | iter time: 620.93 ms\n",
            "Epoch 1 | iter 79 step 19 | loss train: 0.833, val: n/a | iter time: 802.49 ms\n",
            "Epoch 1 | iter 80 step 20 | loss train: 0.895, val: n/a | iter time: 489.81 ms (step)\n",
            "Epoch 1 | iter 81 step 20 | loss train: 0.841, val: n/a | iter time: 321.13 ms\n",
            "Epoch 1 | iter 82 step 20 | loss train: 0.910, val: n/a | iter time: 616.15 ms\n",
            "Epoch 1 | iter 83 step 20 | loss train: 0.870, val: n/a | iter time: 254.14 ms\n",
            "Epoch 1 | iter 84 step 21 | loss train: 0.825, val: n/a | iter time: 946.91 ms (step)\n",
            "Epoch 1 | iter 85 step 21 | loss train: 0.988, val: n/a | iter time: 672.94 ms\n",
            "Epoch 1 | iter 86 step 21 | loss train: 0.864, val: n/a | iter time: 935.27 ms\n",
            "Epoch 1 | iter 87 step 21 | loss train: 0.947, val: n/a | iter time: 605.87 ms\n",
            "Epoch 1 | iter 88 step 22 | loss train: 0.980, val: n/a | iter time: 593.67 ms (step)\n",
            "Epoch 1 | iter 89 step 22 | loss train: 0.920, val: n/a | iter time: 804.77 ms\n",
            "Epoch 1 | iter 90 step 22 | loss train: 0.967, val: n/a | iter time: 719.59 ms\n",
            "Epoch 1 | iter 91 step 22 | loss train: 1.010, val: n/a | iter time: 793.30 ms\n",
            "Epoch 1 | iter 92 step 23 | loss train: 1.015, val: n/a | iter time: 545.58 ms (step)\n",
            "Epoch 1 | iter 93 step 23 | loss train: 1.005, val: n/a | iter time: 878.49 ms\n",
            "Epoch 1 | iter 94 step 23 | loss train: 0.996, val: n/a | iter time: 394.83 ms\n",
            "Epoch 1 | iter 95 step 23 | loss train: 1.121, val: n/a | iter time: 939.69 ms\n",
            "Epoch 1 | iter 96 step 24 | loss train: 1.097, val: n/a | iter time: 779.59 ms (step)\n",
            "Epoch 1 | iter 97 step 24 | loss train: 1.096, val: n/a | iter time: 929.16 ms\n",
            "Epoch 1 | iter 98 step 24 | loss train: 1.112, val: n/a | iter time: 734.96 ms\n",
            "Epoch 1 | iter 99 step 24 | loss train: 0.956, val: n/a | iter time: 611.46 ms\n",
            "Epoch 1 | iter 100 step 25 | loss train: 0.979, val: n/a | iter time: 678.81 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " Oh no, look out below! The clown decided to tickle that cute, innocent looking rabbit. He is sure to get a big laugh from the audience watching him. But that rabbit is not amused, and he is giving the clown a furious look and warning him, \"Not so funny now, eh?!\"\n",
            "\n",
            "iter 100: val loss 0.9491, val time: 32555.93 ms\n",
            "Epoch 1 | iter 101 step 25 | loss train: 0.947, val: 0.949 | iter time: 895.69 ms\n",
            "Epoch 1 | iter 102 step 25 | loss train: 0.911, val: 0.949 | iter time: 558.39 ms\n",
            "Epoch 1 | iter 103 step 25 | loss train: 0.926, val: 0.949 | iter time: 558.40 ms\n",
            "Epoch 1 | iter 104 step 26 | loss train: 0.967, val: 0.949 | iter time: 855.25 ms (step)\n",
            "Epoch 1 | iter 105 step 26 | loss train: 0.996, val: 0.949 | iter time: 667.41 ms\n",
            "Epoch 1 | iter 106 step 26 | loss train: 1.054, val: 0.949 | iter time: 896.30 ms\n",
            "Epoch 1 | iter 107 step 26 | loss train: 1.026, val: 0.949 | iter time: 798.15 ms\n",
            "Epoch 1 | iter 108 step 27 | loss train: 0.949, val: 0.949 | iter time: 554.12 ms (step)\n",
            "Epoch 1 | iter 109 step 27 | loss train: 0.871, val: 0.949 | iter time: 232.45 ms\n",
            "Epoch 1 | iter 110 step 27 | loss train: 0.836, val: 0.949 | iter time: 435.11 ms\n",
            "Epoch 1 | iter 111 step 27 | loss train: 0.857, val: 0.949 | iter time: 612.31 ms\n",
            "Epoch 1 | iter 112 step 28 | loss train: 0.832, val: 0.949 | iter time: 406.37 ms (step)\n",
            "Epoch 1 | iter 113 step 28 | loss train: 0.933, val: 0.949 | iter time: 401.78 ms\n",
            "Epoch 1 | iter 114 step 28 | loss train: 0.934, val: 0.949 | iter time: 688.88 ms\n",
            "Epoch 1 | iter 115 step 28 | loss train: 0.913, val: 0.949 | iter time: 725.67 ms\n",
            "Epoch 1 | iter 116 step 29 | loss train: 0.967, val: 0.949 | iter time: 797.87 ms (step)\n",
            "Epoch 1 | iter 117 step 29 | loss train: 0.853, val: 0.949 | iter time: 250.51 ms\n",
            "Epoch 1 | iter 118 step 29 | loss train: 0.844, val: 0.949 | iter time: 685.76 ms\n",
            "Epoch 1 | iter 119 step 29 | loss train: 0.899, val: 0.949 | iter time: 910.25 ms\n",
            "Epoch 1 | iter 120 step 30 | loss train: 0.914, val: 0.949 | iter time: 478.87 ms (step)\n",
            "Epoch 1 | iter 121 step 30 | loss train: 0.994, val: 0.949 | iter time: 614.21 ms\n",
            "Epoch 1 | iter 122 step 30 | loss train: 1.051, val: 0.949 | iter time: 469.56 ms\n",
            "Epoch 1 | iter 123 step 30 | loss train: 0.981, val: 0.949 | iter time: 908.65 ms\n",
            "Epoch 1 | iter 124 step 31 | loss train: 1.013, val: 0.949 | iter time: 698.11 ms (step)\n",
            "Epoch 1 | iter 125 step 31 | loss train: 0.998, val: 0.949 | iter time: 550.24 ms\n",
            "Epoch 1 | iter 126 step 31 | loss train: 0.940, val: 0.949 | iter time: 796.88 ms\n",
            "Epoch 1 | iter 127 step 31 | loss train: 0.994, val: 0.949 | iter time: 772.85 ms\n",
            "Epoch 1 | iter 128 step 32 | loss train: 0.982, val: 0.949 | iter time: 817.32 ms (step)\n",
            "Epoch 1 | iter 129 step 32 | loss train: 1.016, val: 0.949 | iter time: 669.01 ms\n",
            "Epoch 1 | iter 130 step 32 | loss train: 1.096, val: 0.949 | iter time: 814.32 ms\n",
            "Epoch 1 | iter 131 step 32 | loss train: 1.068, val: 0.949 | iter time: 823.24 ms\n",
            "Epoch 1 | iter 132 step 33 | loss train: 1.035, val: 0.949 | iter time: 720.90 ms (step)\n",
            "Epoch 1 | iter 133 step 33 | loss train: 1.010, val: 0.949 | iter time: 778.40 ms\n",
            "Epoch 1 | iter 134 step 33 | loss train: 0.933, val: 0.949 | iter time: 889.60 ms\n",
            "Epoch 1 | iter 135 step 33 | loss train: 0.930, val: 0.949 | iter time: 886.03 ms\n",
            "Epoch 1 | iter 136 step 34 | loss train: 0.958, val: 0.949 | iter time: 951.93 ms (step)\n",
            "Epoch 1 | iter 137 step 34 | loss train: 0.907, val: 0.949 | iter time: 687.13 ms\n",
            "Epoch 1 | iter 138 step 34 | loss train: 0.919, val: 0.949 | iter time: 675.92 ms\n",
            "Epoch 1 | iter 139 step 34 | loss train: 0.915, val: 0.949 | iter time: 501.48 ms\n",
            "Epoch 1 | iter 140 step 35 | loss train: 0.901, val: 0.949 | iter time: 493.77 ms (step)\n",
            "Epoch 1 | iter 141 step 35 | loss train: 0.935, val: 0.949 | iter time: 619.50 ms\n",
            "Epoch 1 | iter 142 step 35 | loss train: 0.891, val: 0.949 | iter time: 231.83 ms\n",
            "Epoch 1 | iter 143 step 35 | loss train: 0.898, val: 0.949 | iter time: 838.07 ms\n",
            "Epoch 1 | iter 144 step 36 | loss train: 0.846, val: 0.949 | iter time: 591.28 ms (step)\n",
            "Epoch 1 | iter 145 step 36 | loss train: 0.835, val: 0.949 | iter time: 565.93 ms\n",
            "Epoch 1 | iter 146 step 36 | loss train: 0.924, val: 0.949 | iter time: 770.81 ms\n",
            "Epoch 1 | iter 147 step 36 | loss train: 0.937, val: 0.949 | iter time: 941.26 ms\n",
            "Epoch 1 | iter 148 step 37 | loss train: 0.936, val: 0.949 | iter time: 734.65 ms (step)\n",
            "Epoch 1 | iter 149 step 37 | loss train: 0.954, val: 0.949 | iter time: 711.33 ms\n",
            "Epoch 1 | iter 150 step 37 | loss train: 0.952, val: 0.949 | iter time: 939.51 ms\n",
            "Epoch 1 | iter 151 step 37 | loss train: 0.912, val: 0.949 | iter time: 710.96 ms\n",
            "Epoch 1 | iter 152 step 38 | loss train: 0.891, val: 0.949 | iter time: 677.95 ms (step)\n",
            "Epoch 1 | iter 153 step 38 | loss train: 0.860, val: 0.949 | iter time: 708.41 ms\n",
            "Epoch 1 | iter 154 step 38 | loss train: 0.831, val: 0.949 | iter time: 381.50 ms\n",
            "Epoch 1 | iter 155 step 38 | loss train: 0.842, val: 0.949 | iter time: 909.21 ms\n",
            "Epoch 1 | iter 156 step 39 | loss train: 0.922, val: 0.949 | iter time: 899.63 ms (step)\n",
            "Epoch 1 | iter 157 step 39 | loss train: 0.948, val: 0.949 | iter time: 543.73 ms\n",
            "Epoch 1 | iter 158 step 39 | loss train: 0.885, val: 0.949 | iter time: 372.48 ms\n",
            "Epoch 1 | iter 159 step 39 | loss train: 0.878, val: 0.949 | iter time: 840.84 ms\n",
            "Epoch 1 | iter 160 step 40 | loss train: 0.838, val: 0.949 | iter time: 802.82 ms (step)\n",
            "Epoch 1 | iter 161 step 40 | loss train: 0.797, val: 0.949 | iter time: 452.93 ms\n",
            "Epoch 1 | iter 162 step 40 | loss train: 0.787, val: 0.949 | iter time: 382.72 ms\n",
            "Epoch 1 | iter 163 step 40 | loss train: 0.849, val: 0.949 | iter time: 719.00 ms\n",
            "Epoch 1 | iter 164 step 41 | loss train: 0.911, val: 0.949 | iter time: 545.93 ms (step)\n",
            "Epoch 1 | iter 165 step 41 | loss train: 0.928, val: 0.949 | iter time: 781.40 ms\n",
            "Epoch 1 | iter 166 step 41 | loss train: 0.913, val: 0.949 | iter time: 274.98 ms\n",
            "Epoch 1 | iter 167 step 41 | loss train: 0.885, val: 0.949 | iter time: 830.03 ms\n",
            "Epoch 1 | iter 168 step 42 | loss train: 0.870, val: 0.949 | iter time: 605.45 ms (step)\n",
            "Epoch 1 | iter 169 step 42 | loss train: 0.878, val: 0.949 | iter time: 614.51 ms\n",
            "Epoch 1 | iter 170 step 42 | loss train: 0.961, val: 0.949 | iter time: 484.18 ms\n",
            "Epoch 1 | iter 171 step 42 | loss train: 0.898, val: 0.949 | iter time: 306.28 ms\n",
            "Epoch 1 | iter 172 step 43 | loss train: 0.897, val: 0.949 | iter time: 937.18 ms (step)\n",
            "Epoch 1 | iter 173 step 43 | loss train: 0.891, val: 0.949 | iter time: 711.81 ms\n",
            "Epoch 1 | iter 174 step 43 | loss train: 0.979, val: 0.949 | iter time: 437.67 ms\n",
            "Epoch 1 | iter 175 step 43 | loss train: 1.065, val: 0.949 | iter time: 618.92 ms\n",
            "Epoch 1 | iter 176 step 44 | loss train: 1.052, val: 0.949 | iter time: 729.09 ms (step)\n",
            "Epoch 1 | iter 177 step 44 | loss train: 1.093, val: 0.949 | iter time: 596.57 ms\n",
            "Epoch 1 | iter 178 step 44 | loss train: 0.976, val: 0.949 | iter time: 458.97 ms\n",
            "Epoch 1 | iter 179 step 44 | loss train: 0.956, val: 0.949 | iter time: 905.43 ms\n",
            "Epoch 1 | iter 180 step 45 | loss train: 0.963, val: 0.949 | iter time: 744.44 ms (step)\n",
            "Epoch 1 | iter 181 step 45 | loss train: 0.909, val: 0.949 | iter time: 922.36 ms\n",
            "Epoch 1 | iter 182 step 45 | loss train: 0.950, val: 0.949 | iter time: 590.50 ms\n",
            "Epoch 1 | iter 183 step 45 | loss train: 0.837, val: 0.949 | iter time: 610.90 ms\n",
            "Epoch 1 | iter 184 step 46 | loss train: 0.794, val: 0.949 | iter time: 688.96 ms (step)\n",
            "Epoch 1 | iter 185 step 46 | loss train: 0.837, val: 0.949 | iter time: 592.71 ms\n",
            "Epoch 1 | iter 186 step 46 | loss train: 0.873, val: 0.949 | iter time: 489.24 ms\n",
            "Epoch 1 | iter 187 step 46 | loss train: 0.944, val: 0.949 | iter time: 447.32 ms\n",
            "Epoch 1 | iter 188 step 47 | loss train: 0.937, val: 0.949 | iter time: 740.52 ms (step)\n",
            "Epoch 1 | iter 189 step 47 | loss train: 0.966, val: 0.949 | iter time: 727.01 ms\n",
            "Epoch 1 | iter 190 step 47 | loss train: 0.911, val: 0.949 | iter time: 720.32 ms\n",
            "Epoch 1 | iter 191 step 47 | loss train: 0.921, val: 0.949 | iter time: 783.34 ms\n",
            "Epoch 1 | iter 192 step 48 | loss train: 0.964, val: 0.949 | iter time: 943.83 ms (step)\n",
            "Epoch 1 | iter 193 step 48 | loss train: 0.896, val: 0.949 | iter time: 548.27 ms\n",
            "Epoch 1 | iter 194 step 48 | loss train: 0.979, val: 0.949 | iter time: 861.25 ms\n",
            "Epoch 1 | iter 195 step 48 | loss train: 0.957, val: 0.949 | iter time: 926.99 ms\n",
            "Epoch 1 | iter 196 step 49 | loss train: 0.929, val: 0.949 | iter time: 268.17 ms (step)\n",
            "Epoch 1 | iter 197 step 49 | loss train: 0.986, val: 0.949 | iter time: 678.05 ms\n",
            "Epoch 1 | iter 198 step 49 | loss train: 0.928, val: 0.949 | iter time: 811.34 ms\n",
            "Epoch 1 | iter 199 step 49 | loss train: 0.903, val: 0.949 | iter time: 233.74 ms\n",
            "Epoch 1 | iter 200 step 50 | loss train: 0.857, val: 0.949 | iter time: 939.38 ms (step)\n",
            "Validating ...\n",
            "Generate a funny caption for the following photo.\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Generate a funny caption for the following photo.\n",
            "\n",
            "### Response:\n",
            " Ummm...let me see...oh yeah! This poor guy just got a load of roses in his face. Ouch! He must have been in the wrong place at the wrong time...haha!\n",
            "\n",
            "iter 200: val loss 0.9275, val time: 31234.59 ms\n",
            "Epoch 1 | iter 201 step 50 | loss train: 0.880, val: 0.928 | iter time: 436.71 ms\n",
            "Epoch 1 | iter 202 step 50 | loss train: 0.855, val: 0.928 | iter time: 336.42 ms\n",
            "Epoch 1 | iter 203 step 50 | loss train: 0.922, val: 0.928 | iter time: 324.19 ms\n",
            "Epoch 1 | iter 204 step 51 | loss train: 0.935, val: 0.928 | iter time: 939.06 ms (step)\n",
            "Epoch 1 | iter 205 step 51 | loss train: 0.940, val: 0.928 | iter time: 767.58 ms\n",
            "Epoch 1 | iter 206 step 51 | loss train: 1.024, val: 0.928 | iter time: 543.36 ms\n",
            "Epoch 1 | iter 207 step 51 | loss train: 0.979, val: 0.928 | iter time: 551.80 ms\n",
            "Epoch 1 | iter 208 step 52 | loss train: 1.124, val: 0.928 | iter time: 787.61 ms (step)\n",
            "Epoch 1 | iter 209 step 52 | loss train: 1.114, val: 0.928 | iter time: 597.27 ms\n",
            "Epoch 1 | iter 210 step 52 | loss train: 1.050, val: 0.928 | iter time: 683.37 ms\n",
            "Epoch 1 | iter 211 step 52 | loss train: 1.092, val: 0.928 | iter time: 433.31 ms\n",
            "Epoch 1 | iter 212 step 53 | loss train: 0.963, val: 0.928 | iter time: 849.58 ms (step)\n",
            "Epoch 1 | iter 213 step 53 | loss train: 0.909, val: 0.928 | iter time: 548.58 ms\n",
            "Epoch 1 | iter 214 step 53 | loss train: 0.868, val: 0.928 | iter time: 503.78 ms\n",
            "Epoch 1 | iter 215 step 53 | loss train: 0.865, val: 0.928 | iter time: 494.75 ms\n",
            "Epoch 1 | iter 216 step 54 | loss train: 0.886, val: 0.928 | iter time: 721.49 ms (step)\n",
            "Epoch 1 | iter 217 step 54 | loss train: 0.938, val: 0.928 | iter time: 885.87 ms\n",
            "Epoch 1 | iter 218 step 54 | loss train: 0.980, val: 0.928 | iter time: 566.26 ms\n",
            "Epoch 1 | iter 219 step 54 | loss train: 0.932, val: 0.928 | iter time: 584.95 ms\n",
            "Epoch 1 | iter 220 step 55 | loss train: 0.961, val: 0.928 | iter time: 341.46 ms (step)\n",
            "Epoch 1 | iter 221 step 55 | loss train: 0.945, val: 0.928 | iter time: 574.55 ms\n",
            "Epoch 1 | iter 222 step 55 | loss train: 0.932, val: 0.928 | iter time: 781.27 ms\n",
            "Epoch 1 | iter 223 step 55 | loss train: 0.988, val: 0.928 | iter time: 906.77 ms\n",
            "Epoch 1 | iter 224 step 56 | loss train: 0.981, val: 0.928 | iter time: 780.59 ms (step)\n",
            "Epoch 1 | iter 225 step 56 | loss train: 0.899, val: 0.928 | iter time: 925.31 ms\n",
            "Epoch 1 | iter 226 step 56 | loss train: 0.878, val: 0.928 | iter time: 385.19 ms\n",
            "Epoch 1 | iter 227 step 56 | loss train: 0.862, val: 0.928 | iter time: 680.79 ms\n",
            "Epoch 1 | iter 228 step 57 | loss train: 0.858, val: 0.928 | iter time: 724.99 ms (step)\n",
            "Epoch 1 | iter 229 step 57 | loss train: 0.943, val: 0.928 | iter time: 375.79 ms\n",
            "Epoch 1 | iter 230 step 57 | loss train: 0.901, val: 0.928 | iter time: 914.80 ms\n",
            "Epoch 1 | iter 231 step 57 | loss train: 0.833, val: 0.928 | iter time: 324.18 ms\n",
            "Epoch 1 | iter 232 step 58 | loss train: 0.849, val: 0.928 | iter time: 919.00 ms (step)\n",
            "Epoch 1 | iter 233 step 58 | loss train: 0.807, val: 0.928 | iter time: 782.30 ms\n",
            "Epoch 1 | iter 234 step 58 | loss train: 0.855, val: 0.928 | iter time: 703.28 ms\n",
            "Epoch 1 | iter 235 step 58 | loss train: 0.837, val: 0.928 | iter time: 431.32 ms\n",
            "Epoch 1 | iter 236 step 59 | loss train: 0.838, val: 0.928 | iter time: 778.90 ms (step)\n",
            "Epoch 1 | iter 237 step 59 | loss train: 0.835, val: 0.928 | iter time: 931.28 ms\n",
            "Epoch 1 | iter 238 step 59 | loss train: 0.786, val: 0.928 | iter time: 334.19 ms\n",
            "Epoch 1 | iter 239 step 59 | loss train: 0.853, val: 0.928 | iter time: 936.10 ms\n",
            "Epoch 1 | iter 240 step 60 | loss train: 0.783, val: 0.928 | iter time: 921.78 ms (step)\n",
            "Epoch 1 | iter 241 step 60 | loss train: 0.778, val: 0.928 | iter time: 255.78 ms\n",
            "Epoch 1 | iter 242 step 60 | loss train: 0.848, val: 0.928 | iter time: 388.11 ms\n",
            "Epoch 1 | iter 243 step 60 | loss train: 0.949, val: 0.928 | iter time: 857.74 ms\n",
            "Epoch 1 | iter 244 step 61 | loss train: 0.976, val: 0.928 | iter time: 725.42 ms (step)\n",
            "Epoch 1 | iter 245 step 61 | loss train: 1.030, val: 0.928 | iter time: 588.05 ms\n",
            "Epoch 1 | iter 246 step 61 | loss train: 1.044, val: 0.928 | iter time: 456.73 ms\n",
            "Epoch 1 | iter 247 step 61 | loss train: 0.901, val: 0.928 | iter time: 829.87 ms\n",
            "Epoch 1 | iter 248 step 62 | loss train: 0.921, val: 0.928 | iter time: 844.98 ms (step)\n",
            "Epoch 1 | iter 249 step 62 | loss train: 0.855, val: 0.928 | iter time: 717.55 ms\n",
            "Epoch 1 | iter 250 step 62 | loss train: 0.801, val: 0.928 | iter time: 713.04 ms\n",
            "\n",
            "| ------------------------------------------------------\n",
            "| Token Counts\n",
            "| - Input Tokens              :  151019\n",
            "| - Tokens w/ Prompt          :  195803\n",
            "| - Total Tokens (w/ Padding) :  336060\n",
            "| -----------------------------------------------------\n",
            "| Performance\n",
            "| - Training Time             :  232.50 s\n",
            "| - Tok/sec                   :  1445.39 tok/s\n",
            "| -----------------------------------------------------\n",
            "| Memory Usage                                                                 \n",
            "| - Memory Used               :  51.46 GB                                        \n",
            "-------------------------------------------------------\n",
            "\n",
            "Saving LoRA weights to 'out/finetune/lora-open-llama-13b/final/lit_model.pth.lora'\n",
            "{'checkpoint_dir': PosixPath('out/finetune/lora-open-llama-13b/final'),\n",
            " 'precision': None,\n",
            " 'pretrained_checkpoint_dir': None}\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_checkpoint = torch.load(str(pretrained_checkpoint_dir / \"lit_model.pth\"), mmap=True)\n",
            "/opt/conda/lib/python3.12/site-packages/litgpt/scripts/merge_lora.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lora_checkpoint = torch.load(str(lora_path), mmap=True)\n",
            "Saved merged weights to 'out/finetune/lora-open-llama-13b/final/lit_model.pth'\n"
          ]
        }
      ],
      "source": [
        "!litgpt finetune --config config/open-llama-13b-lora.yaml"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": []
    }
  }
}